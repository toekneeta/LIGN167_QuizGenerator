Okay,  let's  start  our  final  project. Presentations  are  on  Thursday. I  gave  out  details  on  Piazza. Are  there  any  questions  about  that? Everyone  also  should  have  gotten  an  API  link  by  now. I  can  see  how  much people  have  been  using  it.  I  want  you  to  use  it. It's  a  bad  sign  if  you're  not  using it,  people  are  using  it. I  know  that  many  groups  have  not  yet  started  using the  API  would  strongly encourage  you  because  I  don't  know  how  you can  do  your  final  project  otherwise. Yeah. Anything  that  people  want  to  ask  before  Thursday? No.  Okay. So,  let  me  actually return  to  something  that  we  discussed  on the  first  day  of  class  and  we  can really  understand  what  this  means  now, Okay?  Do  you  remember  I  showed  you  something  called these  scaling  laws  for large  language  models  on  the  first  day  of  class,  right? And  we  saw  some  graphs  that  looked  like  this. They  are  remarkable  graphs. Do  people  remember  this?  Yeah.  What  is  this  show? Let's,  we  had  no  idea  what  any  of  this  stuff  meant. Then  let's  look  at  the  y  axis  that's  showing  test  loss. These  are  graphs  about  language  model  performance. We're  measuring  test  loss. That  is,  we  have  a  test  set, We're  measuring  the  language  loss  is  on  that  test  set. The  loss  here  is  loss  on  next  word  prediction. How  well  are  you  doing  the  next  word  prediction? Lower  means  better. The  closer  you  are  to  zero, the  more  likely  you  are  to  be  able to  guess  correct  next  word. What  are  these  graphs  showing? Let's  look  at  this  one  parameters. We  now  know  how  to  count  the parameters  in  a  language  model. These  are  transformer  language  models. By  the  way,  they're  studying  transformers  here. They're  studying  what  happens when  we  make  transformers  bigger, and  we  train  them  on  more  data, what  happens  to  their  loss. We  know  how  to  measure  the  parameters  in  a  transformer. Now  we  know  how  to  do  those  exact  calculations. I  believe  we  did  it  last  time  or  the  previous  class. What  do  you  see? There  is  a  log  scale  here. We're  increasing  from  100,000  up  to  1  billion. The  number  of  parameters  in  our  transformer. The  loss  just  keeps  going  down. It  fits  exactly  on  a  straight  line. This  is  real  like  this  has  been replicated  now  by  many  different  groups. This  is  real.  What's  also been  found  is  that  this  continues  to  hold. It  continues  to  hold  at  GPT  four  scale. If  you  read  the  GT  four  paper, they  have  a  graph  that  looks  like  this. They  don't  actually  say  how  big  GPT  four  is, but  they  do  have  a  graph  that  looks  like  this. And  they  say  the  scaling continues  out  up  to  GPT  four  scale. It's  true  for  dataset  size. As  we  increase  the  amount  of  data, these  are  very  tiny  transformers trained  on  a  tiny  amount  of  data. These  were  huge  transformers  at the  time  of  2020  when  this  was  published. Tiny  transformers,  now  it's  only  1  billion  parameters trained  on  1  billion  to  1  billion  words,  very,  very  tiny. You  know,  we're  now  up  to  2  trillion  parameters trained  on  10  trillion  tokens. So  we've  gone  up  by  a  factor  of 1,000  to  10,000  in these  dimensions  in  the  last  three  years. Compute  at  the  end  of  last  class we  were  starting  to  count  the  number  of floating  point  operations  performed by  your  transformer,  right? We  remember  floating  point  operations, just  number  of  multiplications  and additions  that  happen  in  your  transformer. What  they're  doing  here  is  they're counting  of floating  point  operations  that  happened  during  training. We're  training  on  1  billion  tokens with  a  model  of  a  particular  size. That  implies  a  particular  number  of floating  point  operations  that  are performed.  They're  counting  that  up. It's  a  weird  unit  flop  days, it's  a  lot  of  floating  point  operations. What  we  can  see  here  is  they're  measuring  performance. The  best  performance,  the  lowest  loss  that  you get  for  a  particular  budget  of  compute. You  say  I  want  to  spend  one  tenth  of  a  pedophlop  day. That's  basically  a  certain  number of  floating  point  operations. I  want  to  spend  a  certain  amount  of  training  my  model, what's  the  best  performance  that  I  can  achieve? That's  what  follows  this  linear  relationship here  that  you're  seeing. What  does  this  mean  for  the  future? What's  the  dumbest  thing  that  we  could  say about,  let's  not  think  too  hard. What  does  this  mean? Yeah,  it's  just  going  to  keep  getting  lower. Yeah,  I  think  that's  sort  of  the  default  assumption. We  may  run  out  of  data, that's  a  big  barrier  right  now  that  we're  starting  to, we've  used  10  trillion  tokens. There's  not  that  much  more  high quality  text  data  that  already  exists. I  think  we're  going  to  get,  this is  completely  mechanical. The  curves  that  are  shown  here  are  completely  mechanical. It's  like  literally  you  just  go  and  collect more  data  from  the  Internet. You  make  your  model  bigger. It's  going  to  be  exactly  the  same  thing. You  put  no  thought  into  this  and you  just  run  on  more  computers. All  you  need  to  do  this,  more  money, we're  going  to  need  to  go  beyond  that. Most  likely  as  we  start  to  run  out  of  data, there's  going  to  be  some  human  innovation  that's required  at  some  point.  Not  quite  yet. Probably  not  for  GT  five, Probably  for  GPT  six  would  be  my  guess. But  it's  coming,  there's no  indication  of  the  scaling  curves  falling  off. They  might,  but  the  other  side  of  this  is, it  doesn't  factor  in  human  innovation. Here  on  the  issue  of  human  innovation, let  me  show  you  a  paper  that  was  published yesterday,  which  is  a  big  deal. I  think  this  is  an  alternative  to  transformers. It's  very  different  than  transformers. My  prediction  is  that  models  like  this, this  is  not  the  first  model  in  this  class, it's  just  the  best  model  in this  class  that's  been  published. My  prediction  is  that  these  so  called  states  based  models are  actually  going  to  be  probably the  future  in  this  field. They're  not  transformers. They  share  many  principles  with  transformers, but  they're  not  transformers. One  of  the  very  nice  things about  this  paper  is  they  publish scaling  curves  for  their  model  and they  directly  compare  two  transformers. Let's  look  at  those  scaling  curves. What  happens  to  your  model  as  it gets  bigger  and  you  train  it  for  longer? I'm  always  happy  when papers  show  scaling  curves.  What  are  they  showing  here? This  looks  real  to  me,  by  the  way. On  the  x  axis, they're  showing  number  of  floating  point  operations. Flops,  they  go  up  to  1020  or  ten  to  21  flops, That's  a  lot  of  floating  point  operations at  not  relative  to  state  of  the  R, still  a  decent  number  on  the  y  axis. It's  slightly  different,  it's  perplexity. Perplexity  is  actually  loss  monotonic. With  your  loss,  you  want  lower  perplexity. Basically  it's  on  a  log  scale. It's  actually  just  showing  loss  here. It's  on  a  log  scale, it's  actually  just  loss  on  the  y  axis, what  they're  showing  in  purple  here, you  see  mamba  in  orange. Over  here  you're  seeing  the  transformer. If  you  just  take  a  normal  transformer and  you  compare  it  to  mama, you  actually  get  better  scaling  curves  it  looks  like. Then  the  transformer,  I believe  that  there  are  ways  of  improving  transformers, assuming  that  they're  calling  the  transformer, plus  that  actually  end up  achieving  Mamba  level  performance. It's  very  hard  to  see,  but  there's actually  two  overlapping  lines, they're  literally  on  top  of  each  other. Basically  the  best  type  of  transformer  now, which  goes  a  little  bit  beyond what  we  learned  about  in  class, is  comparable  to  this  new  Mol  Mamba. But  Mamba  has  a  bunch  of other  features  that  Transformers  don't. The  most  important  is that  it's  very  cheap  to  run  on  long  sequences. The  transformer  is  expensive  to run  on  long  sequences.  Why  is  that? Can  anybody  think  about  why the  transformer  is  expensive  to  run  long  sequences? You've  already  learned  enough  to  know  why  that's  true. Yeah. Attention,  yeah,  right? Remember  those  attention  matrices that  we  constructed,  right? How  do  attention,  attention  matrices  scale? Right?  Every  word  is comparing  itself  to  every  other  word, where  every  word  is  trying  to  say, should  I  attend  to  every  other  word. So  how  does  attention  scale  in  sequence  length? Yeah,  non  exponentially. The  second  power,  that's  quadratically. Yes,  it's  sequence  length  by  sequence  length. So  that's  sequence  length  squared. You  have  to  construct  an  attention  matrix, your  scale  quadratically  in  sequence  length. That's  not  a  big  deal  if  you  have  100  words, If  you  have  100,000  words  or  1  million  words, let's  say  it's  1  million  words, that  means  you  have  to  construct  a 1  million  by  1  million  matrix. What's  1  million  squared?  It's  1  trillion. If  you  want  to  do  just  regular  attention on  1  million  words, that  means  you  have  to  have  a  matrix. You  have  to  construct  a  matrix  in  it  with 1  trillion  elements.  That's  ridiculous. Like  that's  unbelievably  large,  even  by  the  scale. That's  basically,  that  matrix is  now  as  large  as  all  of  GPT  four. You  have  to  do  that  for  every  single  layer. Okay,  it's  a  big  problem. These  are  now  models  that  do  not  have that  quadratic  relationship  between sequence  length  and  number of  computations  that  you  have  to  perform. In  fact,  they  do  some  experiments. Let's  see  if  we  can  find  them  right here  showing  what  happens as  you  can  actually  run these  models  with  sequence  length  1  million. What  we're  seeing  here  is  sequence  length  on  the  x  axis, it  goes  up  to  1  million. These  models  I  think  are  likely  to  supplant  transformers. There's  going  to  be  human  innovation  here, actually  probably  starting  in  a  year  or  so  from  now. My  guess  is  it's  going  to  be  a  joint, a  true  joint  effort  between humans  and  machines  to  improve  these  models. We  need  another  generation  of  language  models, I  think  for  them  to  be  actually useful  language  modeling  researcher, we're  going  to  hit  that  point  relatively  soon. And  once  we  do,  it's  going  to  be a  joint  effort,  language  model, human  working  to  language  model  performance. I  want  to  also  just  show  you the  authors  on  this  scaling  law  paper. Some  of  the  other  people  on  this  as well,  Jerry  Kaplan,  Dariomade. They  actually  left  open  the  eye shortly  after  his  paper  was  published. They  found  it  anthropic,  which  has  clawed. Some  of  you  probably  have  tried  it. Article  published  in  the  summer, leaders  warned  Congress  that AI  could  be  used  to  create  bio  weapons. There's  all  sorts  of  threats. This  is  a  picture  of  amid  testifying  before  Congress, they  actually  basically  the  people who  have  been  thinking  about  scaling  laws. When  you  start  thinking  about  scaling  laws, you  start  worrying  or  at  least  thinking about  what  the  implications are  because  you  can  make  predictions about  the  future,  about  what's  coming. Yeah,  I'll  just  say  there's  a  connection  here between  being  able  to  see  the  future  a  little  bit, which  is  not  something  that  we  can normally  do  in  this  field. And  then  starting  to  think  about the  long  term  implications  of  this  technology. Any  questions?  Yeah.  Yeah. Mlps,  I  mentioned  that there  are  all  of  these  alternative  architecture. All  of  the  architectures  do  share  a  common  structure, which  is  they  alternate between  tokens,  mixing  words  together, and  then  looking  at individual  tokens  and  trying to  understand  those  tokens  with  an  MLP, they  all  share  that  in  common. The  main  thing  that  differs  across architectures  is  just  how  you  do  the  mixing. The  states  based  models  do  the  mixing  without attention  because  attention  scales  quadratically. Any  other  questions? Yeah, maybe  what  we  should  do, But  fortunately,  a  competitive  environment  here, so  I  can  decide,  I'm  not  going  to  work  on  this  anymore. But  then  someone  else, military  adversaries,  competing  companies, they're  going  to  say,  no,  no,  no. If  I  make  a  smarter  model,  I'm  going  to  get  ahead. Yeah, I'll  give  two  answers  to  that. It's  a  good  question.  The  first  answer  is that  is  exactly  what  the  scaling  laws  are  saying. Which  is  with  a  fixed  architecture, like  a  transformer,  if  you  have  a  certain  budget, here's  the  best  that  you  can  do  with  this  transformer. People  remember  there  are all  these  hyperparameters  for  transformers, the  learning,  the  batch  size,  weight  decay, all  these  numbers  that  you  can  play  with  that  affect what  your  language  model  performance will  be  with  this  compute  budget, You  play  with  all  of  those,  you  find  the  optimal  one. Optimal  performance  is  what  gives  rise  to the  scaling  law  almost  by  definition. If  you've  done  your  job  well, there's  nothing  better  that  you  can  do. The  flip  side  of  that  is  if  you want  to  go  beyond  the  scaling laws  for  a  fixed  architecture, that's  where  human  innovation  comes  in. Innovation  of  some  sort  comes  in. You  have  to  think  hard  about. I'm  not  going  to  use  attention  anymore, I'm  going  to  use  one  of  these  states  based  models. The  scaling  laws  don't  account  for  that. The  scaling  laws  are  about, if  you  fix  your  architecture, here's  what  performance  will  be. Okay. What  I  want  to  today,  we'll  see  what  we  can  cover. First  thing  I  want  to  talk  about is  the  difference  between pre  training  and  reinforcement  learning from  human  feedback. Rlhf.  How  many  of  you  are  familiar  with  this  term, RLHF?  A  few  people. Okay,  What  we've  talked  about, the  only  thing  we've  talked  about  so  far  is  how to  train  a  language  model  to  do  next  word  prediction. We  have  GPT  three. Let's  say  GPT  three  is  a  model  that was  only  trained  to  do  next  word  prediction. We're  training  it  to  give the  word  we  predict  was  given  the  words  we predict  on  GP  three, this  was  the  only  thing  it  was  ever trained  to  do,  predict  the  next  word. Given  the  previous  context, it  was  clear  that  there  was  already  a  lot  of intelligence  in  models  like  this. When  you  played  around  with  GPT  three, you  knew  there  was  something  in  there. It  was  a  real  pain  to really  could  not  use  it  to  do  anything. Let  me  put  it  like  that,  if you  really  wanted  to  solve  a  problem, you  could  not  use  this  model  to solve  that  problem  in  a  serious  way. Why  is  next  word  prediction simultaneously  like  amazing  as a  task  and  a  very  general  task,  but  also  a  pain? Let's  say  that  I  have  a  newspaper  article. I  want  to  summarize  that  newspaper  article. How  would  I  use  a  system  trained to  do  next  word  prediction  to  do  that? Yeah,  summary,  exactly. I  have  to  find. Yeah,  so  this  is a  short  summary of  this  article. Then  what  do  we  do? We  insert  article  here  like  we  actually  have no  copy  and  paste  the  article in  and  then  we'll  have  a  colon,  right? Why  is  this  a  language  modeling  a  compatible  task? What  we're  trying  to  do,  we  have some  intuitive  problem  that we  want  to  solve,  a  natural  problem. Summarization,  we  need  to  turn  that  into  something that  next  word  prediction  system  will  solve  for  us. Given  that  the  only  thing  is  trying to  do  is  predict  the  next  word. It's  a  pain,  I  have  to  find  ways  of  phrasing  it  so  that the  next  word  prediction  system thinks  the  correct  next  word  to  output  is  a  summary. A  lot  of  times  the  right  next  word  to output  is  maybe  not  a  summary. It  depends  on  what  the  distribution of  text  on  the  Internet  that's trying  to  mirror  looks  like. Writing  code,  Look,  I,  it's  unbelievable. This  was  14  months  ago,  I  think. 16  months  ago  that  people  realized  that. A  little  longer  than  that,  maybe  it's  18  months  ago  that people  realized  that  with  models  like  this, like  GPT  three,  that  you could  get  them  to  write  code  for  you. I  remember  my  mind  was  completely  blown  because we  prediction  system  can  write  code  for  me. But  how  did  you  have  to  do  that? Let's  say  I  want  to  write  a  Python  program that  I  don't  know  computes  Fibonacci  sequence. How  would  I  have  to  embed that  problem  into  a  next  word  prediction  system? Yeah,  it  function, I  have  to  make  sure  that  I  give  a  really, really  descriptive  function  name,  right? So  like  deaf  Fibonacci  sequence  or  something. And  then  hopefully  based  on  the  name Fibonacci  Sequence  and  knows  what  comes  after. But  maybe  that's  not  what  comes  after. Maybe  Fibonacci  sequence  is  a  function  that calls  another  function  that computes  the  Fibonacci  Sequence. That  happens  a  lot  in  natural  code. It's  like  it  leaves  the  hard  work  somewhere else  and  that's  what  the  model  will  complete. Just  so  that  everyone's  on  the  same  page will  tell  P  three. Next  word  prediction  system. Please  write  a  function  for  me that  computes  the  Fibonacci  Sequence. What  will  the  model  say  After  that, I  ask  you  to  then  predict  the  next  word. Yeah,  it  might  include  further  instructions. Exactly.  It's  probably  going  to  be  trying  to imitate  a  person  who  would  write  those  instructions. What  else  might  happen? It  might  include  further  instructions  that  say, send  it  to  me  over  slack. That's  the  last  that  you  hear  about  it. No  Fibonacci  code  for  you. You  can  get  these  systems  like, look  if  you  can  actually  predict  the  next  word. It's  ridiculously  hard  to  do  that accurately  if  you  can  do  it  in  a  fully  accurate  way. It  means  you  do  know  how  to program  if  someone  writes half  of  a  Fibonacci  sequence  program. For  me  correctly,  I  have  to  be able  to  predict  how  to successfully  complete  that  program. Which  means  I  have  to  know  how  to program  but  actually  turning  human  instructions. Into  a  form.  The  next  word  prediction  system will  reliably  know  how  to  interpret  correctly  a  pain. I'm  not  going  to  say  it's  impossible,  it's  a  pain. The  systems  that  all  of  you  use  now, GPT  3.5  the  free  version  of  ChachPT,  Claude  GBT  four. They  include  two  stages  of  training. They  happen  sequentially,  one  after  the  other. The  first  is  called  pre  training, which  is  next  word  prediction. The  second  is  called  RLHF, Reinforcement  Learning  from  human  feedback. The  pre  training  stage, this  is  at  least  as  far, here's  what  I'll  say  about  this  as  well. When  people  were  developing  pre  training, all  of  that  work  was  happening  in  public because  there  was  no  way  to  monetize  it  at  that  time. We  have  a  pretty  good  idea  about  how  pre  training  works. Reinforcement  learning  from  human  feedback. We  have  a  lot  of  high  level  ideas about  how  this  works.  There  are  published  papers. My  sense  is  that  there's  a  lot  more  secret  sauce within  these  companies  about  this  second  stage. I'm  going  to,  I'm  going  to  tell  you  today  is  not even  accurate  at  the  level  of  what's  been published  because  it's  a  little  bit  complicated. I'm  going  to  be  giving  you  though,  a  high  level  overview just  to  get  some  intuitions  about what  this  stage  is  doing  during  pre  training. This  is  where,  as  far  as  we  know, and  I  think  it's  almost  certainly  true, almost  all  of  the  actual  computation happens  if  you  have  a $100,000,000  computation  budget  for  training  your  model, the  GPU  budget  that  you  have, 95%  of  that  is  going  to  be  spent  on  this  first  stage. The  first  stage  is  just  what we've  been  talking  about  for  a  long  time. Now  you  take  a  huge  dataset, you  learn  to  predict  the  next  word  on  that  dataset. The  second  stage  is  different. After  the  first  stage,  we  have, let's  say,  a  fantastic  next  word  prediction  engine. It's  great  at  predicting  the  next  word. It's  terrible.  It  actually  like  doing  what  we  want. We  can't  instruct  it  in  any  way. This  second  stage  is  the  method  that  we're going  to  use  to  get this  model  to  learn  how  to  follow  instructions. Not  only  follow  instructions, but  let's  say  do  what  we  want, which  is  a  little  bit  different, especially  if  you're  a  company. All  of  the  controls  that  GPT  four  has  in  it, when  hopefully  none  of  you  are  asking  it  to  build  a  bomb. But  if  you  were  to  ask  GP  four  to  build  a  bomb, it  will  say  I  cannot  do  that. How  do  you  learn  to  say  I  can't  do  that? It  was  through  this  second  stage. If  you  ask  a  pre  trained  language  model, then  that's  the  next  word prediction  system  to  build  a  bomb. Sometimes  it  will  tell  you, so  we  have  to  train  that  out  of  it  in  the  second  stage. Any  questions  so  far? No. Okay. So  talk  about  the  RLHF  architecture  at  a  high  level. Again,  I'm  going  to  be  giving  you  a  sketch  that  I can  convey  in  not  very  much  time. There  are  more  details  but like  if  you  understand  the  sketch, you  basically  like,  you  won't  be  able  to  implement  it, but  you'll  at  least  know  what's  going  on. We're  going  to  actually  have  two  models  at  this  stage. We're  going  to  have  the  language  model. This  is  the  thing  that  we  want  to  train. We  want  train  to  be  helpful.  We  want  to  train  it. Not  help  terrorists. Or  say  it's  funny  because  GPT  four  is simultaneously  being  trained  for important  stuff  like  don't  help  terrorists, but  also  don't  hurt  people's  feelings. Those  things  are  being  squished together  for  open  the  eyes  business. One  of  those  is  actually  much  more important  than  the  other. Business  interests  are  not  necessarily aligned  with  the  social  interests. We're  going  to  have  a  separate  reward  model. The  language  model  is  going  to be  the  thing  that  takes  instructions, hopefully  follows  those  instructions  well. The  reward  model  is  going  to  give the  language  model  feedback  about whether  it  has  accurately  followed  these  instructions. We're  going  to  have  instructions, we  feed  those  into  the  language  model. We  get  some  output,  that's a  response  from  the  language  model. The  output  from  the  language  model gets  sent  to  the  reward  model. The  reward  model  judges, I  should  actually  say  the  instructions  get sent  when  draw  it  like  that, the  instructions  and  the  output, in  a  way,  I  mean, is  that  we  have  an  instructions  output  pair. The  reward  model  gets  to  see, given  this  set  of  instructions, here  is  the  output  that  the  language  model  produced. The  reward  model  looks  at  those  and  it  says, do  I  think  the  language  model followed  these  instructions  or  not? I  send  back  a  high  reward. If  it  didn't,  I  send  back  a  low  reward. We're  thinking  about  this  as  training  a  dog. Did  puppy  follow  instructions?  We  give  it  a  treat. If  you're  nice  to  your  puppy, you  don't  give  it  negative  reward. Usually  at  least  that  I've  never  owned a  dog  with  children  nowadays. We  do  time  out  with  my  two  year  old. We  do  time  out  when  he's  being  following  instructions. Similarities  between  these  processes. Reward  model  here  can  feel  free  to  give maybe  much  more  severe  negative  feedback  if  it  wants. The  reward  is  really  just  going  to  be  a  number. It's  going  to  be  a  scalar  number, positive  number,  or  a  negative  number. It's  going  to  say,  how  much  do  I  want  you  to upwate  this  behavior  or  do  it? Any  questions  about  this?  Yes. Fantastic  question.  Let  me  expand  on  that  question, which  is,  where  did this  thing  come  from?  We  haven't  seen  this. Let's  say,  let's  talk  about  the  reward  model  quickly. The  reward  model,  it  starts  as  a  normal  language  model. We've  trained  GPT  three. We  create  a  new  copy  of  GPT  three  for  this  reward  model. I  literally,  we  just  copy  the  weights  over. That's  going  to  be  the  initialization  for our  reward  model,  Initialized. What  I  mean  by  initialized  is  that the  initial  weights  of this  reward  model  are  a  language  model. We're  going  to  train  this  reward  model to  predict  what  reward, a  set  of  instructions  and  output  should  receive. We  have,  let  me use  a  slightly  more  descriptive  term  and  output  here. Let's  call  this  thing  like  the  response, we  have  the  instructions  and  response. These  things  get  fed  into  this  reward  model. The  reward  model  is  going  to  output a  single  number,  which  is  the  reward. How  do  we  train  this  reward? That's  basically  the  question  here  in this  is  it's  a  normal  language  model. Okay?  Inside  of  this, it's  just  going  to  be  doing  something  very  special, which  is  instead  of  predicting  the  next  word, we're  now  going  to  be  training  it  to  predict  the  reward. Actually,  there  are  two  questions  to  ask  about  this. The  first,  you  told  me  you  had  a  language  model. How  do  you  get  it  to  predict  just a  single  number  instead  of a  language  model  normally predicts  probabilities  for  each  next  word. I  don't  see  that  happening  here. How  did  that  happen?  That's  one  question. The  second  question  is,  what  loss  function  do  we  use? We  want  to  predict  reward. What's  the  loss  function  that  we use  for  predicting  that  reward? Let's  talk  about  the  first  question. We  have  to  actually  do  some  surgery  on our  language  model  in  order  to get  to  predict  a  single  number. We  have  our  language  model  that  we want  to  turn  into  a  reward  model. I'll  call  it  R  M  here. What  we  input  into  it  is  like  summarize this  document  and  then  we  have  a  summarization. We'll  have  the  instructions  followed  by  the  response. We  input  that  into  the  reward  model. Let  me  actually  expose  some  of  the  internals  here. We  have  layer  one  up to  the  final  layer,  we'll  call  it  layer  K. At  this  layer,  we  get  a  sequence  of  outputs. Let's  call  them,  maybe  we've been  calling  them  Z.  I'll  call  them  for  right  now. Again,  remember  we're  starting from  a  normal  language  model,  right? In  the  normal  language  model, for  every  token  in  the  sequence, you  get  a  separate  output  vector,  right? You  input  n  tokens  into  the  sequence, you  get  n  output  vectors. From  this  model,  we  get  one  up  to  z  n, where  n  is  the  sequence  length. Now  normally  when  we're  doing  next  word  prediction, we  got  out  n  vectors  from  this. We  want  to  use  these  vectors  to  predict  the  next  word. What  do  we  do? We've  talked  a  few  times  about  what  we  do  with this  output  step  to  predict  the  next  words. Yeah,  well  we  haven't  even  gotten  probabilities  yet. Not  quite.  We're  missing  a  step  still. Yeah,  again,  this  is just  the  representations  that  we  got  out  from the  final  layer  was a  new  representation  of  the  meaning  of  each  word. It's  a  vector  that  represents  that. What  do  we  have  to  do  mechanically to  predict  the  next  word? So  let  me  do  a  little  bit  of reinforcement  learning  from  human  feedback. Now,  this  is  making  me  very  sad. So  I'd  like  you  to  all  back propagate  and  update  your  weight so  that  you  can  give  me the  answer  to  this  because  you  know  this. I'm  certain  that  you  know  this. Yes,  I  remember  something  Exactly. So  we  have,  for  doing  normal  language  modeling, we  multiply  each  by an  output  embedding  matrix. I've  been  calling  it  is  our  output  and  betting  matrix. This  transforms  it  into the  correct  dimensionality  number  of  vocabulary  items. And  then  we  do  a  Softmax. We  have  to  do  something  different here,  because  what  we  want  out, it's  not  predictions  of probabilities  for  every  single  word. It's  a  single  number. What's  the  reward  that this  instruction  response  pair  should  receive? Was  this  a  good  summarization  or  was  it  not? That's  what  the  reward  model  is, trying  to  judge,  what's the  simplest  thing  that  we  can  do  here. We  want  to  get  a  single  number to  represent  this  entire  sequence. What  can  we  do?  Yeah, it's  a,  it's  a  vector. We  could  call  it  a  vector  as  well. Yeah,  that's  right. So  we  can  have  a  vector  there  that's the  same  size  as  one of  the  s.  What  do  we  do  specifically? You're  right.  So  we're  going  to  transform something  into  a  single  number. What  should  we  transform  into  a  single  number? If  we  do  that,  it's  a  good  observation. If  we  do  that  one, do  we  have  enough  information  at  Z  one  to  assess? Was  this  a  good  summarization? Why  not? No,  it  doesn't  because  of  causal  masking. We  had  causal  masking  the  first  word, could  not  see  anything  about  subsequent  words. So  if  we  look  at  Z  one, we  cannot  assess  anything  about whether  this  was  a  good  summarization. We  can  continue  this  line  of  reasoning, realize  we  can't  make  an  actual  judgment  about  this until  the  final  output  vector  Zen. Zen  is  the  only  vector  that contains  information  about  the  entire  sequence. Zen  can  judge  was  this  a  good  summarization  or  not? At  the  information  is  there. In  principle,  we're  going  to  take  Zen  and we're  going  to  multiply  that  by just  going  to  take  the  dot  product  with  a  vector. Simplest  way  to  get  a  reward. You  can  do  more  complicated  things, but  it  illustrates  that  we  can  get multiple  types  of  outputs  from  the  same  core  model. Okay?  Multiply,  last  vector z  n  by  reward  vector. This  is  basically  a,  we'll  call  it, we  can  think  about  this  is  the  vector  that  points  in the  direction  of  reward,  right? It  says  it's  like  literally  like, here's  the  direction  of  good  stuff. Don't  go  in  the  opposite  direction. If  you  do,  you're  going  to  get  penalized. Okay,  so  we're  just  going  to  take  the  N  times. If  this  is  high,  that  means  high  reward. If  it's  low,  it  means  low  reward. Okay,  so  we  start  from  here. Now,  how  do  we  train  our  reward  model? I've  said  that  we're  going  to  have  this  vector, but  where  does  it  come  from? How  do  we  actually  learn  the  direction  of  reward? Yeah,  human  feedback. This  is  where  the  reinforcement  learning from  human  feedback  comes  from. The  exact  way  that  it's  done  is  a  little  bit  complicated. Let's  talk  about  it. What  we  do  is  we take  pairs  of  instructions  and  responses. Let's  say  we  have  a  summarization  task. We  have  one  summary  that  was  produced  by the  model  and  another  summary that  was  produced  by  the  model, that  is  the  language  model. We  have  two  of  these  things. We  have  these  two  responses to  the  same  set  of  instructions. We  have  reward  one  and  reward  two. We  separately  ask  a  human  look  at  these  two  responses. Which  of  them  do  you  prefer? I  mean,  what  did  open  the  eye  do  here? They  hired  a  lot of  people  in  all  sorts  of  places. Depending  on  the  task,  they  were  either hiring  like  high  school  math  teachers, were  high  school  math  teachers to  assess  like  math  problems, or  they  were  poorly paid  contractors  in  Africa  and  Southeast  Asia. It  depended  on  the  type  of task  that  they  were  interested  in, like  the  complexity  of  the  task, but  these  were  actually  humans. There.  This  was  not  done  internal, mostly  to  open  AI, these  are  basically  like  new  call  centers. We  basically  have  a  call  centers full  of  high  school  math  teachers. Now  we  basically  have call  centers  of  people  who  are  looking  at pairs  of  model  responses. We  give  the  model  some  instructions, they  produce  two  responses. We  ask  a  human  to  judge  which  of  these  do  you  prefer. Just  get  like  binary  responses, basically  which  of  these  do  you  prefer? Now  we  have  rewards  that the  reward  model  produced  for  these. What  we're  going  to  do  is  train the  reward  model  to  match  the  human  judgments. The  particular  loss  function they  use  is  a  little  complicated, but  basically  what  we  want  to  make  sure  is that  when  the  reward  model says  the  first  response  is better  than  the  second  response, that  agrees  with  human  judgments  on  it. There  was  a  question.  Yeah. Yeah,  yeah. We  don't  know  how  they're  using  that  data. Data  quality  is  important,  potentially. I  don't  know  how  they're  using that  data,  if  at  all,  but  yes. But  they  want  to  collect  that  data  in  case it  turns  out  to  be  useful  for  training, training  the  model  that  is  basically  how  it's  being  used. It  may  not  even  be  used  to  train  the  reward  model, It  might  be  used  to  directly  train  the  language  model. I'll  talk  about  that  in  a  second. It's  a  that's  a  good  question.  Any  other  questions? Yes.  Do  you  think  there  would be  some  cases  where  a  survey open  eyes  about  that if  someone  asked  a  question  about  exposing  open  eyes? But  yeah,  how  would  they? Right.  So,  I  mean, that's  an  interesting  question. So  the  question  is  basically  what  happens  if. The  Raiders  who  are  training  the  reward  model, that  their  judgments  don't  agree  with  open  eyes. Now  they  have  a  very  detailed  set of  instructions  for  Raiders. And  I  think  that  my  guess  is that  if  that  was  important  to  them, they  would  explicitly  tell the  Raiders  to  judge  in  this  way. Now  there's  an  issue  about  potential  leaks  of  that. That  would  be  the  main  reason not  to  instruct  the  Raiders. They  might  leak  the  documents  and  then they  would  open  look  bad. If  you  didn't  want  to  do  that  and  you  also  did  not  want to  have  your  own  in  house  engineers  doing  this. There  is  an  approach  that  you  could use  that  would  be  pretty  effective  for something  that  simple  like  it  does this  defame  open  y  in  some  way. What  would  be  a  very  simple  approach  to training  reward  for  simple  tasks? Okay,  I'll  tell  you  what  the  answer  is. It  would  be  to  use  your  language  model to  make  a  judgment  about  it. If  you  already  have  a  language  model  that's been  trained  using  RLHF, it's  already  a  language  model that  knows  how  to  follow  instructions. Then  you  just  ask  the  language  model, does  this  defame  open  AI,  yes  or  no? It's  a  simple  judgment  to  make  and  you  use those  judgments  to  empower  your  reward  model, which  then  will  train  your  language  models  more. Okay.  Yes. Reward.  Yes,  I  understand. Yeah.  So  we  haven't  gotten  there  yet. So  the  reward  model,  all  that  we've, all  we're  talking  about  right  now  is how  to  train  your  reward  model. We  actually  have  not  talked  yet  about  how  to train  your  language  model  using  this  reward  model. So  it's  a  multi  step  process. We  first  have  to  train  a  reward  model. We  train  the  reward  model  to predict  human  reward  judgments. Now  we're  going  to  use our  trained  reward  model  to  train  the  language  model. That's  the  step  we're  getting  to. Yes,  there  might  be  some  formatting. It's  a  hyperparameter  that  you  can tune  what  the  right  way  to  do  that  is. I  think  having  a  concatenation, maybe  you  have  a  token  that indicates  where  one  ends  and  the  other  begins. But  yeah. Okay,  we  have  our  trained  reward  model. If  it's  really  working  well, this  reward  model  can  mimic  human  responses. It  can  look  at  something  and  say, yeah,  that  was  good,  or  no,  that  was  not  good. Given  that  we're  going  to train  our  language  model  to  actually  follow  instructions. Now  we  have  a  set  of  instructions  that  we  give  our  LM. This  is  training  the  LM, the  language  model,  the  reward  model. We  have  our  instructions, they  go  into  the  language  model. We  have  the  response,  We have  our  reward  model, which  looks  at  this  pair  and  gives  us  back a  reward,  just  like  up  there. We  know  how  to  now  compute  the  reward. What  we're  going  to  do  now is  have  a  new  type  of  loss  function. We're  going  to  update  our  loss  function  here. Our  loss  is  going  to equal  the  reward  that  we  got  back. Times  the  negative  log  probability of  the  response  given  the  instructions. So  let's  think  about  this  a  little  bit. Response,  that's  not  correct. Let's  say  that  we  got  rid  of  the  reward. We  reward  is  equal  to  one. It  goes  away.  What  does  this  loss  correspond  to? We're  trying  to  minimize  this  loss. Because  we're  trying  to  minimize  this, that  means  we're  trying  to  maximize the  log  probability  of the  response  given  the  instructions. Or  alternatively,  we're  trying  to  maximize the  probability  of  the  response  given  the  instructions. That's  the  reward,  is  equal  to  one. What  does  maximized  probability response  given  instructions  mean? What  behavior  will  that  result  in? Yeah,  well, it's  a  little  bit  different  than  that because  this  is  not a  ground  truth  response  that  the  model  produced. We  fed  the  instructions  in, and  now  the  model  generates a  response  from  those  instructions. We  have  to  do  generation,  we  have  to do  token  by  token  generation. Given  this  response,  these  tokens  in  response  to  that, we're  going  to  be  maximizing the  probability  of  the  response  given  the  instruction. So  what  are  we  saying  in  that  case? I  mean,  you're  close.  You're  very  close. I  feel  like  I'm  not  seeing  a  sufficient  level of  excitement  on  people's  faces. Look,  let  me  be  completely  honest  with  everyone. The  questions  I'm  asking  right  now  are considerably  easier  than  other  questions that  you've  all  answered  earlier. In  this,  in  previous  lectures, I  can  just  see  there's  a  bit  of  a  low  energy  level going  on  today  to  some  extent,  understandable. But  this  is  also  like,  this  is  it  guys. We're  now  talking  about  how  GPT  four  actually  works. Like  after  we  know  this,  there's  no  more  magic. This  is  the  final  frontier.  Perk  up? Yes,  yeah, yes, exactly. A  positive  reward  means  we  want  more  responses  like  this. A  negative  reward  means we  don't  want  responses  like  this. We  get  the  model  to  generate, we  collect  a  set  of  different  instructions. How  do  we  collect  this? Who  knows  how  they're  collecting  it? Now,  honestly,  GP  four  is probably  writing  a  lot  of  these  instructions  now. But  before  we  had  a  model  as  intelligent  to  GP  four, they  were  having  humans  write  a  bunch  of  instructions. We  had  an  instructions  dataset. It  was  just  like  define  a  bunch  of different  tasks  that  you  might want  a  language  model  to  do. We  collect  a  dataset  of  these  things. We  run  them  through  the  language  model. We  collect  responses. We  then  use  the  reward  model  to  measure, is  this  the  type  of  response  that  we  want  or  not? If  it  was  the  type  of  response  we  want, then  we  maximize  the  probability  of that  response,  just  back  propagation. If  it  was  not  the  type  of response  that  we  want  and  they  got  a  low  reward, then  we  minimize  the  probability  of  that  response. We're  punishing  it  for  bad  responses. Rewarding  it  for  good  responses? Yes,  yes,  that's  always, sorry,  negative  lock  probability. No,  yes,  it  will  put  positive, it  will  output  a  positive  number. That's  correct.  Positive  number,  Yeah. So,  I  mean,  think  about  it. The  model  can  directly. I  think  maybe  what  the  reward  is  fixed. The  reward  is  a  constant  in the  model  when  you're  doing  back  propagation. The  model  cannot  adjust  the  reward. The  only  thing  that  can  do  is  adjust  the  probability  that assigns  to  a  response, given  the  instructions  in  the  way  to  think  about  this, if  you  have  a  positive  reward, let's  say  a  reward  of  equal  to  one. What  will  you  want  to  do  to this  probability  in  order  to  minimize  the  loss? It's  question,  what  will  you want  to  do  if  you  have  a  positive  reward? How  do  I  minimize  the  loss? I  have  to  minimize  the  thing  on  the  right. Which  is  equivalent  to  doing  what? There's  a  negative  sign  here,  minimizing a  negative  quantity,  raising  the  probability. Now,  suppose  that  the  reward  is  negative, it's  equal  to  negative  one. Actually,  this  minus  sign  and  this  minus  sign  cancel  out. In  that  case,  all  I'm left  with  reward  is  equal  to  negative  one. All  I'm  left  with  is  the  log  probability  of  the  response. I  want  to  minimize  that  quantity. That  means  I  want  to  minimize the  probability  of  the  response. I  think  what  was  getting  tripped  up  there  is  that the  reward  itself  cannot  be  controlled  by  the  model. A  great  question.  Any  others? Okay.  So  one  question  I  was  expecting  to  hear, but  I  have  not  heard, this  whole  thing  doesn't  make  sense. Why  does  it  not  make  sense? Or  there's  a  few  things  here  that  don't  make  sense. Where  do  humans  come  in? First  of  all,  it's reinforcement  learning  from  human feedback  where  humans  come  in. Yeah, that's  for  training  the  reward  model. The  human  only  comes  in  for  training the  reward  model  in  writing  the  instructions. Absolutely.  Yeah,  I  wouldn't think  of  that  exactly  as  the  reinforcement learned  from  human  feedback. It's  referring  to  having  a  human  in  the  loop. In  this  way,  the  human  trains  the  reward  model. Isn't  this  a  little  weird? It  seems  very  indirect,  right? We  have  a  reward  model  that  the  human  has  to  train, but  the  way  the  human  trains  it  is  by saying  which  of  two  things  it  prefers. Then  we  have  the  language  model that  then  is  trained  by  the  reward  model. The  way  the  language  model  is  trained  is  by  showing the  reward  model  what  it  did and  then  the  reward  model  train. It  seems  like  it  seems  like  it's  a  bit  of  a  last  day. It's  okay.  Do  you  all  of  you  know  about  my  special  chalk? A  few  people. This  is  this  is  Th,  this  is  complete. Th,  This  is  Hager  Romo. Chalk.  If  you  ever  need  to  use  chalk, Romo  is  the  way  to  go. There's  a  documentary  about  it that  I  would  highly  recommend  to  everybody. Okay,  let's  go  back  to  reward  model. Look,  this  doesn't  make  sense. It's  a  little  bit  ugly. It  seems  very  indirect. The  question  I  have  is,  why  not directly  train  language  model? Using  the  human  judgments  language  model produces  some  responses. A  human  provides  the  reward, cut  out  the  reward  model  altogether. Why  are  we  doing  this  really  roundabout  thing? Yeah, right. It  seems  like  it's  higher  throughput. That's  basically  what  you're  getting  at. We  can  train  the  reward  model  once, then  the  reward  model  knows  how  to  simulate a  human  in  terms  of  providing  rewards. Then  we  can  have  the  language  model  show the  reward  model  things  that  human  never  saw. The  reason  you  want  to  do  this  is  so that  you  can  run  way. Collecting  human  data  is  very  expensive. Reward  judgments  are  much,  much  cheaper. For  the  same  cost,  you  can  collect  100  or  1,000 times  more  reward  model  judgments  rather  than  human. Now  when  I  hear  that  it  sounds  very  nice, I  think  an  obvious  concern  that  one  may  have  with  this. Which  is  like,  aren't  you  getting  something  for  nothing? It  feels  like  you're  cheating  here.  Why  are  you  cheating? Let's  say  that  you  train  your  reward  model on  1,000  human  judgments. You  collect  1,000  comparisons. You  train  your  reward  model  to match  1,000  human  comparisons. Won't  your  language  model  only  be  as good  as  those  1,000  human  comparisons? That's  the  only  human  data  the  reward  model  actually  got. Wouldn't  you  just  have  been  as  well  off  directly training  your  language  model with  1,000  human  comparisons? Why  expect  to  get  anything  more  out  of  it? I  haven't  actually  seen  this experiment  done  head  to  head, but  there  is  an  intuition  that  people  in the  field  have  about  why  that's  not  true. The  intuition  has  to  do  with  an  asymmetry, a  difference  between  generating something  and  recognizing  something. I'm  a  fantastic  movie  critic,  right? I  can  look  at  the  movie  and  tell you  is  this  a  good  movie  or  not. I  cannot  make  a  movie. I  would  have  no  clue  where  to  start. Sing  with  many  other  things,  art  critic, I  can  go  to  an  art  museum  and  I  could  give  you lots  and  lots  of  opinions about  why  this  painting  is  good, have  gotten  some  idea  about my  artistic  abilities  from  this  class. However,  there's  a  similar  asymmetry  here  where  the  idea, as  far  as  I  know,  this  is  not actually  been  evaluated  in  detail, but  this  is  what  people  in  the  field  believe. It's  much,  it's  much  easier  to  train  a  reward  model, to  predict  human  judgments, to  recognize  whether  something is  good  or  something  is  bad, than  it  is  to  actually  train  a  model  to, to  generate  something  good  or  something  bad. In  this  stage  here, we're  trying  to  train  the  language  model to  produce  good  summaries. Producing  good  summaries  is  much harder  than  assessing  whether  a  summary  is  good. We  don't  need  very  much  data  to  train a  good  reward  model  that actually  can  mimic  human  judgments. Once  it  can  mimic  human  judgments from  relatively  few  data  points, then  we  can  use  that  to  train  our  language  model. Yes. Okay. The  issue  of change  is  very  important  here. There's  a  few  things  that  can  change. One  would  be  human  judgment,  human  cultural  judgment. What  was  socially  acceptable to  say  last  week  maybe  is not  socially  acceptable  anymore. It  happens,  there's  no  magic  there. You'll  have  to  go  back  and  retrain  your  reward  model. There's  a  subtle  thing  that  changes  over  time. When  we  were  collecting  data  for training  the  reward  model,  we  were, let's  say  we  start  from  the  base  GPT  three, the  way  the  GPT  3.5  was  constructed. The  reason  it's  called  GPT  3.5  is  that  they  start, the  initial  language  model  they started  from  was  GPT  three. It  was  just  the  next  word  predictor. And  then  they  applied  RLHF  to  it, and  that's  how  you  got  the  3.5 When  we  start  out  training  our  reward  model, we're  going  to  be  making  judgments  about output  that  GPT  three  provided. Gpt  three,  the  next  word  predictor is  going  to  be  giving  us  a  bunch  of  judgments about  how  to  summarize  or how  do  classification  on  this  text  or various  things  that  various tasks  that  we  may  want  to  perform. It's  going  to  be  following  those instructions  or  trying  to. Then  we're  going  to  be  making  judgments about  those  GPT  three  responses. The  reward  model,  in  other  words, is  learning  to  score  GPT  three  behavior. We  then  train  GPT three  using  the  reward  that  we  got  from  the  reward  model. When  we  train  GPT  three  in  this  way, its  behavior  starts  to  change. The  types  of  responses  it  will  produce  start  to  change. As  the  responses  that  it  produces  start  to  change, the  reward  model  may  not  know  how  to  deal with  those  responses  anymore because  it's  never  seen  anything  like  them. There's  going  to  be  a  distribution  shift in  the  type  of  data. Imagine,  look  at  first, the  reward  model  is  going  to  be seeing  lots  and  lots  of  crap because  GT  three  doesn't  know  how to  actually  follow  instructions. The  reward  model  is  going  to  be  doing  gradations  of  crap. Is  this  a  little  bit  less  crappy  than  the  other  one? Over  time  though,  the  language  model will  actually  start  to  learn  to  follow  instructions. Its  performance  will  get  a  lot  better. The  types  of  judgments  that the  reward  model  needs  to  make  will  change. There's  a  distribution  shift,  yeah, you  need  to  deal  with that  distribution  shift  in  some  way, basically  by  retraining  the  reward  model. Okay.  So  now  you  know  how  GPT  four  works. I'm  happy.  You're  happy. Let's  send  class  here  and  I will  see  everyone  on  Thursday.
