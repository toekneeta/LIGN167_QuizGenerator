Okay,  let's  start  us a  little  while, okay? I  don't  know  how  many  of  you  saw  this  yesterday. There  was  an  open the  eye  dev  day  where  they  announced  the  new  products. It's  a  very  sad  state  of  affairs  that  like classes  turn  into  doing  publicity  for  a  private  company. This  is  the  world  that  we  live  in. These  are  actually  interesting  developments in  the  relevant  for  this  class. Basically,  new  features,  mostly  new  features  have  been released  for  GPT  four  for  your  projects. Several  that  are  relevant. One  of  them  text  to  speech  through  the  API. You  have  any  of  you  used  the  GP  four  text  speech? No.  For,  you  know? Yeah,  See, I'm  actually  not  sure  if  I  can  do  it. Can  I  do  it  on  here? I  have  to  do  it  on  my  phone. So  no, ask  think  of  a  question that  you  want  students  in  this  class  to  be  Williams. Okay.  I'll  give  you  a  few  seconds. So  while  Nico  is  thinking, so  this  feature  has  already  been available  on  the  mobile  app  which  if  you're  paying customers  you  should  all  have like  there's  no  reason  not  to, just  comes  along  with  a  subscription so  you  can  talk  to  GP  four. Now  that's  the  upshot  which  we're  going  to demonstrate  that's  been  available for  a  while  through  the  app. What's  new  is  that  this  is  available  through  the  API. So  you  can  actually  do  this  like you  can  build  new  apps  that  use  this. Now,  this  is,  first  of  all, a  terrifying  thing  for  the  world.  Why? I  can  hear  you.  That  technology  was  already available  speech  to  text,  but  now  it  can  talk. Why  is  this  terrifying? Anyone?  Want  to  hazard? Guess  what's  that? Scam  bots,  old  people  getting called  and  there's  going  to  be  just  this  massive, massive  industry.  I  mean  there  already  is. It's  going  to  be  huge  though. We're  going  to  be  entering  a  world  where like  everyone  needs  their own  like  defense  bots  to  fight  against  the  scammer  bots. I'm  sure  they  have  some  safety  mechanisms  in  place. I  don't  see  how,  look, even  this  particular  technology  is  probably  fine. They  probably  are  doing  some  like, you  know,  essentially  surveillance  of  its  use. Hopefully  it's  good  enough. They  must  be  confident  that's good  enough  if  they  released  it. But  like  this  is  now  possible, it's  going  to  be  released  in various  open  source  forms where  you  can  do  whatever  you  want  with  it. Okay,  go  ahead.  Okay.  Okay. What  is  the  purpose  of  the  embedding  layer in  a  neural  network  for  NOP  tasks, and  how  does  it  differ  from  one  Hot  encoding? The  embedding  layer  in  a  neural  network for  natural  language  processing, NLP  tasks  serves  as  a  way to  convert  words  into  a  dense  vector  of  fixed  size. This  is  different  from  one  hot  encoding, where  words  are  represented  as sparse  vectors  of  a  much  larger  size. Here's  an  easy  way  to  think  about  it. Imagine  you  have  a  huge  cabinet representing  one  hot  encoding. All  right,  we  get  the  idea,  it's  54. Accepting  to  talk  like  look, so  it's  terrifying  but  also  like  this  is  going  to  be super  useful  for  tutor  Aps,  right? Yeah. Okay.  You  should  think  about  whether, I  mean,  I'm  not  saying  all  of  you  need  to  use  this. I  mean,  look,  I  think that  for  instructor  centered  use  cases, if  you're  trying  to  build  an  application  for  instructors, my  guess  is  actually  that  I  don't  like  I  want  to  type, I  don't  want  to  have  to  talk  to  this  thing. That's,  that's  going  to  be  super  annoying. So  some  of  you  should  not  be  using  this. But  if  you're  trying  and  administering  exams,  right? If  you're  trying  to  do  practice  exams  or  something, that's  going  to  be  super  annoying  to  do  it  over  voice, I  think  you'll  have  to  see. But  there  are  going  to  be  other  use  cases where  it  is  much  better  to  talk  to  the  system. So  you're  going  to  have  to  decide  it's  now an  option  for  which  is  good. Okay,  so  that's  one  new  thing. Another  new  thing,  this  will  start, there  we  go,  is  128,000  token  context  length. What  does  this  mean?  It  means  the  length  of the  text  that  the  system  can  process  at once  is  now  128,000  tokens. That's  around  probably  a little  bit  less  than  100,000  words. Can  someone  tell  me  how  long  is  100,000  words? Give  me  like  a  common  object that  like  with  around  100,000  words. Yeah,  entire  dictionary  words. Okay,  well,  sure  all  the  words. Yeah,  that's,  that's  on  the  order  of 100,000  But  that's  not  like  I can  show  4100000  random  words or  no  words  from  the  vocabulary. It's  not  clear  that  that's  something  I  want  to  do. What's  a  common  thing  that  I  might  want  to  process? That's  around  100,000  words. Not  saying  unique  words. Just  yeah,  a  novel. A  novel,  yeah,  that's  on  the  short  side  for  a  novel, but  it's  a  perfectly  respectful  novel. So  let's  take  the  great  gasp. So  GBD  four  has  already  read  the  great  gas. We're  going  to  copy  and  paste  it. So  I'm,  I'm  in  the. I  don't  think  I've  shown  this  before. Okay.  So  this  is  another  way  that  you  can  use  P  four? It's  it's  called  the  playground. Yeah.  Let's  actually  let's  talk  about  this  just  briefly. This  will  be  useful  for  you  testing  out your  app  in  some  early  stages  potentially. So  you  have  to  pay  to  use  this, you  know,  it's  like  $0.01  per  1,000  tokens  or  so. This  demo  is  going  to  cost  me, I  guess  like  a  few  dollar. Maybe  you  go  to the  playground  and  you can  choose  which  model  you  want  to  use. That's  one  of  the  nice  things  here. What  we're  doing  right  now  is  we  want  to use  the  41106  preview, the  preview  version  that  we  released  yesterday. There's  other  versions  that  you  can  choose, the  four  that  they  released  in  June  6,  13. Gp  for  they  released  in  March  14. You  can  do  3.5  There  are  other  models  to  non  chat  models, you  probably  don't  want  to  use  them  actually. You  can  select  the  model.  What's  the  temperature? We'll  talk  about  that.  Basically,  the  temperature controls  how  much  randomness  there  is  in  the  sampling. Higher  temperature  means  more  randomness, lower  temperatures. Basically,  the  model  is  selecting  the  best  thing  it  ever. We'll  talk  more  about  what  that  means and  the  maximum  length  means. How  many  tokens  can  the  model produce  when  you  give  the  text? There  are  several  different  areas where  I  can  type  in  text  here. This  is  what's  called  the  system  prompt. It's  not  exposed  for  you  in  the  chat  version. I  mean,  you  know  in  the  public  version  of  GT  four. The  system  prompt  basically  is giving  high  level  instructions  to  the  system. We're  not  going  to  use  it  right  now, so  we  don't  need  to  worry  about  this. The  user  prompt  is basically  what  you  normally  would  say,  the  GP  four. So  we're  going  to  just  copy  in  the  text  here, let's  get  rid  of  that  thing. At  the  end,  there  should  be  something  that  pops  up, maybe  not,  maybe  it's not  going  to  give  us  the  token  counts. I  checked  beforehand,  I  believe  that  the  great  Gas, the  entire  Great  Gatsby  is  like  65,000  tokens. We  could  ask  questions  about the  great  gas,  I  could  say,  okay. Now  you've  read  the  Great  Gasp  and ask  some  questions  and  it  will  get  those  questions  right. But  there's  a  confounding  variable  which  is that  the  great  gasp  and  lots  and  lots of  student  essays  about  the  great  gasp  have already  been  in  the  GP  four  training  set. So  we  don't  want  to  ask directly  questions  about  this  like  we  don't know  if  it's  actually  understanding  of  Texas  law. I  haven't  done  this  experiment  yet. We're  going  to  find  out  on  the  fly what  we're  going  to  do  is  we're  going  to  insert an  error  somewhere  in  the  middle  of  the  text. I  want  to  see  what  happens because  actually  can  actually  see. Pick  this  up,  let's just  pick  a  random  section  midway  through. Let's  insert  something  here  that  just  could  not  have occurred  in  the  1928  novel  or  whatever  this  is. Let's  talk.  What  should  we  talk  about? Yeah. Okay. Okay. We  answer  a  single  sentence  like,  let's  see  what  happens. So  let  me  let  me  give  it  this  instruction  actually. Before  and  before  we  I  inserted  a  random  sentence. A  random,  let's  say  like  historically  inappropriate, I  inserted  a  historically  inappropriate, historically  inappropriate  sentence  into a  random  location  of  the  great  Gatsby. I'd  like  you  to  find  it.  So  then  now  we're  going  to reminded  of  this  instruction at  the  end  of  the  reason  I'm  giving  it. I'm  being  generous  towards  the  model  right  now because  giving  this  instruction  so  that  it's  reading, this  model  reads  from  left  to  right, it's  reading  from  left  to  right. I  want  it  basically  to  be  able to  look  for  this,  know  what  to  look  for. It's  like  an  easy  version  of  this  task. Like,  you  know  that  you're looking  for  something  historically  inappropriate. Can  you  find  it  in  the  65,000  tokens? Now,  tell  me  what  text  I  inserted  That. Oh,  interesting. That's  historically  inappropriate. Maybe  I  think  I'm  talking  about like  another  type  of  inappropriate  I. Okay,  So  here's  a  cool  thing  that  you  can  do  in  GP  four. We'll  play  around  with  this  for  a  few  seconds. If  we  have  technical  difficulties  getting  this  to  work, because  refusing,  we  won't  continue. But  here's  a  cool  thing  that  you  can  do. You  can  actually  tell the  assistant  what  text  you  wanted  to  start  with. Sure,  I  found  it. So  rather  than  refusing  our  request, we're  going  to  start  off  with  it  saying  yes, let's  see  if  that  changes  things. Okay.  That  was  historically  out  of  place.  Let's  see. That's  weird.  Okay.  So,  I  let  let's  just  ask  you  why this  is  pretty  weird. Uh. Oh,  weird. That's,  that's  pretty  interesting, so.  Okay. Okay.  I  have,  I  have  to  look at  this,  that's  very  interesting. So,  yeah,  go  ahead. I'm  wondering  like  what  they're  doing? This.  It  seems  feasible  that  all  of  this  is, it  is  being  input  verbatim into  the  model.  Almost  certainly. You're  right  there.  There's  a  possibility  that  it's  not. You're  raising  a  fantastic  question. I  mean  there's  a  detail  there  that  I think  is  maybe  not  right, but  you're  raising  a  fantastic  question  which is  processing  context  this long  is  extremely  computationally  expensive. You're  taking  a  shortcut  of  some  sort. And  my  guess  is  not  only  are  you  taking  a  shortcut, but  GP  four  has  some  awareness  of the  shortcut  and  the  limitations  that  it  causes. Which  is  which  is  actually  pretty  shocking  the  previous. There's  another  system  called  Claude  which is  released  by  anthropic, which  is  open  the  eyes  competitor Claude  has  a  similar  length, slightly  shorter  but  similar  length  context  size, but  it  has  no  awareness  of the  limitations  that  it  has  as  a  result  of  this. Like  if  you  ask  the  question  like  this, it'll  just  make  stuff  up. My  guess  is  that  preliminary  evidence  anyway, that  GPT  is  doing some  sort  of  approximation  that  loses  information, but  that  it  actually  knows about  this  loss  of  information. That's  pretty  weird.  Okay,  maybe we'll  play  again  with  this  in  the  next. I  want  to  take  up  too  much  time. Let's  go  back  to  the  MLP, the  multi  layer  perceptron, the  classical  argument  for why  we  would  not  want  to  use  this  to  process  language. I'm  not  saying  this  is  a  correct  argument, but  we  need  to  have  some  intuitions  for why  this  might  seem  like  a  good  argument. This  is  true,  he's  a  nice  guy. We  have  our  word  vectors here  that  I'll  indicate  like  this. We  take  all  of  the  words, and  we  saw  last  class  that every  word  in  the  vocabulary  gets  mapped  to a  distinct  word  or there's  something  called  word  embeddings. These  are  the  three  word  vectors  for  these  three  words. I'm  going  to  run  these  word  vectors  through  an  MLP. We  have  a  slightly,  our  notation  here  is  a  little  bit different  from  what  we  were  saying  before. 123.  This  is  123. Matrix  one  here. Now,  can  someone  remind  me  of  the  exact  formula? Because  there  was  something  funny  here,  right? Because  the  inputs  into  the  MLP  were  themselves  vectors. And  we  had  to  decide  how  is  an  MLP  going  to process  inputs  which  are  themselves  vectors. Previously  the  input,  the individual  nodes  that  were  going  into  the  MLP, those  were  just  scales,  single  numbers. Here  we're  inputting  actually  three  vectors. What  was  our  solution  last  time? It's  not  like  it's  the  correct  solution, it's  just  that  could at  where  you  can  write  down  Mathsterish. What  did  we  do? Yeah,  we  can  cabinate  all  the  vectors. Right,  So  we'll  say  our  input  vector, we  can  cain  them. A  vector  is  equal  to  one, the  weight  matrix  times the  input  vector  H  is  equal  to  relu  of  that. Let's  say  we  just  have  a  single  output  node  now,  one. And  when  we  get  a  probability, one  is  equal  to two  times  p  is  equal  to  sigmoid  of  one, and  now  we  have  the  probability that  this  sentence  is  positive. Okay,  so  what's  the  problem  here? We  were  talking  about  one  possible  problem  last  time, which  is,  I  think it's  a  legitimate  issue  that  you  could  bring  up, which  is  this  thing  cannot  process sentences  with  more  than  three  words  in  them,  right? We  have  11  input  node  per  word. What  happens  if  you  have  a  four  word  sentence? What  do  you  do  with  that  fourth  word? There's  nowhere  to  stick  it  here. So  that's  a  problem. It  is  a  problem,  but  it's  not  really  that much  more  of  a  problem  than what  modern  architectures  deal  with. As  we  just  saw,  GPT  four  has  a  maximum  context  size  of 128,000  That's  the  longest  text that  you  can  input  into  GPT  four. We'll  learn  about  why  basically that  you  can't  go  bigger  than  that. For  a  model  trained  in  that  way, there's  a  little  bit  of  fuzziness  about  this, but  basically  to  a  first  approximation, even  modern  models  have  a  maximum  context, a  maximum  text  length  that  you  can  input  into  them. If  you  told  me  just  for  this  MLP, it  needs  to  have  128,000  input  notes,  maybe  that's  right. That  alone  the  fact  that  there's  going  to  be a  maximum  length,  That's  the  problem. I  would  say  that's  not  one  of  the  primary  problems. It's  a  problem  that's  shared  by all  current  architectures. Let's  keep  thinking  along  these  lines  though. What  would  happen  if  I  increase the  context  size,  this  model, the  maximum  text  length  to  let's  say 100,000  Let's  say  100  for  right  now,  let's  just  say  100. I  want  to  make  our  lives  a  little  bit  simpler. I  don't  want  the  input,  I  take  that  back. Let's  keep  things  exactly  as  they  are. It  won't  change  anything  too  much. Let's  say  that  I  have  100  input  vectors  into  my  MLP. I  have  one,  this  is  a  vector  two, this  is  a  vector  100,  this  is  a  vector. Let's  say  that  the  size  of my  hidden  layer  is  the  number  of  nodes  in  m  hidden  layer. Let's  say  that  stays  the  same,  so  that  stays  a  three. So  I  have  a  123. Let's  go  back  to  the  actual  math that  this  represents  over  here. So  what's  my  input  now? My  input  is  going  to  be the  concatenation  of  all  100  vectors.  Here. Should  be  concat  of  this. Then  I  have  a  is  equal  to  W  one  times  input. So  what  can  we  say  about  the  dimensions  of  W  one? W  one  is  a  matrix. By  what  matrix? Yeah. Okay,  so  number  of  hidden  units. How  many  hidden  units  are  there? Three.  Okay,  So  it's  33  rows.  How  many  columns? The  whole  length  of  this  input  vector, but  can  you  give  me  a  formula  for  that  length? So  let's  say  that  the  dimensionality of  one  or  any  of  the, any  of  the  input  vectors  we  have, each  of  the  input  vectors, let's  say  this  is  one, is  300,  it's  100, maybe  it's  300,  maybe  it's  one. Almost.  Actually  doesn't  really  matter  for  us  right  now, it's  some  number  and then  we  have  to  multiply  that  by  100. How  large  is  this  matrix  here? That  is,  how  many  numbers  are  inside  of  this  matrix? Yeah,  right? It's  going  to  be  three times  100  times  the  dimensionality. Now  let  me  note  one  feature of  how  people  actually  build their  neural  networks  in  practice, which  is  the  number of  hidden  states  here  is  not  going  to  be  three. It's  usually  actually  equal  to approximately  the  size  of  your  input. One  is  of  dimensionality  100, probably  you're  going  to  have  around  100  hidden  units there.  Why  is  that? Because  if  you  don't,  then  you're losing  a  lot  of  information. Basically,  you're  going  to  be projecting  down  into  a  much  lower  dimension, and  so  you  lose  a  lot  of  information  about  your  input. People  don't  want  to  do  that  in  practice. It's  not  like  a  hard  and  fast  rule within  an  order  of  magnitude. This  is  going  to  be  basically the  dimensionality  of  your  word  embeddings. I'm  choosing  one  here. Whatever  your  word  embedding  is, it's  going  to  be approximately  the  dimensionality  of  that. Now  the  size  of  one  is  equal  to the  dimensionality  of  your  word  vectors times  100  times  the  dimensionality  of  your  word  vectors. You  have  dimensionality  of  your  word vector  squared  times  100. This  part,  you  see  something  that's  squared. That  maybe  is  not  great. That's  inevitable  though,  okay? All  neural  architectures  that work  are  going  to  have  something  that  look  like  this. That  is,  if  you  have  an  input, generally  that  input  gets  mapped  to  something with  approximately  the  same  dimensionality. You  don't  want  to  make the  dimensionality  of  the  output  too  small  or  else  you lose  information  this  square  way. It  means  just  that  you  basically, most  neural  networks  consist  of  matrices that  are  approximately  square. You  have  this  quadratic  here. Okay,  what  about  this  100  though? Where  did  that  100  come  from? Yeah,  it's  actually  the  maximum  number of  words  that  you  want  to  input. It's  not  the  number  of  words  in  a  particular  text. It's  you're  designing  your  architecture to  handle  a  maximum  size  length, maximum  length  input.  That's  what  100  is. Okay,  so  what  happens  if we  want  to  update  our  architecture  to  handle texts  of  length  100,000  So  we  have 12  all  the  way  up to  100,000  Should  be, give  that  one  an  extra  big  circle. What  happens  to  the  size  of  W? One.  Now  our  weight  matrix. How  many  numbers  does  it  have  in  it? What's  that? 1,000  times  1,000  times  more  than  the  original. So  it's  going  to  be  100,000 times  the  dimensionality  of  your  word, embedding  squared.  Let's  right. The  same  way  is  over  there,  dimensionality of  squared  times 100,000  That's  1,000  times  more  than  our  previous. Let's  get  a  sense  for  what  these  numbers  look  like. People  routinely  use  a  small  is  model. Not  very  small,  but  a  small  ish  model  these  days. Dimensionality,  your  word  embeddings. This  is  dimension  of  word  embeddings. Let's  say  it's  around  1,000  that's  not  big. Models  way  smaller  than  GPT. Four,  say  it's  approximately  1,000  That's  ten  to  three, that  implies  size  of  one  is  what? That's  going  to  be  103  squared  times what's  100,000  That's  ten  to  five, that's  ten  to  11. This  is  for  word  embeddings  that  are  not  very  large. You're  going  to  have  ten  to  11  parameters  in just  this  single  weight  matrix  1011. What  number  is  that?  100  billion. That's  the  entire  size  of  GPT. 31  of  the  largest  models  that's  ever  been  trained. This  is  one  of  the  major  reasons why  people  do  not  use  MLPs. It's  because  the  size  of  your  weight  matrices increase  linearly  with  the, the  maximum  length  of  the  sequence  that  you  can  allow. It  leads  to  absurdly  large  weight  matrices. Again,  this  is  only  directly  processing  the  input. Deep  learning  is  about  deep  models  that  models, that  do  many  layers  of  processing. You  don't  want  all of  weights  to  be  in  the  lowest  level  of  the  model. You  want  your  weights  to  be  spread out  among  the  first  layer  of  the  second  layer, the  third  layer  of  the  fourth  layer. Here  we're  building  a  GPT, four  size  model  just  to  do  the  first  level  of processing,  that's  a  disaster. A  lot  of  decisions  in  deep  learning come  down  to  just  considerations  like  this. We're  not  thinking  very  hard  right  now,  right? We're  doing  simple  arithmetic and  just  seeing  like  what  happens  if  we, if  we  decide  to  do  something. Any  questions  about  this? Okay,  yeah. Oh,  right.  Oh,  great. Okay,  great.  So  where  did  this  100,000  come  from? 100,000  Yeah. Yeah,  yeah,  yeah. Dimensionality  squared  comes  from  the  fact  that So  this  is  our  hidden  layer,  right? So  this  is  our  first  hidden  layer. When  we  project  down, we're  mapping  from  the  input  vectors  to  a  hidden  layer. And  generally,  people  want  the  size  of the  hidden  layer  to  be  approximately  the  same  size as  the  individual  word  embeddings. Yes,  yeah,  yeah. It's  100  billion  parameters  to  process. Just  do  this  level  of  processing. This  matrix  W  one. Yeah,  no,  no,  no. This  is  just  to  do  input. I  mean,  this  is  like  you  want  to  build like  the  way  that  people  think  about  the  size  of models  is  basically  the  size  of  the  embedding  vectors. The  size  of  the  embedding  vector  basically determines  the  size  of  your  hidden  states. You  could  choose  it  otherwise, but  that's  what  works  best  in  practice  to train  a  not  particularly  large  model  in the  sense  that  your  word  embeddings only  have  1,000  dimensions. You  need  to  build  a  100  billion  parameter  model. That's  what  we've  just  learned,  which  is  crazy. Any  other  questions  about  this? Okay,  that's  one  basically  look, I  gave  some  math  here  that  gives  some  intuitions. Hopefully  high  level  idea  is  just  if  you  have  an  MLP, your  weight  matrix  has  to get  bigger  and  bigger  and  bigger. As  your  sequence  length  increases, it  gets  ridiculously  big  very  quickly. If  you  do  this,  there's another  reason  I'd  say  more  to  do  with  linguistics. Many  people,  I'd  say most  people  in  the  field  still  think  MLPs  are  a  bad  idea. Yeah,  word  order  is  fine  here  actually. But  let's  talk  about  that.  It's  a  great  question. What's  your  concern  about  word  order captured  in  the  order? So  that's  a  great  observation. When  we're  doing  this  concanation  here, we  absolutely  are  preserving  the  order in  which  these  words  arrived. The  model  will  know  about  word  order. Why  does  the  model  know  about  word  order? Where  is  that  information  represented? The  fact  that  I  came  before  like  here. How  can  the  model  know  that? Yeah,  exactly. We  have  different  weights  coming  off  of  the  first  word, in  the  second  word,  in  the  third  word, and  so  on  in  the  sentence. Each  word  location  contributes differently  to  each  distinct  hidden  state. That's  how  the  model  will  keep  track  of  word  order. Word  order,  actually  something  MLP's  do  great. The  way  I  would  actually  think  about  this  problem, it  is  related  to  word  order  but it's  actually  almost  the  opposite. Give, yeah. I  mean  you're  training the  rate  based  position  Coast  law and  you're  trying  to  capture some  relation  between  life  and  Cos  law. If  you  can  serve  and  say  I  don't  like cost  law  or  I  really  don't  like  Coastlaw'. Fantastic. That's  that's  exactly  right. So  if  I  change  the  position of  I don't  like  Coleslaw, so  don't  we  get  the  idea. So  I'll  just  write  it  in  here. If  I  change  the  position  of  the  words, any  word  in  the  sentence, then  basically  all  words  that  come  after  that, they're  now  in  a  completely  new  location and  the  weights  that  they touch  are  now  completely  different. Previously  we  said  we  had  Lila, like  touched  this  particular  set  of  weights, touched  this  particular  set  of  weights. When  we  shift  the  words  just  by  one  word, do  they  see  a  completely  new  set  of  weights? Now  those  weights Imagine  we're  only  training  on  a  finite  amount  of  data. There  are  going  to  be  all  sorts  of combinations  of  words  in  our  data that  there  are  going  to  be all  combinations  of  words  that we  never  got  to  see  in  our  data. If  you  present  the  MLP with  something  that  it  hasn't  seen  before, there's  no  reason  necessarily  to think  that  it  will  be  able  to  generalize. The  weights  that  will  be touching  each  word  are  potentially completely  unrelated  to  the  weights that  touch  those  words  in  the  training  set. Another  way  of  saying  that  is  it's  not  clear where  in  the  model  you  can  represent the  similarity  in  like I  like  Cols  law  and  I  don't  like  Cols  law. Those  are  almost  the  same  sentence,  right? They  mean  almost  exactly  the  same  thing, except  one  is  positive,  one  is  negative. But  like  the  meaning  is  the  same, it  doesn't  seem  like  there's  anywhere  in  the  model where  that  sameness  or that  similarity  could  be  represented. You  just  have  two  completely  different  representations for  those  two  sentences. That's  a  consequence  of  the  fact  that each  input  word  gets its  own  set  of  weights,  each  input  location. I  should  say  this  is  a  theoretical  argument. I  would  say  that,  let's say  a  little  bit  more  about  this. What  are  the  types  of  ways  that we  can  superficially  alter sentences  while  retaining  a  lot  of similarity  in  meaning?  You  can  insert, so  the  insert  qualifiers, you  can  insert  adjectives,  right? Like  I  like  yummy  colsla. What  else  can  you  do  where  you  end  up shifting  the  positions  of the  words  but  retaining  a  live  of  similarity? Yeah.  Okay,  Right, contractions  or  not  using  contractions,  what  else? Yeah,  Okay,  qualifiers  of  various  sorts. So  not  adjective  necessarily  but  qualifiers  like, you  know,  the  law  that  they  have  at  the  store. I  mean,  something  like  that. Yeah,  I  guess,  yeah. So  could  you  give  an  example  of  that? I  don't  exactly.  Yeah,  Yeah. I,  student  of  167, do  not  like  law.  Okay. Relative  clauses. Another  case  would  be  embedding  like, if  I  don't  like  law, then  I'm  not  going  to  like  this  party. Because  obviously  the  party, all  they  do  is  they  serve  cola, embedding  the  sentence  in  various  ways. I'm  just  giving  you, this  is  just  the  taste,  there's  lots  of  these  things. There  are  lots  of  ways  of  basically  creating new  sentences  out  of  old  ones that  basically  preserve  the  meaning  of  the  old  sentence, but  shift  the  words  around  in  various  ways. There's  a  I  don't  like  that's  not  quite  Yoda, but  there's  ways  of  shifting  a  word. Order  means  the  same  thing. The  neural  network  will  not perceive  those  sentences  the  same  way  though. This  is  the  classical  argument  for  this. People  found  it  persuasive. There's  not  strong, I'd  say  empirical  evidence  against  it  yet. My  prediction  though,  is  that  I  always  used  to  give this  argument  very  confidently  in  previous  years. It's  too  soon  to  call  it,  but  my  guess  is that  this  argument  is  not  going  to  hold  in  the  future. I,  there  are  going  to  be  architecture. I  think  the  future  basically  looks  like every  word  position  serve its  own  set  of  weights.  Something  like  that. And  it  doesn't  matter  what  I'm  saying  right  now  is that  another  way  of  saying  this is  M  do  not  have  good  inductive  biases. We  want  architectures  that  have  good  inductive  biases. What  is  an  inductive  bias? Inductive. For  many  people  in  machine  learning, biases  are  a  good  thing. You  don't  like  bias, but  machine  learning  researchers traditionally  have  like  bias. Bias  means  basically  information that  the  model  has  before  you  even  start  training  it. Why  do  you  want  to  put  information  into your  model  that  it  learns  more  easily? Patterns  that  actually  occur  in  the  world, those  are  easier  for  the  model  to. That's  what  many  machine learning  researchers  have  traditionally  liked. I  would  say  the  traditional  idea, build  systems,  build  models  with  good  biases. This  is  the  traditional  argument, we  want  to  build  something  into our  model  that  says  word  order  matters, obviously,  but  it  should  not  matter  this  much. You  want  information  to  be  shared across  different  positions  in  a  sentence  or  in  a  text. The  problem,  the  entire  history  of the  field  is  about  taking  models, removing  inducted  biases  from  them, and  getting  improved  performance. There's  nowhere  in  the  field  basically, where  inductive  biases  have  helped. The  trajectory  has  been  just  taking existing  models  and  stripping out  anything  that  looks  like  an  inducted  bias, seeing  much  better  performance. I  think  that's  what's  going  to  happen here,  has  not  happened  yet. One  of  the  major  reasons  why  has  not  happened  yet  is this  problem,  the  weight  explosion. The  fact  that  you  get,  if  you use  an  MLP  in  a  very  straightforward  way, your  model  is  just  so  huge. We  have  to  figure  out  a  way  to  get  rid  of that  problem  while  maybe keeping  the  low  inductive  bias of  the  MLP.  That's  my  prediction. Basically,  what's  the  rest  of  this  course  going  to  be? It's  going  to  be  various  architectures  that  go  beyond the  multi  layer  perceptron  that actually  solve  both  of  these  problems  together. There  are  different  approaches  to  this. The  first  architecture  that we're  going  to  be  talking  about is  the  recurrent  neural  network. It  is  interesting  that  basically both  of  these  problems  always  get  solved  together. The  neural  network  solves both  of  the  problems  at  the  same  time, in  the  same  way  transformers  do  as  well. Let's  see  how  recurrent  neural  networks  do  it. By  the  way,  when  I  was  talking about  this  problem  about  word  order, it  doesn't  only  apply  to  single  sentences. It  used  to  be  the  case  that  these  models basically  looked  at  single  sentences, but  that's  obviously  not  true  anymore. They  process  entire  books. When  you  think  about  it  from  that  perspective, this  problem  like  shift  in  variance, like  that's  what  we  want,  that  if  you  move  words, words  in  the  text  to  the  right, the  meaning  doesn't  change  much. At  least  not  perfect. Shifting  variance,  it's  like  approximate. What's  the  issue  that  you  encounter? If  you're  processing  a  book, it's  that  a  sentence  on  page  one  probably  means approximately  the  same  thing  as the  same  sentence  that  appears  on  page  50. Maybe  not  exactly,  there  can  be  some  wrinkles  there, but  to  a  first  approximation,  it  means  the  same  thing. The  MLP  obviously  does not  capture  that  in  a  straightforward  way. Let's  talk  about  these  two  properties  that  we  want, and  I'll  call  them  two  good  properties  in quotations  because  maybe  one  of  them  is  not  so  good. But  this  is  the  perspective  of  the  field. One  will  be  slow  or  no  growth  of  model  size. With  increasing  sequence  length, as  you  have  longer  and  longer  texts, your  model  does  not  grow linearly  with  the  length  of  the  text. Then  the  second  property  is  approximate  shift  invariance. Again,  you  don't  want  perfect  shift  in  variance, but  something  that's  not  completely  as  variant  as  an  MLP. So  we're  going  to  start  talking  about current  neural  networks,  RNNs. Rnns  are  very  interesting because  this  used  to  be  everyone. Until  2017  everyone  used  RNNs  processing  language. There  was  just  nothing  else  basically. Then  2017  came, all  of  your  favorite  language  models  are  still  transform. We're  going  to  learn  about  transformers  after  RNNs, but  the  very  interesting  thing  is  that  RNNs  have some  properties  that  are  nice  that  transformers  don't. There  are  new  architectures  that  basically combine  the  best  features  of  transformers  and  RNNs. They're  not  Transformers  anymore. Now,  that's  not  what  GP  four  is. All  I'm  missing  something.  The  AC  went  off. Is  this  the  first  time  this  quarter  of  this has  happened  or  Yeah. Okay.  I'm  going  to  have  to  get  used  to  this. Okay,  Transformers  are  like  the  end. All  all  we're  going  to  see  that. We'll  see.  Okay,  we'll  probably  see  this  even  now. Rnns  have  at  least  one  very  important  property that  transformers  don't  like. It  will  be  great  to  have  models that  combine  these  things. How  should  we  start? Okay,  this  is  freaky. So  yeah,  it's  early  quiet. Okay,  let's  do  this  over  here. So  we  have  a  sentence. What  we'd  like  to  do  is  process  this  sentence. Get  basically  a  representation of  what  this  sentence  means. That's  going  to  be  our  ultimate  goal. It's  going  to  be  to  take  in a  sentence  like  this  or  a  phrase  like  this, turn  this  into  a  hidden  state, turn  this  into  basically  a  vector, and  then  we're  going  to  use  that  vector  for  an  MLP. Does,  the  MLP  that  we  had  turns sentences  into  vectors  that then  we  use  for  classification. We  have  a  vector,  we  map  it into  one  of  those  output  units. We  run  that  through  a  sigma that  gives  us  back  a  probability. We  want  to  do  exactly  the  same  thing  here. We  want  to  have  a  function  that  turns  sentences  like this  into  vectors  in  RNN. We're  going  to  learn  about  one  type  of  RNN  right  now. It's  a  particular  strategy for  doing  this.  Where  is  the  RNN  do? I'm  going  to  give  a  simple  example  of  this. Okay,  what  the  R  and  N does  is  it  processes  words  one  at  a  time. You  feed  the  word  into  a  function. You  get  a  hidden  state  h  one. You  take  that  hidden  state  h  one, and  you  combine  it  with  the  next  word that's  going  to  give  you  a  hidden  state  h  two. You  take  the  hidden  state  h  two  and  you  combine  it with  the  next  word  that  gives  you  a  hidden  state  h  three. How  many  people  here  have  worked  with Markov  chains  before  hidden  Markov  models? Maybe  not  that  they're  not  even  teaching  this  anymore. It's  very  reminiscent,  at  least, of  a  system  classical  modeling  approach that  people  use  called  the  Hidden  Markov  model. Not  exactly,  but  very  closely  related  to  it. Let's  write  down  some  math  and  we'll  see  how  this  goes. But  again,  the  basic  idea  is  that  we're  going  to be  processing  these  words  one  at  a  time. And  feeding  the  past  results  of our  processing  into  the  next  state  of  processing. This  is  our  initial  hidden  state. How  do  we  choose  this  thing?  We  can just  randomly  initialize  it. We  just  randomly  choose  an  initial  hidden  state.  Okay? What's  H  one  equal  to? Let's  give  a  formula  for  one. Again,  I'll  assume  that  we've transformed  all  these  words  into  word  vectors. That's  always  going  to  stay  the  same. Whenever  we  have  a  word,  we  have a  lookup  table  that  says  what's  the  word  vector  for  this? This  is  a,  this  is  red,  this  is  Jama. Has  anyone  read  these  books, by  the  way?  No. They're  like  on  the  less  annoying  side  of  toddler  books, there's  some  nice  things  about  them. If  you're  in  a  situation  where  you need  to  read  toddler  books  for  whatever  reason. Recommended.  Yeah,  question  I. Okay,  fantastic  question. This  is  a  choice  that  you  have when  you're  building  a  tokenizer. Tokenizer  is  something  that we  talked  a  little  bit  about  tokenization  last  time. I  gave  a  very,  very  simple  type  of tokenization  where  each  distinct  word  in your  vocabulary  gets  a  distinct, it's  just  another  choice  that  you  can  make. It's  another  hyper  parameter,  in  fact,  for  your  model, that's  how  people  think  about it  that  you  can tune  before  you  start  training  your  model. So  what  are  the  advantages  of choosing  to  capitalize and  uncapitalized  words  separately? And  what  are  the  advantages  of choosing  to  represent  them  combined? Separated  it,  yeah.  Yeah. Worth  Almost  none  of those  will  actually  appear  in  your  corpus to  a  first  approximation. If  it  doesn't  appear  in  your  corpus, you  can't  learn  it,  so  you  can  just  throw  it  out. You  have  two  for  each  one,  right? So  what  are  the  consequences  of  that? It  would  be  twectrmb  exactly, So  you're  embedding  matrix  is  now  twice  as  large. That's  one  consequence.  Let's  talk  about, let's  thing  about  learning  though. So  I  guess  it  would  be  good  in a  way  you  tell if  they're  capitalizing  a  word  at the  beginning  of  the  sentence,  for  example. You  could  tell  the  registered,  uh  huh. If  they  don't  capitalize  it,  maybe. Okay.  So  if  you  do,  if  you  do  capitalize  it, then  you  can  keep  track  of  someone  who  like  the  fact  that someone  decided  to  capitalize  something  in one  position  in  the  sentence.  That  can  be  important. It  can  tell  you  whether  something  is  a  name  or  not  like. There  are  some  examples  of  this, of  words  that  can  be  names, but  if  not  capitalized,  are  not  names. One,  there's  one  example  I  can  think of  that's  not  appropriate  for  this  class. No,  that's  not  the  one. Not  my  favorite  example,  but  it's  a  legitimate  example. Yeah.  Look,  Aids  capitalized versus  not  mean  different  things. If  you  decide  to  collapse  those  two  things  together, maybe  you  can  use  the  rest  of the  context  to  figure  it  out. But  like  your  model  now  has  to  do  some  work  to  do  this. Was  there  another  example? Yeah,  great  Bell. The  last  names  are  a  lot  easier.  You're  right. Bell  means  something  different  if  capitalized. Great.  In  general, if  you  decide  to  collapse  these  things  together, you  lose  information  about  the  text. One  principle  is  a  very  good  principle  for  tokenization. Almost  anything  to  do  with  processing  is  you  want  to keep  around  as  much  information  about  the  input  textile, then  your  model  can't  use  that  information. It's  just  not  there.  You  can  always  throw  out, there's  no  way  to  put  back  information  that  is  lost. On  the  other  hand,  there  are disadvantages  besides  the  weight  matrix  issue. It's  related  to  you're doubling  your  weight  matrix  size,  potentially. You're  not  actually  going  to  double  it  because most  words  won't  appear capitalized  in  your  text and  you  don't  need  to  worry  about  it. But  there  are  some  disadvantages beyond  that  issue  to  having separate  vectors  for  capitalized  and  non  capitalized. That  those  vectors  would  be  very  close  to  each  other. Almost  like  the  meaning  would  be  almost  the  same  as  just a.  I  think  that  you're  on  the  right  track  that  I would  put  the  emphasis  on  a  different  part  of  that. You're  right,  you're  going  to  have, for  many  words,  lower  case  in  capital  bell  might  be  very, very  different  because  one  person, like  one  is  a  physical  object  like  not  a  person. Those  factors  will  actually  be quite  different  from  each  other,  I  expect. Yeah,  for  some  things  you're  going  to  very  often. Okay,  great.  So  you're  going  to  run  into data  sparsity  issues  where now  you  have  all  these  capitalized  words that  you  only  saw  four  times  in  your  corpus. It's  very  hard  to  learn  exactly  what  they  mean. The  broader  point  here  is that  there's  no  more  information  sharing. When  you  collapse  capitalized,  non  capitalized, you  get  to  share  information  across these  two  different  like  word  forms. Now  you're  saying  no,  these  two  things are  completely  unrelated. Things  that  you  learn  for  one no  longer  carry  over  for  the  other. You  have  to  learn  things  twice. Now,  in  many  cases, what  I  would  say  is  that  learning things  twice  is  not  a  problem  if  you  have  enough  data. One  of  the  lessons  of  deep  learning  is  that  if  you  can, you  should  just  learn  everything that's  very  closely  related  to the  issue  of  inductive  bias. Whether  you  collapse  these  things  or not  is  a  form  of  inductive  bias. It's  high  inductive  bias, you  do  collapse  low  inductive  bias  in. Low  inductive  bias  in  general  is better  when  you  have  huge  amounts  of  data. High  inductive  bias  is  better  if  you  don't. Yeah,  yes. So  basically,  yes,  although you  might  think  that  there's  less  information being  carried  in  misspellings  if  they're  not  deliberate, what  information  will  be  carried  is  basically  information about  the  speaker  and  the  fact that  the  speaker  is  in  the  context. The  speaker  is  the  sort  of  person  who  in  this  context, would  make  spelling  errors. That  does  tell  you  about what's  probably  going  on  in  this  text. You  will  lose  information, but  it's  not  going  to  be the  same  type  of  information  that  you  lose, at  least  as  with  capitalization. Yeah. Yeah,  it's  true. No,  no,  you're  absolutely  right. I'm  just  saying  that  the  consequences of  this  inductive  bias  are  going  to  be  different. But  you're  right,  it  is an  inductive  bias  and  it  will  cause  you  problems. It's  probably  inferior. If  you  have  lots  and  lots  and  lots  of  data, we'll  get  the  misspelling  one  is  a  little  bit subtle  because  there  are  so  many  ways  to  misspell  things. So  it's,  it's  different  than  capitalization. I  think  that  the  trade  offs  are  different  there. We're  going  to  talk  more  about the  details  of  tokenization in  a  couple  of  weeks  from  now. Oh  yeah,  that's  an  exciting  topic. It's  important.  We  will  be  talking  about  it, bring  up  the  spelling  issues, then  we'll  come  back  to  that. Okay,  let's  just  discuss  formally  what  this  is. I'm  going  to  give  an  extremely  simple  version  of  this. There  are  many  more  sophisticated  RNNs. We're  going  to  go  through  the  simplest  11  vector  one. What's  going  to  be  equal  to? It's  going  to  be  equal  to,  we'll call  it  for  hidden  times. So  there's  different  versions  of  this  that  we  can  do. Okay,  let's  do  this  version h. Okay? So  what  do  we  do  here?  Let's,  let's ignore  the  rail  you  for  right  now. We  took  the  initial  hidden  state, we  multiply  it  by  a  matrix  H. H  stands  for  our  hidden  weight  matrix. An  incoming  hidden  state. The  hidden  state  that  represents the  previous  sentence,  previous  part  of  the  sentence, we  multiply  by  H.  We  add  that  to  our  input  vector  Ma, multiplied  by  a  weight  matrix,  WI. We  have  separate  matrices  for  processing the  previous  hidden  state  and  the  current  input  word. We  add  those  together, we  could  do  something  fancier  for  right  now. Let's  just  say  we're  adding  them  together  and then  we  run  the  result  of  that  through  relate. Yeah. Yes,  that's  the  key. That's  like  the  defining  property of  an  RNN  or  one  of  the  main  defining  properties. Let's  talk  about  how  we  calculate  H22. What  we  do  to  calculate  this  hidden  state. We  take  the  previous  hidden  state,  that  hidden  state, we  multiply  that  hidden  state  by  H.  We  add that  to  the  contribution  from  the  current  word. It's  going  to  be  relu.  We  take the  previous  hidden  state  which  was  H  one, we  add  that  to  I, the  input  weight  matrix times  the  current  input  which  is  red, H  three,  our  final  hidden  state. What's  the  sequel  to  re,  same  formula. So  we  do  H, we're  calculating  H  three. We  look  back  at  the  previous  hidden  state  h two  plus  I  times  pajama. What's  going  on  here?  One  crucial  property which  was  already  noted  is that  it's  the  same  exact  weight  matrix  being applied  to  the  hidden  states  in the  inputs  at  every  single  time  point. We've  immediately  solved  this  problem. We  have  no  growth  of the  model  size  with  increasing  input  length. What  are  these  hidden  states  doing? Like,  what  does  H  one  represent? How  can  we  think  about  that  in  the  context  of  this  model? Yeah,  so  far  we've already  only  read  Laama. It  represents  all  of  the  information that  this  Llama  and  before  represent. Basically  what  it  knows  is  that  you  had one  word  in  the  sentence,  that  word  was  llama. What  does  H  two  represent? Yeah,  exactly  two. If  things  are  working  well, H  two  includes  information  about the  entire  sentence,  up  through  the  word  red. It  got  that  information  directly  from  the  word  red here  and  then  as  summarized  by  H  one  there, one  way  to  think  about  is  you're recursively  summaries  of  all  of  the  previous  part  of the  sentence  and  fusing those  summaries  with  the  current  word. So  H  three  gets  information  from  H  two  and  pajama. Now  at  the  point  of  H  three, we  have  a  single  hidden  state that  represents  the  meaning  of  the  entire  sentence. What  can  we  do  with  this  hidden  state? We  can  run  it  through  a  classifier. Let's  say  we  want  to  do  sentiment  classification. I  don't  know.  I  guess  it  depends  on  your  mood, whether  when  you  see  this  phrase, that's  a  positive  or  negative  phrase. But  we're  going  to  have  a  special  output  matrix. We're  going  to  have  one, our  output,  let's  say  it's  equal  to  O. I'm  going  have  a  special  matrix  there. Times  H  three  or  times  the  last  word  in  my  sentence. Then  my  probability,  probability  of  positive, it's  equal  to  sigmoid  of  one. We  do  exactly  the  same  thing  that  we  did  in  the  MLP. We  have  a  final  hidden  state. We  run  that  hidden  state. We  multiply  that  hidden  state  by  a  matrix to  turn  it  into  something  of  the  right  dimensionality. We  run  that  thing  through  a  sigmoid  or a  soft  max  to  get  back  a  probability. If  we  do  classification. Yes.  Yes,  I for  inputs.  Yeah. Yes.  Well,  okay,  can  you  take a  guess  how  we're  going  to  get  these  matrices? Yeah,  where  are  these  W's  come  from? How  do  we  get  them? Yeah,  back  propagation.  We  have  a  loss  function. We  want  to  classify  sentences  correctly. We  compute  loss,  we  do  back  propagation. And  because  this  is  an  RN,  N, the  type  of  back  propagation that  we  do  is  a  little  bit  different. It's  called  back  propagation  through  time. I  think  we're  going  to  learn  about  that  next  time. Okay?  See  everyone  then.
