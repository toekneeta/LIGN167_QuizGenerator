Web  design  front  end  developers  here. Yeah.  Does  anyone  want to  volunteer  for  a  little  experiment  right  now? Yeah.  You  want  to  volunteer?  Okay,  great. So  why  don't  you  come  up, does  anyone  have  a  blank  piece  of  paper? We  can  make  it  do  without  it. That's  fine.  Just  be  a  little  better. If  we  had  a  blank  piece  of  paper with  I  only  have  the  line  stuff,  that's  fine. What  I'd  like  you  to  do  is  a  sketch  of  a  simple  web  site. Just  draw  a  sketch however  you  would  do  it  when  you're  doing, imagine  that  you're  doing  prototyping  for a  new  website  and  you're  drawing  out  ideas,  brainstorming while  he's  working  on  this. Any  questions  I  can  answer, not  about  the  course  material,  but  about, let's  say,  the  course  structure  right now,  your  problem  sets. They  were  due  last  Thursday. We're  in  the  process  of  grading  them. I  hope  that  we  can  get  them  done  this  week. We're  trying  to  get  a  quick  turnaround of  the  problem  sets  in  this  class. Next  problem  set  I  believe  is going  to  be  released  on  Thursday. Again,  strong  encouragement  to start  thinking  about  your  final  projects. Post  questions  about  this. I'm  very  happy  to  engage  on  there. And  I  like  taking  questions  on  pizza  because  then everyone  else  in  the  class  can see,  can  see  the  discussion. Yeah,  absolutely. Yes.  Let's  say  next  week. In  terms  of  guidelines, it's  very  open  in  terms  of  what  you  can  do. It  has  to  be  AI  tutor, but  that's  an  extremely  broad  thing. It's  going  to  be  an  application  that  you  build, it  should  have  an  interface  that  students  can  use. Students  should  be  able  to  talk  to  it  in  some  way. I  think  the  major  question  is what  are  you  trying  to  atom? Which  part  of  the,  It  could be  an  existing  part  of the  class  that  you're  trying  to  automate. It  could  be  a  new  part  of  the  class  that  cannot exist  right  now  because  of  cost  or  feasibility. But  what  is  it  that  you're  trying to  accomplish  with  this  thing? I  think  that's  the  major  question  for  you  to  think  about. Uh  huh.  Sure.  Okay.  That's  good. Okay.  Thank  you.  Okay.  So  you  can  have  a  seat. Maybe  we'll  I  might ask  you  some  touch  ups  to  this  in  a  few  minutes, but  I'm  going  to  take  a  picture  of  this. I'm  going  to  send  it  to  myself. Okay?  So  let's  take a  look  at  the  picture  first.  This  is  what  it  looks  like. It's  a  nice  simple  website,  company  name  description. Okay,  So  we  have  a  little  sketch  here. What's  your  name,  by  the  way,  Eric?  Thank  you,  Eric. Let's  give  this  to  GPT  four. Create  this  website  for. Sorry,  there's  a  problem. I  opened  up  the  wrong  GPT  four. I  should  be  using  the  default one  rather  than Okay. I  don't  want  that  boring boiler  plate  that's  gonna  give  me, let's  just  take  a  look  at  this  for  a  second. So  it's  going  to  give  me  simple  structure  with  navbar, company  name,  header  and  description. Let's  make  sure  that's  what  we  wanted. Navbar  up  company,  we  didn't  ask  for  a  company  logo, but  it's  a  good  idea. Header  description,  I  think  that  pretty  well. So  there's  the  company  logo  in  there  that's  adding, It's  generating  the  code  very  slowly. I  have  to  say  this,  maybe they're  under  a  very  high  load  right  now. Okay,  thank  you  for  clarifying  that. So  let's  ask  what's  the  simplest  way  to  test  this  out? So  I'll  just  mention  that  I feel  like  my  decision  to  never learn  HTML  and  CSS  is  a  good  one. That  sounds  too  complicated. Maybe,  maybe  it's  okay. Well,  let's  say king, so  let's  copy  this. What  did  it  say  it  was  on  the  left, say  the  HTML  thing  was  on the  left  hand  side  according  to  the  HTML  section. Didn't  know  where  we're  on  the  side.  It  was  okay. Let's  okay,  it  has  home  about  us  company  logo, contact  us  all  good  company  however it  is.  Let's  see  if  I  can. It  was  missing  a  little  bit  here. The  page  should  be  divided  into  two, so  will  be  able  to  there's  some  CSS  here. Is  that  right? Will  this  be  able  to  read  CSS if  I  paste  it  into  this  section? Could  that  be  the  problem? Yeah,  CSS.  Oh,  I  see. Okay.  Okay.  Okay,  so  did  not  get  it  quite  right, let's  quickly  try  to  fix  it. King  is  that  correct? I'm  not  sure  about  that.  There's  no. Okay,  so  there  actually  seems  to  be an  issue  with  its  visual  perception  here. Oh,  I  see. I  judge  too  quickly,  it  sees  a  horizontal  line separating  company  name  and learn  more  headern  description. Good.  So  let's  make  sure  it  gets  fixed. That's  too  much  work. Just  give  me  something  to  copy. So  what  does  this  mean  for  us? Right,  It's  a  serious  question. I  don't  have  the  answers. In  all  seriousness,  none  of your  other  professors  on  campus  have  the  answers. And  the  reason  is  because  almost  none  of your  professors  know  that  this  is  possible. It's  a  remarkable  thing How  many  people  interested  in this  subject  have  not  actually  used  this  technology? I  think  it's  me, if  I  had  to  make  a  guess  from talking  to  many  people  across  campus. This  is,  it's  weird  and  scary, but  if  I  told  you  before  this  quarter  started, certainly  if  I  told  it  to  you  like  six  months  ago, you  would  have  thought  that  I  was  crazy. And  yet  here  we  are.  So  what do  you  do  with  this  information? I  don't  have  the  answers  to  that. I  can  create  web  pages  now. Without  knowing  anything  about  how  to  create  web  pages, That  seems  like  a  pretty  big  deal. Yeah,  yeah.  What  would  I  do? I  would  think  hard  about which  skills  these  systems are  not  going  to  be  able  to  automate. They're  not  technical  skills,  unfortunately. Uh,  it's  like  dealing  with  people  there, parts  of  sales  that  this  will  help  with. But  a  lot  of getting  money  from  rich  people  is still  going  to  be  a  face  to  face  thing. For  a  very  long  time. Yeah. It  dealt  with  the  lawyer.  Yeah,  that's  a  technical  thing. You  helped  me  massage  my  message. That's  what  I  mean  is  dealing  with  people, there's  always  going  to  be  a  bottleneck  which  is the  final  interactions  are  going  to  be  in  person. So  you're  going  to  need  a  person  there, people  who  are  really  good  at  that  last  mile. That's  going  to  be  very  valuable. I  think  having  understanding  of technical  material  will  still  be  valuable  because as  knowing  that  tech  is  not  magic, that  will  be  very  useful. Let's  see  if  it  got  it  right.  Yeah.  Okay. It  did  put  in.  The  main  problem  here is  that  should  have  been  it  missed. The  fact  that  this  part  be centers  minor  detail.  We  could  have  it  fix  it. Okay.  We  can  create  simple  web  pages. We'll  try  to  push  this  capability maybe  in  some  future  classes. Yeah,  technical  skills  I  have a  feeling  are  not  going  to  be  right  now. There  are  a  good  barrier  to  getting  a  safe  job. Basically,  getting  a  secure  job, that's  not  going  to  be  sufficient. We  need  to  know  this.  It's  true  for  me  as  well, to  the  extent  that  reputation,  or  whatever  it  is, is  built  on  my  understanding  of technical  knowledge,  that's  not  enough. Okay, let's  continue  talking about  stochastic  gradient  descent. I  believe  that,  by  the  way,  I'll  just  mention, I  think  all  of  you  should  have  access  to the  GPT  four  vision  feature  now. I  mean,  I  think  I  was  like  the  very  last  person in  the  world  to  get  access  to  this. And  you  can  see  how  I  spent  my  weekend. Yeah,  it's  a  lot  of  fun  to  play  with. The  remarkable  thing  about  it  is  you  might think  that  there's  something  special  to  it. By  which  I  mean  like  you  think,  okay, there's  processing  language,  there's  do  vision. And  those  must  be  two  different  things. One  of  the  major  discoveries  of  the  last  couple  of years  is  that  you  can  turn  vision, what  GP  is  doing  right  now, into  basically  language  modeling. That  sounds  crazy,  but  the  vision  system, we  don't  know  the  exact  details, they  haven't  released  it  publicly. But  we're  pretty  sure  the  vision  system that  they're  using  is  basically a  language  model  where  you  first  basically  turn the  image  into  words  and  then  you  process  those  words. There  are  special  types  of  words. Maybe  at  the  end  of  the  quarter we'll  learn  about  how  that  works, but  you  turn  the  image  into  words  and  then  you process  the  image  just  like  any  other  sequence  of  words. That's  crazy.  That's  how  the  system  works. Okay,  Sarcastic,  grading  descent. We  have  a  loss  function. We  have  a  loss  function  on  our  dataset D.  It's  the  loss of  a  parameter  vector  theta  on  a  dataset  D. This  is  equal  to  a  sum  over the  dataset  of  the  loss  on  the  individual  data  points. Here  is  a  vector.  What  should get  rid  of  the  minus  sign  there? We  have  a  sum  over  the  losses on  the  individual  data  points. The  loss  is  equal  to  the  negative  log  probability of  the  data  given  our  parameter  vectors  data. I'm  assuming  we  have  a  dataset  D. It's  a  set  of  individual  data  points,  D, I,  we're  trying  to minimize  the  loss  that  turns  into maximizing  the  log  probability  of  the  dataset. We  need  to  compute  gradients. Last  time  we  saw  that  we  can compute  the  gradient  of  the  loss, which  measures  how  does  the  loss  change. When  we  change  theta  by  a  very  tiny  amount, the  gradient  distributes  over  a  sum. So  this  is  equal  to  a  sum  of  gradients. Gradient  descent  says, I  should  say  what's  called  full  batch  gradient  descent. Normal  gradient  descent  says  look  at  your  entire  dataset. Compute  the  gradient  of  the  loss, take  a  direction  opposite  that  gradient, and  then  update  your  parameters  with that  new  updated  pat  we  saw  last  time. That's  extremely  inefficient. If  you  have  a  dataset  that has  100  billion  documents  in  it, you  have  to  loop  over  100  billion  documents or  10  billion  documents  in order  to  compute  the  gradient  there. That's  crazy,  It  will take  you  millions  of  dollars  to  do  that. For  a  large  model,  we  don't  want  to  do  that. What  we'd  like  to  do  instead  is  take  many  steps, where  each  step  is  based  on  only  a  subset  of  the  data. What  does  stochastic  gradient  descent  say? Let's  not  call  that  stochastic  gradient  descent. Let's  call  this  just  gradient  descent. Stochastic  gradient  descent, SGD Says  approximate  the  gradient. So we're going  to  take  a  sample, it's  a  strict  subset  of  our  dataset  D.  And  then compute  the  gradient  on. How  do  we  do  that? We're  going  to  take  the  gradient of  the  sum,  let's  call  it. We're  going  to  say  D  belongs to  sum  over  all  data  points  in our  subset  of  the  loss  on  those  data  points. And  that's  equal  to  a  sum of  the  gradient  on  those  data  points. We're  just  pushing  the  gradient  inside  here. We're  going  to  run  gradient  descent on  these  approximate  gradients. We  wrote  down  the  full  algorithm  last  time. I'll  refer  you  to  that  algorithm  if  you want  to  review  the  exact  thing. But  basically  we  are  going  to split  our  dataset  randomly  into  sub  samples. We're  going  to  have  subsample  one, subsample  two,  subsample  three. They're  going  to  cover  the  entire  dataset. We  compute  the  gradient  on  subsample  one. Take  one  gradient  descend  step  From  that  approximation, compute  the  gradient  on  subsample  two. Take  a  step  from  that  approximation  and  we're  going  to keep  going  until  we've  gone  through  the  entire  dataset. I'll  introduce  one  piece  of  terminology, which  is  one  sweep, one  pass  through the  dataset  is  an  epoch. This  is  the  terminology, Remember,  we've  divided  our  dataset  into  pieces. Once  we've  gone  through  all of  those  pieces  of  the  dataset, we've  seen  every  single  data  point  once. That's  called  an  epoch. You  can  train  your  model  for  less  than  one  epoch. You  can  train  it  for  one  epoch, or  you  can  train  it  for  multiple  epochs. Subsection  disjoint. Disjoint.  Even  stronger  than  unique. Yeah,  potentially  non  unique if  there's  a  lot  of  repetitions  in  your  data. If  your  dataset,  if  it's 1  million  copies  of  the  same  data  point,  then  not  unique. But  yeah.  Any  other  questions  about  this? Yeah,  what  do  you  mean  by  that? How  many  epochs  you  run  for?  Is  that  what  you  mean? Let's  go  over,  we  talked  about  this  last  time, but  let's  say  we  have  our  dataset. It's  equal  to  D1d2,  up  to  D. Let's  assume  D  is  in  a  random  order. I  don't  want  to  have  to  deal  with  changing  around the  indices  we  need  to  randomize. We'll  think  about  why  that  is, it's  in  a  random  order  already. We're  going  to  have  a  set  of  batches. Each  batch  is  going  to  be  a, we're  going  to  have  a  set  of  batches, which  consist  of  subsets  of  the  data. One  is  going  to equal  D  one  up  to  D  B  where  is  our  batch  size? We  have  one  subset,  the  first  subset  of  the  data  S  one  is called  the  batch  S  one  is equal  to  D  one  through  DB  S  two  is  equal to  DB  plus  one  up  to  D  two. Each  batch  has  number  of  data  points  in  it. Then  we  go  up  to  K, where  this  is  the  final  batch  A. During  stochastic  gradient  descent, we  first  compute  the  gradient  on  S  one. We  update  our  parameters. Now  we  compute  a  new  gradient  on  S  two. We  update  our  parameters, we  go  all  the  way  up  to  SK. Once  we're  done  with  SK, we've  seen  every  data  point  in  the  dataset  exactly  once, assuming  they're  unique  data  points. At  that  point,  we're  finished  with  one  epoch  and we  can  continue  training  for  multiple  epochs  after  that. Now  your  question  is,  when  do  we  decide  to  train for  multiple  epochs?  Is  that  right? Great,  it's  a  fantastic  question. It's  something  that  comes  up  in  practice  all  the  time. People  think  about  what  are  the  pros  and  cons, or  what  are  the  situations  where  you'll want  to  train  for  multiple  epochs. Are  possible  cons,  possible  problems  with  that.  Yeah. More  money  for  training  for  multiple  epochs. Yes,  absolutely,  that's  true. There  was  a  hand  up  there. More  money  and  resources. That's  definitely  true,  yeah.  Fantastic. Training  for  too  long  can  result  in  overfitting. If  you've  seen  all  of  your  data  points  many, many  times,  you  can  start  to  memorize  your  data. Now  it  turns  out  in  deep  learning, memorizing  your  data  actually happens  even  on  a  single  epoch. And  somehow  it  doesn't  matter. Everything  you  know  about  overfitting has  to  go  out  the  window. When  you're  talking  about  deep  learning, there's  actually  the  world  expert  on  this  topic. I  don't  know  if  you  know  this,  he's  on  campus. He's  a  faculty  member  in  data  science. Do  people  know  this?  Okay. The  world  expert  on the  weirdness  of  overfitting  or  lack  of  badness, of  overfitting  and  deep  learning. His  name  is  Misha  Belkin. You  should  take  his  classes  if  you  have  the  opportunity. All  overfitting  still  exists in  deep  learning  is  just  weird. It's  not  what  you  learn  in your  other  data  science  and  statistics  classes. If  they're  not  deep  learning  classes, we'll  probably  talk  some  about  that. But  yes,  it  can  lead  to  overfitting, where  you  see  your  data  too  many  times  and you  start  to  memorize  it and  you  don't  generalize  anymore. That's  a  bad  thing.  We've  seen two  bad  things  about  training  for  multiple  epochs, cost  and  overfitting.  Why  might  you  do  it? There's  one  very  practical  reason  why people  train  for  multiple  epochs  in  practice. Yeah,  I  guess  like  at  least  in  the  case  of  something like  when  you're  over  data,  the  data, all  languages  100%  agree  with  that. And  I  have  100%  disagree  with  that. And  I  don't  know  how  to  reconcile  those  two  things. The  reason  I  agree  with  that  is  you're  right. It's  like  everything  that  anyone's  ever  written, anything  that  anyone's  ever  written  that  people thought  to  keep  around like  that  other  people  would  care  about. That's  what  GP  four, that's  what  next  year's language  model  will  probably  be  trained  on. Four  is  maybe  not  quite  there,  but  okay. So  yeah,  you've  seen  lots  and  lots  of  valuable  stuff. It's  probably  not  so  bad  to  just  memorize  that the  combinatorics  of  language make  that  a  little  bit  implausible  though. So  how  many  sentences  or how  many  texts  of  length  $1,000  possible  texts. But  let's  just  give  a  ballpark  estimate how  many  words  in  English  are  there. It's  like  50,000  or  so,  something  like  that. Now,  not  every  combination  of  words  is  grammatical, but  many  combinations  of  words  are  grammatical. Not  only  are  many  combinations  grammatical, if  you  give  GPT  four, basically  an  arbitrary  combination  of  words, it  will  be  able  to  detect  for  you,  is  this  grammatical? What's  the  closest  grammatical  sentence? What  might  you  mean?  It's  able  to  do  that for  basically  any  text  of  length  1,000  Not  quite, but  getting  close  there. How  many  texts  of  length  1,000  are  there? It's  50,000  raised  to  the  1,000 That's  a  lot  bigger  than  the  10  trillion  tokens that  GPT  four  was  trained  on,  unimaginably  bigger. That's  the  reason  why  I  agree  with  you. I  have  to  disagree.  I'm  internally confused  about  this  issue. There's  clearly  a  lot  of generalization  that  the  moles  are doing  even  beyond  what  they're seeing  in  their  training  set. Now  I'll  mention  also  was  only  trained  for a  single  epoch  and  that's  because they  have  a  lot  of  10  trillion  words. It's  very,  very  expensive  to  train  four. Let  me  take  that  back.  We  don't  know  that  was  trained. They  may  have  trained  it  for  a  small  number  of  epochs. Most  language  models  before GB  four  were  only  trained  for  a  single  epoch. B,  four  might  have  been  trained  for  like  three, but  it's  either  one  or  a  very  low  number. Why  might  you  in  practice  as  NLP  practitioners, you  go  into  an  industry  job, you  want  to  figure  out  how  many  epochs  train  for. There's  one  factor  that  is  going to  generally  answer  this  for  you. At  least  give  you  your  first  guess  of how  many  epochs  to  train for  or  should  you  do  more  than  one. Any  guesses? It  is  a  very  simple  heuristic that  you're  going  to  look  at. How  big  is  your  dataset? If  you  have  a  really,  really  tiny  dataset, that's  all  the  data  that  you  have and  you  might  have  to  train  for multiple  epochs  and  just  cross your  fingers  that  there's  not  too  much  overfitting. You  need  to  squeeze  as  much out  of  that  dataset  as  possible. Okay.  Any  other  questions  about  epochs? Yeah,  yeah,  that's  right. Yes  the  way  that. No,  it's  not  that  it's  scrambled  after  each  batch. It's  scrambled  after  each  epoch. Yeah,  you  permute  your  dataset when  you  see  it  the  first  time, then  you're  done,  and  now  your  dataset. Any  others?  Really  great  questions. Yeah,  the  dataset  size  effect, the  learning  rate  you  should  use. I  don't  know  of  any.  I  could give  a  heuristic  argument  for  why. Let  me  get  to  that  in  a  little  bit. There's  another  factor  that  is probably  more  relevant,  but  it's  a  good  question. Yeah. Okay.  All  right,  so  we  go  through  for  one  epoch. Good.  Now  let's  talk about  the  significance  of  the  batch  size. The  batch  size  is  how  many  points  we're  going  to use  for  computing  the  gradient  at  any  one  time. Let's  say  that  we're  looking  at  the  first  batch  here. We  have  the  gradient  of  the  loss  on  one. This  is  equal  to  a  sum  from  I  equals  one  to  B, which  is  the  batch  size  of  the  gradient  of  the  loss  on. I'm  looking  at  B  data  points computing  their  Ls,  summing  them  up. Let's  actually,  let  me introduce  a  slight  notational  variant  here. It's  just  going  to  be  a  re  scaling, so  I'm  going  to  introduce  this  factor  one  over  here. Okay,  let's  see, where  else  can  I  introduce  this  factor  over  here? It's  going  to  be,  I want  to  divide  everything  by  the  sample  size,  okay? It's  just  a  re  scaling. It's  going  to  make  our  lives a  little  bit  simpler  in  terms  of explaining  what's  going  on  over  there. One  over  here  and  one  over  n  here. I'm  dividing  the  loss  by  the  size  of  our  dataset, or  the  size  of  the  sample of  the  dataset  that  we're  looking  at. I  missed  one  over  here, so  I  got  all  of  them. Let's  talk  about  the  relationship  of  this. The  average  loss  on B  data  points  that  have  been  randomly  sampled. I  want  to  talk  about  the  relationship  of this  thing  to  this  thing  over  here. This  is  the  average  gradient  on  the  entire  dataset. I  have  the  average  gradient  on  a  subset  of  the  dataset, a  random  subset,  the  average  gradient on  the  entire  dataset. What's  the  relationship  between  those  two  quantities? Or  what  can  we  say  about  the  relationship? Yeah,  a  partial  average. That's  correct. I  could  take  all  of  these  partial  averages, take  their  average,  and  I'll get  back  that  term  over  there.  That's  correct. Now,  that's  not  what  happens  in  practice  because we're  updating  the  parameters every  time  that  we  take  the  gradient. It's  actually  that  we  compute  here, we  update  the  parameters  and  now  we're going  to  get  a  new  gradient  on  two. That's  exactly  how  the  math  works  out. I  want  to  know,  just  looking  at  a  single  batch. Yeah.  What's  the  relationship, under  what  conditions  will  there  be  almost  equal? Look,  you're  on  the  right  track. Okay,  That's  certainly  true. As  B  gets  closer  to, let's  say  let's  said the  batch  size  equal  to  N  the  entire  dataset, then  those  things  will  be  equal. Yes.  I'm  sorry. If  I  assuming  it's  randomized? We're  always  assuming  it's  randomized. What  happens  if  B  is  equal  to  one? Let's  assume  it's  randomized. So  we  take  a  single  data  point, the  average  gradient  on a  single  data  point  is just  the  gradient  on  that  data  point. Is  there  any  relationship  between  that  thing and  the  average  loss  on  the  entire  data? The  average  gradient  of  the  loss  on  the  entire  dataset. The  answer  to  that  is  not  necessarily a  single  data  point. Its  gradient  may  be  very  non  representative of  the  entire  batch  or  the  entire  dataset. Think  about  this  as  trying  to  compute  the  average  height, United  States,  by  taking  random  samples  of  people. We're  going  to  sample  some  number  of  people  from the  United  States  at  random compute  their  height,  compute  its  average. What's  the  relationship  between  our  sample  size and  the  true  average  height  of  people  in  the  US. If  I  take  one  person, maybe  I  get  someone  by  chance  who's  seven  feet  tall and  I'm  going  to  get  a  very  bad  estimate  of  the  average. If  I  take  100  people  might  not  be  bad. I'm  going  to  miss  some  outliers. Depends  on  exactly  what  the  shape  of the  distribution  is,  whether  it's  normal  or  not. And  maybe  we  don't  know  ahead  of  time. If  I  take  100,  it's going  to  be  definitely  better  than  taking  one, and  it's  probably  not  going  to  be  bad. If  I  take  1,000  that's  getting  pretty  good. If  I  take  10,000  at  that  point, I  actually  think  I'm  going  to  get a  very  accurate  estimate. If  I  get  10,000  random  people  from  across the  US,  sum  up  their  heights, take  the  average,  that's  probably  going  to  be a  very  good  estimate  of  the  average height  of  everyone  in  the  US. But  think  about  the  savings  that  I  got  from  taking 10,000  versus  350  million  the  population  of  the  US. That's  a  factor  of  10,000 difference  more  than  that  in  cost. That's  the  same  principle  here. When  we're  doing  stochastic  gradient  descent. We're  using  the  subset  of data  points  to  get  a  random  estimate, just  like  our  random  sample of  people  in  the  United  States. A  random  estimate  of the  true  gradient  on  the  entire  dataset. What  we  want  to  do  when we're  choosing  a  gradient,  when  it's  possible, at  least  what  we'd  like  to  do  when choosing  a  batch  size  is  find  the  smallest  batch  size. Such  that  when  you  take  an  estimate  like  this, a  random  estimate  of  the  gradient, it's  not  too  far  off  from  the  true  gradient. I'll  just  mention  this  for  people  who  are interested  that  batch  size  has  a  name. It's  called  the  Critical  Bat  Size. This  is  beyond  what  we're  covering, but  it's  a  useful  concept  to  have, at  least  for  everyone,  even  if you're  not  like  implementing  it  in  practice. The  critical  bat  size,  it's  the  smallest  batch  size where  your  random  estimate  of  the  gradient  is  close to  the  true  gradient in  general  is  much, much,  much  smaller  than  your  entire  dataset. For  GPT  four,  we  don't  know. Let's  say  for  other  large  language models  where  your  corpus  is 100  billion  or  1  trillion  words, a  usual  batch  size  will have  1  million  to  10  million  words  in  it. Basically,  it's  close  to  a  factor  of  1  million  smaller. Than  your  entire  dataset. Somewhere  between  a  factor  of 10,000.1000000  smaller  than  your  entire  dataset. It's  big,  big  savings  that  you  get.  It's  for  free. Right.  What  the  critical  batch  size means  is  that  if  you  choose  the  critical  batch  size, you're  getting  as  good  of  an  estimate  of the  entire  gradient  as  if  you just  did  the  entire  dataset. This  is  why  polling  works  like  pulling  for  elections. It's  the  same  principle.  Any  questions? Okay, let's  talk  about  some  trade  offs  here. For  practical  reasons, you  may  not  know  what  the  critical  bill. Let's  talk  about  reasons  why  you  may  not make  that  number,  that  critical  batchize. There  are  a  number  of  good  reasons. Here's  the  most  practical  one, which  is  actually  it's  pretty hard  to  compute  the  critical  bai. A  lot  of  interest  involves  experimentation. It  can  be  difficult  experiment. Yeah,  it's  always  easy to  compute  that  thing  of  interest,  trial  and  error. Another  issue  is  that your  computer  may  not  be  able  to  hold a  dataset  that's  as  large  as the  critical  batch  size  in  memory  at  once. So  that  means  you  cannot  actually process  a  sample  that  large  at  once. What  happens  when  you  go  beneath  the  critical  batch  size? Your  base  is  such  that  this  is  no  longer a  great  estimate  of  the  true  gradient. Imagine  that  we  only  sample  100  people  from  the  US. And  average  their  height  rather  than  10,000  people. What  happens  as  a  result  of  choosing 100  rather  than  10,000  What  are  we  doing? Yeah,  it  will  be  more  skewed. That's  right,  Skewed  might  have  a  technical  meaning. What  Is  there  a  more  generic  way  to  say  that? Great,  I  might  get  more  tall  people, I  might  get  more  short  people. In  general,  I'm  going  to  expect a  larger  difference  between my  sample  and  the  true  gradient. Another  way  of  saying  that  is  I'm injecting  more  noise  into this  process  every  time  that  I  take  a  gradient  update. Is  a  gradient  update  look  like? Gradient  update  says  take your  current  theta  and  subtract  epsilon, which  is  the  learning  rate. It's  our  L  R,  it's  our  learning  rate, it's  minus  epsilon  times  the  gradient. Let  me  include  that  one  over  B  factor  here. If  B  is  very  small, then  this  quantity  may  not  equal the  true  gradient  for  the  data  set. My  gradient  update  will  be  noisy. The  theta,  I  get  a  new  updated  theta  from this  theta  may  be  for  at  least  a  little  f, a  little  far  from  where  I would  go  with  a  correct  gradient  update. Let's  summarize  that  lesson  here. Smaller,  that  size  means more  noise  in gradient  updates  and  estimates, what  happens  as  a  result  of  having  more  noise? You  brought  up  overfitting  before  you  said,  okay, I  look  at  my  dataset  many,  many  times, I'm  going  to  get  overfitting  where  I start  to  memorize  all  of  the  data  points. What's  the  relevance  of  a  small  batch  size  to  that? Okay.  There's  one  intuition  that small  batch  sizes  might  give  us  more  overfitting. Let  me  think  about  it  from  another  way. What's  our  ultimate  goal? Our  ultimate  goal  is  to, our  ultimate  goal  is  to minimize  the  loss  on  our  data  set. We  want  to  find  the  theta  that  minimizes  the  loss. The  loss  is  equal  to  the  some from  I  equals  one  to  n  of the  negative  log  probability of  a  data  point  given  theta. Let's  think  about  it  from  this  perspective. We're  trying  to  find  a  theta  that maximizes  the  probability  of  our  dataset. Now,  what  does  memorization  mean  in  this  context? It  means  basically  that  we  assign  every  data  point, like  the  data  points  that  actually exist  in  the  data,  in  the  dataset. Very,  very  high  probability. It  means  we  assign  any  data  points that  are  not  in  the  dataset. Let's  say  very,  very  low  probability. Imagine  that  we  have  1  million  sentences  in  our  dataset. Overfitting  in  that  data. The  most  overfitting  that  you  could  have  would say  each  of  those  sentences  gets probability  1/1000000 Every  other  possible  sentence  gets  probability  zero. That  would  be  what  overfitting  means. Here,  memorizing  the  dataset, we  assign  as  high  a  probability as  possible  to  the  things  that  we  see, no  probability  to  anything  else. How  injecting  noise  into  our  gradient updates  affect  our  minimization  of  the  loss  function. Yeah,  good. It  will  lead  to  underfitting can  relate  it  to  this  optimization  issue. So  I'm  trying  to  understand  how  the  connection between  this  noise  and  optimization, you're  absolutely  on  the  right  track. Yes,  exactly. We're  not  going  to  be  minimizing our  loss  function  as  effectively  anymore. Let's  think  about  it,  very, very  simple  visualization  here. So  what  does  gradient  noise  mean  in  this  context? Let's  think  about  this  in  an  extreme  way. We're  at  this  point,  we  want  to know  should  we  move  right  or  should  we  move left  gradient  Noise  Sometimes you  think  you  should  move  left. You  think  the  gradient  points  in that  direction  or  the negative  gradient  points  in  that  direction. What  happens  when  you  think  that  it means  you  move  farther  away  from your  minimum  rather  than  closer? Yeah,  everyone. And  that's  going  to  happen  randomly. Every  time  that  you  sample  a  new  batch, you're  going  to  get  new  types  of  noise. Maybe  they  average  out,  maybe  they  don't. Really  depends  on  what  the  loss  function  looks  like. Now  the  loss  function  is  this, incredibly  complicated  things. Not  one  D,  it's  two  D, 10  trillion.  It's  complicated  thing. You  can  have  all  weird  shapes. You  get  less  minimization  of  your  loss  function, let's  say  less  effective  minimization  of  loss  function. Now  what's  a  practical  consequence  of  this? It's  that  if  you  see  your  model  fitting, you  oftentimes  encounter  this, your  model  is  overfitting. You  have  all  these  hyper  parameters  that  you  can  tune. Let's  think  about  what  are  hyperparameters. Here  we  have  two  knobs  that  we  can  turn  so  far, one  of  which  is  the  B  size, one  of  which  is  the  learning  rate. We'll  get  to  that  one  in  a  second. Before  we  start  training  our  model, we  can  turn  these  knobs. Those  knobs  are  called  hyperparameters. We  see  our  model  overfitting. It  looks  like  it's  memorizing  the  data  set. When  we  show  it  something  new that  was  not  in  our  training  set, it  fails  miserably  at  that. One  thing  that  we  can  try  to  do  is  lower  our  batch  size. We  want  our  model  to  do  as  funny  as  this  sounds, we  want  our  model  to  be  less effective  at  minimizing  our  loss  function. We'd  like  it  to  find  something that's  maybe  not  quite  at  the  minimum. Maybe  it's  better  than  where  it  started, but  not  quite  at  the  minimum. Larger  batch  size.  We  can  think  about  it  in reverse  if  we  find  that  our  model  is  underfitting. Where  underfitting  means  even  on  our  training  set, we're  doing  very  badly on  the  actual  data  that  we're  seeing. The  loss  is  still  very  high and  maybe  it's  not  decreasing. What  do  we  do  in  response  to  underfitting? We  can  increase  the  batch  size  and  get a  less  noisy  estimate  of  the  gradient, which  will  help  us  to  get, do  better  at  minimizing  the  loss  function. We've  talked  about  batch  sizes. Let's  look  at  a  second  which is  intermingled  hyper  parameter  here, which  is  the  learning  rate  epsilon. Let's  imagine  that  we  choose an  extremely  large  learning  rate. Let's  go  back  to  our  example  over  here. Let's  say  that  we  accurately  compute the  gradient.  The  gradient  looks  like. This  gradient  descent  does not  tell  us  how  big  of  a  step  we  should  take. Depending  on  what  we  set  epsilon  to, we  can  either  take  a  very  small  step  over  here, or  we  could  take  a  large  step. Potentially,  we  could  take  a  step  all  the  way  over  here. If  we  say  our  learning  rate  large  enough, if  our  learning  rate  is  very  large, we  will  end  up  taking  too  large  of  a  step. The  gradient  gives  us  a local  approximation  to  the  function. The  gradient  says,  here's  how  the  function behaves  in  this  small  region. If  we  take  a  large  step, that  approximation  is  no  longer  true  anymore. Here's  one  way  to  think  about  it. This  function,  it's  a  parabola. It's  approximately  linear  in the  small  region  of  this  point. As  you  start  to  move  away  from  this  point, you  see  it's  a  curve,  it  starts  to  bend. That  linear  approximation  breaks  down. If  you  take  too  large  of  a  step, the  approximation  that  you  had from  the  gradient  is  now  bad, it  doesn't  tell  you  anything  anymore. And  you  may  end  up  in  a  region that  has  actually  higher  loss  than where  you  started  learning. Bad  size  and  learning  rate  are  probably the  two  most  important  hyperparameters  that  we  can  tune. The  two  most  important  knobs  that we  can  tune  when  we're  training  models. Let's  talk  about  what  happens  when  we  choose  a  L  R. What  is  a  smaller  learning  rate  lead  to? Yeah,  Okay. It's  going  to  lead  to  potentially, let's  be  a  little  bit  more  objective about  it.  Smaller  steps. Smaller  steps. What  else? At  least  the  smaller  steps. What's  related  consequences  of  this? You're  saying  slower  learning, that  ones  a  little  tricky. So,  yeah,  so  it  may  take, that's  related  to  the  suggestion  over  here. You're,  it'll  take  more  epochs  potentially  to  train. It's  like  the  tortoise  and  the  hair,  right? When  you  take  very,  very  tiny  steps, you're  basically  guaranteed  to  get  to the  minimum  or  at  least  a  local  minimum. If  you're  careful  to  take  very  tiny  steps  here, you'll  eventually  reach approximately  the  minimum  down  there. Let's  call  it  more  likely  convergence  to  a  minimum. Mm. What's  the  connection  between  this and  overfitting  and  underfitting? We  can  think  about  this  very  much  in the  context  of  batch  size  as  well. You  have  a  smaller  LR, you're  going  to  take  really,  really  tiny steps  to  find  a  minimum. It's  going  to  be  slow  potentially, but  it  will  get  there  if  you  wait  long  enough. Are  you  more  likely  to  overfit  with a  large  learning  rate  or  a  small  learning  rate, assuming  you're  willing  to  wait  long  enough. So  remember,  overfitting  basically  means,  you  know,  it's, it's  caused  by  driving your  training  loss  as  low  as  you  can, basically  like  in  some  circumstances driving  your  training  loss  as low  as  you  can  will  lead  to, will  lead  to  overfitting. So,  what's  the  connection  here? Yeah,  sorry, you  might  miss, right. Smaller  learning  rates  in  general  lead  to more  overfitting  as  you're more  effective  at  optimizing  your  loss  function. The  way  that  deep  learning  people  think  about  this, I'm  giving  you  intuitions  right  now, actually  connecting  this  in a  rigorous  way  to  math,  basically. I  think  nobody  knows  how  to  do  that  right  now. The  situation,  okay? Alchemy  gets  a  bad  name. Alchemy  was  chemistry  before  chemists  had  any  theories. People  for  hundreds  of  years  were  doing chemistry  under  the  name  of  alchemy. They  discovered  lots  and  lots  of  good  stuff. Like  a  lot  of  the  protocols that  used  once  chemistry  was  actually  discovered, they  were  just  straight  from  alchemy. The  alchemists  believed  a  lot  of good  stuff  and  they  also  believed  a  lot  of  nonsense. They  had  no  theory  that  connected  what  they  knew. They  knew  a  lot  of  practical  stuff, but  they  just  didn't  know  why  any  of  it  worked. I'm  teaching  you  alchemy  in  that  sense. In  this  class,  I'm  giving  you  intuitions. That's  what  they  are.  Okay,  so we  can  say  that  the  bat  size  and learning  wraith serve as  regularizers. This  is  how  people  in  deep  learning  think  about  this. Regularization  is  a  general  term  from  statistics where  basically  it's  a  way  of preventing  your  model  from fitting  your  training  data  too  well. That's  what  the  goal  of  regularization  is. You  want  to  make  your  model  as  simple  as  possible. You  want  to  prevent  it  from  wrapping  around all  the  data  and  just  sucking  it  all  up. Lower  size  is  a  regularizer, Prevents  your  model  from  overfitting  too  much. Higher  learning  rate  is  a  regular  riser. Prevent  your  model  from  overfitting too  much  depending  on, let's  say  a  little  bit  more  about  this, that  size  learning  weight  decay, which  we'll  talk  about  it  at  some  point. Model  architecture, dropout  rate, am  I  missing  anything? There's  some  others.  I'll  write  them  under,  et  cetera. There's  a  bit  of  a  list.  These  are, I'd  say,  probably  the  most  important  ones. These  are  all  hyperparameters. What  is  a  hyper  parameter? A  hyperparameter  is  anything that  cannot  be  optimized  using stochastic  grading  descent  that's  subtle. Here's  one  way  of  thinking  about  it.  We're  gods. When  we  do  deep  learning, we  design  the  overall  system. We're  going  to  by  hand  craft  a  bunch  of  stuff Before  we  start  doing  any  deep  Lear're going  to  say  what's  our  batch  size, what's  our  learning  rate,  what's  our  weight  decay, what  model  architecture  are  we  using? We're  using  a  transformer  like  what's  used  in  GP  four. Are  we  using  a  recurrent  neural  network? Are  we  using  something  else? Convolutional  neural  network. We're  going  to  say  all  of  this  stuff  ahead  of  time. After  we  fixed  all  of  that  and  our  data  set, we're  just  going  to  turn the  Crancostocastic  gradient  descent. But  all  of  that  stuff  needs  to  be  decided  on before  we  can  start  turning  the  ranco  gradient  descent. That's  why  they're  called  hyperparameters  and not  parameters  like  theta  over  here. The  thetas  are  the  things  that  we're optimizing  with  grading  descent. The  hyperparameters  are  everything  else. That's  the  stuff  that  gets  decided  ahead  of  time. Now  we  have  a  bunch  of  hyperparameters. We  learned  about  two  of  them. They're  extremely  important. These  two  especially  are  important. This  one  can  be  important, but  you  don't  need  to  fiddle  with  it  too much  to  I'd  say  in  practice, the  two  most  important  ones  are  not. The  only  ones  are  the  two  most  important  ones. Once  you  decide  on  the  model  architecture, how  do  you  decide  what  hyperparameters  you  should  choose? Can  anyone  guess  the  state  of the  art  method  for  selecting  your  hyperparameters? Yeah,  try  with  different  ones. That's  correct.  That's  the  state  of  the  art. It's  random  guessing.  Now,  it  doesn't like  there's  nothing  more systematic  that  you  can  do  than  just try  a  bunch  of  stuff  and  see  what  works. That  being  said,  what  I'm  giving  you  are  some  intuitions. When  you  see  particular  types  of  failures  in  your  model. My  model  is  underfitting. The  loss  is  not  even  going  down  on the  training  data,  model  is  overfitting. It's  going  down  way  too  much  on the  training  data  and  like  it's  going  up, it's  not  going  down  fast  enough. On  my  test  set  or  my  validation  set. You  have  these  different  situations  that  you  encounter. These  are  the  knobs  that  I  would turn  first  when  I  encounter  those  situations. Any  questions,  I  assume these  days  everyone  is  familiar  with  training  set, validation,  set,  test  set  stuff. Is  that  fair?  I  mean, it  was  not  true  a  few five  years  ago  when  I  started teaching  this  class.  That  was  not  true. I  think  machine  learning  had  not  as thoroughly  permeated  the  curriculum  here. But  I  think  lots  and  lots  of  classes  talk  about  this. Also,  I'm  a  little  bit  worried  about  like  everyone, assuming  that  everyone  already  knows  this, because  they're  all  taking  machine learning  classes  all  the  time. Let's,  I  want  to  say,  okay, let  me  say  something  informative about  hyperparameter  selection. We'll  learn  about  these  concepts in  the  context  of  how  do  we  select  hyperparameters. We're  going  to  split  our  data set  into  three  parts  and  we  should  always  do this  training validation  and  test. The  purpose  of  the  test  set  is  very  clear. We're  going  to  train  the  model  on  the  training  set. We're  going  to  show  our  language model  1  billion  documents  from  our  training  set. We're  going  to  get  it  to  learn. Hopefully  the  structure  of those  at  the  very  least  is  going  to  be model  is  trying  to  minimize  the  loss  on  the  training  set. What's  the  purpose  of  the  test  set? It's  to  give  us  an  unbiased  estimate of  our  model  performance  on  a  new  data  set. We  imagine  ourselves  like  we've  trained  our  model. We  want  to  now  use  this  model  in  practice, release  it  in  a  product. We  want  to  know  how  well  is  this  model  going  to  perform. It's  already  been  trained  on  the  training  set. It  memorized  or  memorized  a  lot  of  the  data  there. That's  going  to  give  us  a  very biased  estimate  of  our  model  performance. We  need  to  get  a  fresh  sample  and tested  on  data  that  has  not  already  seen, That's  the  function  of  the  test  set. I'll  assume  people  are  basically familiar  with  that  concept. Why  do  we  do  this  three  way split  between  training,  validation,  and  test? What's  the  function  of  that  there? Yeah,  that's  where  we're going  to  tweak  our  hyperparameters. We're  going  to  be  trying, depending  on  where  our  budget  is, a  small  number  or  a  very  large  number of  hyperparameter  configurations. We're  not  going  to  test  all  those configurations  on  our  test  sets. We're  going  to  first  test  them  on  our  validation  sets. We're  then  going  to  select  the  one  that  has the  best  performance  on  the  validation set  that  will  be  used  for  testing  on  our  test  set. Why  did  we  do  this  three  way  split? We're  going  to  be  optimizing our  hyperparameters  using  our  validation  set. We'll  be  looking  at  what's a  performance  on  the  validation  set. Why  do  that,  didn't  we  just  directly? Let's  think  about  this.  It  seems like  a  reasonable  strategy. We  try  all  of  our  hyperparameter  settings. We  train  on  the  training  set  for  each  setting. We  then  see  how  well  does  it  generalize  on  the  test  set, and  we  choose  the  best  one. That's  our  model  performance. Why  don't  we  do  that?  Let  me  give  you  a  hint. Many  people  in  machine  learning  secretly  do  this. Even  if  they  say  that  they  don't. Why  should  we  do  this?  Should  you  feel guilty  If  you  get  anything  from  this  class, it's  that  you  should  feel  guilty  if  you  do  this. I  want  Professor  Bergen, a  little  guy  in  the  back  of  your  head, like  talking  to  you  next  time  that  you  do  do  this, why  should  you  do  it? Yeah,  it's,  that's  right. You  are  serving  as  the  training, as  basically  a  training  algorithm for  your  hyperparameters, and  you're  going  to  be  overfitting to  your  validation  set. There  will  be  a  lot  of  randomness  in  terms  of  what the  performance  of  different  hyperparameters are  on  the  validation  set. And  you're  just  going  to  be  selecting  for the  stuff  that  maybe  happens  to  randomly  do  better. It  will  give  you  an  upwardly  biased  estimate  of  how effective  your  model  is  when  it  generalizes. Yes,  that  really  depends  on  how  precise. I  wouldn't  say  there's  any  particular  ratio. It  depends  on  how  precisive  an  estimate you  need  for  your  generalization  accuracy. That's  not  really  a  function  of  the  dataset, the  absolute.  Let  me  say  it  this  way. If  you  have  100  data  points, every  single  one  of  those  data  points  is  precious. And  actually  you're  in  a  lot  of trouble  because  you're  not  going  to  be  able  to get  an  accurate  estimate  of  anything  from a  tiny  validation  set  or  a  tiny  test  set. Let's  say  you  have  10,000  points. Okay,  maybe  you're  going  to  allocate  500, 500  validation  and  500  test  because  that  will  give you  a  fairly  precise  estimate of  your  validation  and  test  performance. Let's  say  you  have  a  data  set with  100  billion  data  points  in  it. Do  you  need  5%  of  your  dataset  to  be  in  validation? No,  absolutely  not. Because  a  validation  set  that's  11000 that  size  will  still  give  you a  very  accurate  estimate  of  validation  performance. It's  not  really  about  the  ratio,  it's  more  about, think  about  how  this  number of  data  points  will  give  me  this  precise  an  estimate. That's  how  I  should  choose  these  things. It's  a  good  question.  I'd  say  there's a  heuristic  that  people  use  which is  five  to  10%  in  each  of  them. That  can  be  a  good  heuristic. But  you're  also  taking  away  data, maybe  crucial  data,  from your  training  set.  There's  a  trade  off  there. Any  others? Okay,  let's  send  here, I'll  see  everyone  on  Thursday.
