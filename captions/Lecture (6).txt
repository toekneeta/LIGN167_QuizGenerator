Gentle  reminder,  maybe  start  working on  your  final  project.  It's  not  too  early. This  final  project  is  very different  from  other  final  projects that  you  may  have  done  in  other  classes. And  actually  very  different  from  how  we've run  final  projects  in  this  class  before. Because  you  do  not  need  to  know the  material  in  this  class in  order  to  do  your  final  project. That's  a  very  interesting, it's  basically  like  another  set of  material  that  we're  teaching  you, which  is  how  to  build  systems  using  language  models. It's  just  a  donor  skill. I'm  trying  to  show  you some  prompting  techniques  in class  that's  like  my  contribution. But  a  lot  of  it  is  going  to  be  you  learning  how  to  do  it. One  of  the  reasons  why  I'm  not  showing  you  how  to build  applications  like  this  is  because  nobody  knows. Because  there's  just  no  precedent  for  this  technology. We  don't  know  how  to  build  good  applications. So  I'm  hoping  that  you  will  be  teaching me  something  through  experimentation. So  please  get  started  with  that.  Any  other  questions? Yeah. Photo, I  mean,  look, so  first  of  all,  it's  absolutely fascinating  that  Does  it  work? Is  it  able  to  actually  read  the  text? Yeah,  yeah. Okay.  So  this  is a  weird  situation  that  I didn't  anticipate  because  I  don't currently  have  access  to  the  image  interpreting  model. Hopefully,  I'll  get  access  to  it. So  for  right  now. Why  don't  you  paste  a  screenshot  of  your  interaction? I  yeah,  you  can  paste  the  link  to  that  as  well. I'm  very  curious  to  see  how  it handles  like  image  to  text  in  this  way. Do  you  know  why  that's  amazing? This  is  an  absolute  this is  the  first  model  that  I  think  is  able  to  do  that. We  definitely  the  first  model, It's  actually  integrated  with  something  that  can  also understand  language,  downstream  language  model. Do  you  know  why  that's  amazing? Because  of  PDF,  the  most  important  information  in the  world  is  locked  inside of  PDFs  that  are  not  machine  readable. It  seems  like  this  can  now  read  PDFs or  at  least  you're  providing  some  evidence  for  that. That's  going  to  be  amazing  as  far  as  like taking  all  of  this  scientific  content that's  out  there  that  all  of  these  publishers  have. But  it's  like  completely  locked  up. And  one  of  the  major  reasons  why it's  locked  up  is  just  because we  have  no  way  of  automatically  processing  it. We're  going  to  be  able  to  do  that  now. Okay,  let's  talk  about, let's  finish  up  logistic  regression. Remember,  logistic  regression,  we  have  a  model  that  says Probabilistic  model  of  our  data. We  have  a  data  set. Probability  of  the  data  is  equal  to the  product  of  individual  data  points, where  the  probability  individual  data  point is  given  by  this  formula. We  have  a  loss  function. The  loss  function  is  the  thing  that  we  want  to  minimize. The  loss  is  equal  to, let's  just  say  the  loss, how  bad  a  particular  parameter  vector is  given  the  data  set. That's  equal  to  negative  log  probability  of  given  theta. You'll  also  sometimes  see  this  written  equivalently as  this  quantity  where  we just  move  the  minus  inside  of  the  log. What's  our  strategy  for  optimizing  this  parameter  theta? In  order  to  minimize  the  loss? We're  going  to  take  the  gradient. There's  different  things  we can  try  to  do  with  the  gradient. But  we  computed  the  gradient  last  time, the  gradient  of  the  loss, we  let's  write  down  the  partial  derivatives. Partial  derivative  of  the  loss  with  respect  to the  parameter  in  the  weight  vector  is  equal  to this,  should  actually  be  okay. So  we  calculated  this was  the  partial  derivative  of  the  loss  function. Actually,  there  should  also  be  a  minus  sign  out  there. Partial  derivative  of  the  loss  function  with  respect  to the  parameter  in  the  weight  vector. One  strategy,  we  have  this  loss  function, we  want  to  find  the  best  set  of  parameters. One  strategy  for  optimizing  the  set  parameters  is  to  take all  these  partial  derivatives  and  set  them  equal  to  zero, and  then  solve  for  the  weight  parameters. By  the  way,  we  want  to  solve  for  the  W's,  right? We  want  to  find  a  set  of W's  such  that  this  partial  derivative, or  this  full  set  of  partial  derivatives  is  equal  to  zero. Where  do  the  W's  appear  in  this  equation? I'm  looking  at  this  equation right  now,  I'm  not  seeing  a  WI. But  that's  the  thing  that  we  want  to  solve  for. The  weights  are  the  thing  that  give  us  our  model. Where  could  they  appear  or  where  do  they  appear? So  I  could  go  about  trying  to  solve  that. Set  this  equal  to  zero, solve  for  all  the  W's. And  I  apologize,  this  should  actually, that  should  have  been  WJ.  Okay,  good. We  made  that  same  error  last  time. We  don't  want  our  indices  to  clash. Okay,  so  where  do  the  W's  appear  in  this  equation? Yeah,  exactly. They're,  they're  inside  of  here. So  we  wrote  down  this  equation  over  here. That's,  that's  where  the  W's  are. Why  don't  we  just  directly  solve  this  equation, or  solve  this  set  of  equations? We  did  it  for  linear  aggression.  It  was  pretty  easy. You  can  do  it  for  a  bunch  of  other  problems  as  well. Why  don't  we  do  it  here?  Well,  it turns  out  that  there's no  closed  form  solution  for  the  W's. You  can  try  to  solve  this  equation, and  we're  not  going  to  get  anywhere. You  end  up  basically  with  something  that's  circular, you  end  up  having  the  W's  In  terms  of  other  W, you  don't  actually  get  a  solution. What  we  would  want  to  find  is an  equation  where  on  one  side  of  the  equation  other W's  and  on  the  other  side  of  the  equation is  something  that  we  can  compute  from  our  dataset. So  we  can  turn  our  dataset  into  a  solution, into  an  optimal  solution  for  I's. You  can't  do  it  because  we cannot  directly  calculate  the  optimum. Here  we  go  to  find  an  approximate  solution. We're  going  to  find  a  way  of  approximating  the, of  this  loss  function. The  way  that  we're  going  to  do  that is  through  gradient  descent. We  saw  gradient  descent  a  little  bit  last  time. Show  hands.  How  many  people  have already  learned  about  grading  descent  before? Most  people  in  the  class? Good,  we'll  do  this  pretty  quickly. Let's  look  at  a  simple  function  like  this. We'll  write  out  of  our  function  over  here. Looks  something  like  that. We  start  out  at  an  initial  point, let's  say  it  is  negative  two. We  want  to  compute  which  direction  should  we move  in  order  to  decrease  this  function. This  is  a  one  dimensional  function. We  only  have  two  choices  for  the  direction. We  can  either  move  right  or  we  can  move  left. Obviously,  we  want  to  move  right. Let's  see  why  we want  to  do  that.  So  what  are  we  going  to  do? We'll  take  the  derivative  of  this  function, that's  equal  to  two  x  function  evaluated  at  negative  two. What's  it  equal  to?  It's  equal  to  four. What  does  that  mean? Where  does  that  information  tell  us? Yeah,  sorry,  negative  four. Thank  you.  Where  does  that  information  tell  us? Yeah,  so  the  slope  of  the  function  is  negative. Okay,  so  where  does  that  tell  us? But  what  is  this  negative  four  tell  us? I  just  want  to  understand  that  first  you're  correct. What  is  the  negative  four  tell  us? Exactly? Yeah.  Okay,  Right.  We're  always going  to  be  taking  a  linear  approximation. Everything  that  we're  doing,  we're  making  an  assumption which  is  not  exactly  true  but hopefully  is  approximately  true. That  if  we  move  one  unit  in, one  unit  to  the  right,  this  is  the  effect  of  that, we're  going  to  get  a  decrease  of  negative  four  units. Okay,  if  we  wanted  to  increase, we  would  in  the  opposite  direction, we  in  fact  want  to  decrease  here  though, this  gradient  is  actually telling  us  this  derivative  is  actually telling  us  what  direction  should we  move  in  to  get  the  biggest  increase. That's  another  way  of  interpreting  this, and  we'll  see  why  in  just  a  moment. This  is  telling  us  what  direction  should  we  move in  to  get  the  biggest  increase  in  the  function. This  is  saying  if  you  move  in  the  negative  direction, you're  going  to  get  an  increase. We  don't  want  to  get  an  increase, we  want  to  get  a  decrease. So  we're  going  to  move  to  the  right  gradient. Descent says  move  in the  opposite  direction of the  gradient  in  this  case. What  does  that  mean?  It  means  we're  going  to  do, let's  say  that  we  move  one  unit  to  the  right. It's  going  to  be  F  of  x  plus, let's  just  say  we  move  one, x  is  equal  to  negative  two, that's  equal  to  negative  one. What's  negative  one  equal  to  F  of  negative  one? That's  equal  to  what  we've  gotten  a  decrease. Good,  here's  a  question. If  I  wanted  to  be  able  to  predict what  value  I  was  going  to get  when  I  moved  one  location  to  the  right, I  was  starting  from. Uh,  x  equals  negative  two. I  wanted  to  predict  what  would  happen if  I  move  one  unit  to  the  right. How  could  I  have  done  that?  What's  the  equation  for  that? Yeah.  Okay,  I  can  draw,  right? So  I'm  going  to  look  at  the  tangent. Tangent  says  if  you're  at  negative  one, then  maybe  you're  somewhere  down  here. Let's  actually  see  what  that  looks  like. What's  the  general  formula? F  of  x  plus, what's  the  general  formula  for  approximating  this? It's  not  going  to  be  exactly  correct. What's  our  general  approximation  here? Yeah,  plus what  one  I  meant. This  is  supposed  to  be  a  y. So  does  that  change  your  answer? Okay,  you  close. I  move  units  to  the  right  or  in  some  direction. I  mean,  Y  could  be  negative, so  I'm  moving  y  units  either  to  the  right  or  to  the  left. How  do  I  compute  my  prediction  for  what  I'm  going  to  get? Yeah,  okay,  y  times  f  prime  of  x. Great.  I'll  write  it  like  this. F  prime  of  x  times  y. Let's  see  what  this  was  going  to  give  us. We  found  out  that  when  we  moved  one  unit  to  the  right, we  ended  up  getting  the  answer  one. What's  our  prediction?  This  is the  justification  for  doing  gradient  descent. Let's  see  if  x  is  equal  to. We'll  set  x  equal  to  negative two  of  what's  F  of  negative  two? F  of  negative  two  was  equal  to four  F  prime  at  negative  two. What  was  that  equal  to?  It  was  equal  to  negative  four. Then  y  was  equal  to  one, that's  equal  to  zero. This  was  optimistic. What  this  is  telling  us  is that  the  gradient  did  give  us  some  good  information. In  this  case,  it  said, moving  to  the  right  is  going  to  get  you  a  decrease. It  was  correct  about  that.  However,  the prediction  about  how  much  of a  decrease  it  would  get  us  was  optimistic. It  predicted  a  much  larger  decrease than  what  we  actually  got. Any  questions? Okay.  Okay,  I'm  at  negative one  now  in  my  function  is  equal  to  one. What  do  I  do  from  this  point? So  I  have  a  new, my  x  updated  is  equal  to  negative  one. What  do  I  do  to  decrease  the  function  further? Yeah,  check  the  derivative  again. F  prime  at  x, so  it's  going  to  equal  negative  two. That  says  in  order  to  get  a  decrease, I'm  going  to  keep  moving  to  the  right. Okay?  And  we  could  do  exactly  the  same  thing  again, if  I  moved  one  unit  to  the  right. In  this  case  I'm  actually  going  to hit  the  exact  minimum  of  the  function. That's  good.  I  will  still  be  overshooting. However,  in  the  sense that  my  gradient  prediction  will  still  overshoot, it's  going  to  be  overly  optimistic. But  I  will  in  fact  hit  the  minimum  of the  function  if  I  move  one  unit  to  the  right. Okay,  Basically,  grading  descent in  this  case  is  extremely  simple. Grading  descent  is  extremely  simple  in  any  cases  that we  can  actually  explain  in  class. The  funny  thing  about  neural  networks is  going  to  turn  out  that  we're still  going  to  use  gradient  descent  for  these  functions, and  we're  not  going  to  have  any  idea  why  it  works. I'm  going  to  be  pretending  like  these  simple  functions are  actually  teaching  us  about what  we're  doing  in  this  class. We're  all  going  to  go  along  with  this  charade, but  it's  not  going  to  be  true. But  that's  the  position that  poor  professors  like  me  are  in. It's  not  that  I'm  hiding  anything  from  you. No,  nobody  knows  grading descent  works  beyond  simple  cases  like  this. We'll  talk  a  little  bit  more  about  that. Let's  give  a  little  bit  more  general  justification for  why  we're  doing  gradient  descend. In  this  case,  it's  obvious  in the  sense  that  like  the  derivative, it's  a  slope,  right?  A  scalar. If  the  scalar  is  negative and  you  want  to  minimize  your  function, you  want  your  function  to  decrease. You  move  to  the  right. If  your  gradient  is  positive, the  scalar  is  positive, then  to  decrease  your  function, you  should  move  to  the  left. Over  here  the  slope  is  positive, so  we  want  to  move  to  the  left  to  decrease  our  function. It's  obvious  what  the  gradient  means  in  this  case. My  guess  is  that  even  though  many  of you  have  learned  about  grading  descent  before, you  actually  probably  still  don't  have grade  intuitions  about  the  multivariate  case. I'm  going  to  go  out  on  a  limb  and  make  that  guess. Let's  talk  about  how  I  would  go about  justifying  doing  grading  dissent in  the  multivariate  case. Again,  going  along  with  the  pretense  that this  is  what's  actually  happening  in  play. Why  do  grading  descent? We  saw  over  there  in  the  one  case that  F  of  x  plus  y  can  be  approximated. This  should  really  have  been  app, I'm  using  these  squiggles  here to  indicate  approximately  equal  to, not  exactly  equal  to F  of  x  plus  y  can  be  approximated  by  you  start  at  x, then  you  look  at  your  slope, multiply  that  by  the  direction and  magnitude  that  you're  moving  in. This  is  the  best  linear  approximation  to  this  function. At  this  point,  that  that's  how  we're  predicted  to  move. What  about  the  case  of  many  dimensions  multivariate  case? We  call  this  the  N  D  case. In  this  case  we  have  a  function. I'm  assuming  it's  a  scalar  function. It's  a  function  that  takes the  vector  and  spits  out  a  single  number. It  takes  the  vector,  we're  going  to  move  this  vector  b  a.
