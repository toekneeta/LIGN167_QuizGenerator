All  right,  let's  get  started. So  I  want  to say  we're  going  to  continue  with  back  propagation  today. I  want  to  say  something  a  little  bit  more  about the  multi  layer  percept,  which  we've  been  working  with in  the  multi  layer  perceptron. We  have  our  input, we  have  some  number  of  hidden  layers, and  then  we  have  an  output  layer  of  some  dimensionality. Let's  say  this  would  be a  multi  layer  perceptron  with  one  hidden  layer  here. So  input  layer, hidden  layer,  output  layer. What  was  the  name  that  we  gave  for the  things  inside  of  the  circles  last  time  it  was  a. Okay,  let's  stick  with  that. Let's  write  down  the  formula  for  this  again. So  we  have  our  x  vector  down  here,  right? We  have  our  weight  matrices 1.2  Let's  ignore  the  bias  term  for  right  now. Let's  just  assume  our  bias  term  is  zero. Nothing  really  fundamental  changes  if  we  do  not, let's  just  assume  the  bias  term  is  zero. What's  the  formula  for  this  model? It's  that  you  have  a  is  equal  to one  x  h  is  equal  to relu  of  a  and  then  o  is  equal  to Two  times,  say  that  times  h. Okay,  So  let's  unpack the  definition  of  a  little  bit  here. O  is  equal  to  two  times h.  What's  h  equal  to?  It's  equal  to  this. O  is  equal  to  two  times  L  u as  a,  it's  one  x. Just  unfolding  the  definitions  here. Now  you  may  ask  yourselves, we  gave  some  motivation  for why  we  need  this  non  linearity  before. What  I  told  you  is  that  if  you  do  just  a  single  mapping from  an  input  space  to  a  new  feature  space, you  have  your  original  feature  space,  x1x2. You  do  a  mapping  to  a  new  feature  space, and  you  just  try  to  do  classification  there. What  we  saw  is,  it  seems  like  you  need a  rail  there,  right? Remember,  we  need  the  non  linearity. Otherwise  everything  ended  up  on  the  same  line or  you  couldn't  linearly separate  anything  after  doing  the  mapping. We  talked  about  that  arguing  before. But  here  we  have  two  layers. We  actually  two  different  mappings  that  we're  doing. We're  doing  a  mapping,  the  input  space into  this  new  feature  space, and  then  we're  doing  a  new  one  into this  output  space  here, and  we're  going  to  do  a  soft  Max or  something  on  the  output  space. The  question  that  I  would  like  to  pose  is, do  we  actually  need  this  railue  here? Or  what  would  happen  if  we  got  rid  of  it? Let  me  put  it  another  way. Let's  say  that  we  have  a  multi  layer  perceptron with  multiple  hidden  layers. We  just  didn't  have  a  rail  in  any  of  those  hidden  layers. What  would  be  the  consequence  of  that? I  mean,  I  can  try  to  figure  that  out. So  this  over  here  is just  a  mathematical  representation of  what  this  diagram  means,  right? So  my  question  is, is  like,  do  we  actually  need  the  non  linearity? So  what  can  we  say  would  happen  if  you  got  rid of  the  non  linearity  in  that  hidden  layer? Yeah.  Why  is  that? Okay,  so  I  think  you're  on  the  right  track, but  I  just  want  to  know  what  happens when  I  get  rid  of  the  rail. You  so  let's  call  it  like. Okay,  so  it's  our  new  definition. I  all  I  do  is  I  get  rid of  the  rail.  So  it's  the  same  diagram. I  just  don't  have  like  the  H  three  anymore, that's  the  result  of  the  rail. So  it's  equal  to  two  times  L,  sorry, not  times  L, one  times  x. I  take  my  input  vector  x, I  multiply  it  by  two, then  I  multiply  it  by  one. What's  the  consequence  of  doing  this? Yeah,  just  a  linear  transformation. Can  we  be  a  little  bit  more  direct  about  that? How  can  I  immediately  see  that it's  just  a  linear  transformation? Exactly.  I  can  multiply  2.1  right? Instead  of  multiplying  these  two, first  matrix  multiplication  is  associative. So  I  can  do  things  in  any  order  that  I  want. The  first  thing  that  I  can  do  is  multiply  two  by  one. What's  two  matrices  multiply  together? It's  just  a  new  matrix. This  is  equal  to,  I'll  call  it, it's  some  new  matrix  times  x. If  I  get  rid  of  the  rail, you  then  I'm  back  to  just  like  a  linear  model, just  a  matrix  multiplication  matrix multiplied  by  my  input  features. That's  linear  regression  or  logistic  regression. That's  true  no  matter  how  many  layers  my  network  is, if  I  have  a  100  layer  network, but  I  don't  have  non  linearities anywhere  in  that  network  that's at  the  set  of  solutions  I  can find  are  the  same  as if  I  just  had a  single  matrix  and  there  was  a  single  layer. I  can't  write  down  any  new  types  of  functions. Any  non  linear  functions  using  a  deep  linear  network. I  have  to  have  non  linearities. Any  questions? Okay,  So  someone  asked that  question  at  the  end  of  the  last  class, or  a  question  that  was  basically  along  those  lines. So  I  wanted  to  answer  that. Now  why  do  we  really  need  these  non  linearities? Why  are  they  doing?  Well,  they're  preventing your  network  from  collapsing  into  just  a  linear  model. So  let's  go  back  to  the  example  from  last  time. The  only  difference  from  that  example,  I  believe, was  I  had  three  output  notes, so  let's  just  write  in  those  output  notes  here, okay?  And  then  what  happens? I  have  three  output  nodes. We're  assuming  we're  in a  classification  problem  where there's  three  different  classes, positive,  negative,  neutral,  or  happy, sad,  and  neutral,  I  guess  we  were  doing  last  class. Each  of  this  is  basically, this  text  is  how  much  do  we  think  this  text  is? This  is  how  much  do  we  think  this  text  is  neutral? To  get  probabilities  out  of  this, we  run  it  through a  soft former  vector. I  run  a  soft  max  on  the  vector. I  get  back  a  probability distribution  over  the  three  classes. So let's  just  introduce a  little  bit  more  notation  here. Remember  what  the  soft  max  is. This  is  equal  to  one  over  a  sum  from  I  equals one  to  three  of  the  I  e, the  two  over  the  same  thing. The  three  over  the  same  thing,  we  have  probabilities. Let's  give  some  slightly  nicer  notations  here. Let's  say  this  is  of  happy  probability  of happy  probability  of  sad,  probability  of  neutral. We  have  a  data  point  by  my  command. This  data  point  is  a  happy  data  point. It  consists  of  a  pair  xy  or  class. One  thing  I  have  my  loss  function. The  loss  for  our  parameters  theta  or  parameters  data. Let's  get  rid  of  B.  I  don't  want  to  have  to  worry  about the  bias  term,  our  parameters  data. It  is  just  the  collection  of  2.1  The  loss  that theta  achieves  on  this  data  point D,  is  D  really  a  vector? It  has  the  word  happy  in  it.  I  don't  know. Look,  we  know  what  this  means. This  is  equal  to  the  negative  log  probability of  happy given  x  and  theta. Lots  of  abusive  notation  going  on  here. But  we  want  to  know  just  what's the  probability  that  your  model  says, given  this,  what's  another  name that  we  have  here  for the  probability  of  happy  given  this? It's  this  right  here. It's  just  happy. That's  just  the  name  I  gave  it. So  let's  rewrite  this. It's  negative  log  happy. Any  questions? It's  funny  that  the  lecture  thinned  out  after  last  class. Actually,  I've  been  impressed  with  how well  attendance  has  been  going  until  now. This  is  really  the  meat of  this  class  that  we  did  last  time. And  then  today  back  propagation, we're  doing  back  propagation. This  is  the  deep  learning  algorithm. Without  this  algorithm,  there's  no  deep  learning. That's  just,  this  is  the  thing  that  drives  deep  learning. This  is  the  magic.  It's  not  even  that  complicated. Look,  I'm  not  saying  that  you, I'm  not  saying  that  everyone will  understand  it  on  the  first  try. Hopefully  you  will.  But  I'm  not trying  to  say  it's  like the  easiest  thing  in  the  world  to  learn. But  once  you  learn  it,  it's  not  magic. It's  not  like  the  hardest  thing  ever. I'm  very  happy  that  all  of  you  are  here  today. I  hope  people  are  watching  online  if  you're  not, because  this  is  if  you  want  to understand  what's  going  on  with language  models  and  all  of  this, this  is  the  thing  to  understand. If  you  don't  understand  this, then  everything  else  is  going  to  be  mystical  forever. Let  me  just  reiterate  the  question. Any  questions  about  this  up  to here  is  mostly  just  notation  so  far.  Okay? What  do  we  do  in  back  propagation?  Go  backwards. We  about  this  diagram  is basically  a  graph  and  we proceed  backwards  through  this  graph. There's  basically  directions  in  this  graph. This  is  the  beginning,  this  is  the  end.  We  go  backwards. We  see  how  does  changing  each  element in  the  graph  change  the  things  that  come  after  it. And  then  we  reuse  all  of the  computations  that  we've  done  previously. What's  the  first  thing  that  we're  going  to  do? We're  just  going  to  see  how  does  changing happy  change  the  loss  loss. Let's  our  stages  again, we'll  go  through  this  part  quickly  since  we did  it  on  Tuesday, but  we're  going  to  get  to  a  part then  that  we  have  not  done  yet. Step  one  is  computes  with  respect  to  happy. Actually  we  need  to  compute  n.  That's  it. That's  actually  all  that  we need  to  compute  at  this  stage. What  is  this?  Can  someone  remind  me? Yeah,  negative  one  over  happy.  Okay,  good. We're  done  with  that.  Next  thing we  do  is  we  go  one  step  further  back  in  the  graph. So  we  computed  basically happy  is  the  output  of  the  soft  max. We  could  draw  a  little  arrow  here  for  happy. That's  like  the  final  thing  that we  get  out  from  the  soft  max. Now  we're  going  to  see  how  does  happy  change, does  the  loss  change  when  we change  the  stuff  that's  inside  of  the  soft  max  there, we're  going  to  compute  what's  the  loss  given  I. We  break  that  down  to  two  stages. We  first  see  when we  change  that  changes  the  output  from  the  soft  max. And  in  particular,  it  changes what's  the  probability  of  happy. We  see  how  does  happy, when  change  I  then  changes. We  did  a  little  change  of  happy  and  then  we want  to  see  how  does  that  change  influence  the  loss. We  just  multiply  those  two  things  together. The  reason  we  do  that  is  because that's  what  the  chain  rule  tells  us  to  do. This  term  here  is  a  little  nasty, it's  just  going  through  a  little  bit  of  calculus  and the  chain  rule  again  on  the  output  of  the  soft  max. So  we're  not  going  to  do  it  this term  we've  already  computed  in  this  previous  step. We're  done  with  this.  I  just want  to  make  this  a  little  smaller. Just  remember  what's  the  chain  rule tell  us  if  we  have  two  functions. Let's  just  make  this  in  the  single  variable  case. It's  the  same  thing  for  partial  derivatives, but  it'll  be  a  little. What  does  the  chain  rule  tell  us? The  chain  rule  tells  us  that  this  is  equal  to f  prime  of  g  of  x  times  g  prime  of  x. That's  the  same  thing  that  we  wrote  down  here. What  is  this  measure?  This  measures  how  does changing  x  by  a  little  bit  change  the  function  g? To  measure,  what  are  we  doing  here? We're  measuring  what  this  quantity  is trying  to  tell  us  is  if  we  change  x  by  a  little  bit, how  does  this  function  composition  first  do, then  put  the  result  of  that into  how  does  that  thing  change. You  break  it  into  two  parts. You  first  measure  how  does  the  function  change. Then  you  measure  how  does  the  function  change  at the  current  value  of g.  When  you  change  that  by  a  little  bit, we're  changing  by  a  little  bit. We're  putting  that  little  change into  and  we're  seeing  how  does  that  influence  the  result. So  you  have  to  just  sort  of  trust that  the  chain  rule  is  true. I  mean,  you  can  prove  it,  but  that's almost  different  than  trusting  it, right?  Like  I  can  prove  it. But  it's  a,  it's  a  weird,  I  mean, I  still  find  the  chain  rule,  shockingly  simple. It's  like  it  didn't,  it  sort  of didn't  have  to  be  this  way, except  mathematically  does  have  to  be  this  way. I  don't  know.  I  don't  know  how  quite  to  put  it. It  makes  a  lot  of  sense, but  you  have  to  internalize  it. Any  questions?  Okay.  Yeah. Did  was  there  a  question  there? No.  No.  Okay.  Okay. Three,  what  do  we  do  next? We're  going  to  measure  how  does  we've  gone  through. We've  measured  how  is  changing  o, each  of  the  O's  change  the  loss. Now  we're  going  to  measure  how  is  changing the  things  in  W  change  the  loss. The  way  that  we're  going  to  do  that  is by  measuring  how  does  the  things  in W  change  and  how is  changing  the  things  in  change  the  loss. Remember  our  wonderful,  terrible notation  for  the  elements  of  the  weight  matrix? This  is  the  I  throw jth  column  in  the  weight  matrix  W  two. We  want  to  measure  how  does  that  change  the  loss? To  figure  this  out  last  time  we  did something,  what  was  the  thing  that  we  did? I  want  to  know  what  is  this  thing? What's  IJ?  Why  does  it  correspond  to  here? Yeah,  exactly. It's  one  of  the  arrows. It's  a  connection  between one  of  the  A's  and  one  of  the's  Which  connection  is  it? So  it's  J  that  the  general  IJ. So  just  to  clarify,  I'm  saying I  and  J  can  be  anything  here. Yeah,  goes  to  I  from  J.  That's  right. So,  it's  the  arrow  that  that  points  to  from  AJ, like  this  one  here  would  be  223. It's  the  strength  of  the  connection between  those  two  nodes. What's  our  formula  for  the  O's? Let's  say  we  have  one. What's  my  formula  for  one? In  terms  of  the,  in  terms  of  two? Yeah,  forget  about  the  biases. Definitely  not.  W  is  the  input,  right? Yeah,  that's  our  vector O  is  equal  to two  times  h.  I'm  ignoring  the  bias,  doesn't  exist. What's  our  formula  then  for  one, the  first  coordinate  in  this  output one,  what's  that  equal  to? So  remember,  a  matrix  multiplication is  just  the  collection  of  dot  products. I  do  one  dot  product  for  the  first  row  of  the  matrix, another  dot  product  for  the  second  row,  and  so  on. Yeah,  21  from  21 from  1122211,  you  said? Yeah,  211. Okay.  Yeah,  two  se,  great. Yeah,  okay. This  formula  holds. Two  has  a  similar  formula. Three  has  another  similar  formula. Okay.  How  am  I  going  to  break  this  down? This  is  equal  to  this  partial  derivative. I  do  the  chain  rule. I  first  see  how  does  changing  this  changes. I  see  how  changing  the  O's  change  the  loss. I'm  going  to  say,  which  should  I  focus  on  IJ? I  want  to  know  that only  is  going  to  change  a  single  one  of  the  O's. It  only  goes  into  one  of  the  O's, only  changes  one  of  them. Which  does  it  change? What's  that? Great.  So  I'm  going  to  see  how  does  change  IJ  times, how  does  the  loss  change  a  change  in  O I  k.  This  I've  already  computed  right  here. Why  about  this?  How  do  I  compute  that? So  I  have  my  formula  down  here  for  one. We  have  a  similar  formula  for  O  I each  day.  Each  day, the  only  thing  that  W  IJ touches  directly  is  an  H.  It's  HJ. You  multiply  those  two  things  together and  you  sign  up  with  a  bunch  of  other  stuff that's  irrelevant  here,  okay? Any  questions  about  this? Okay.  Let's  go  on  to  step  four. Now,  step  four  is  an  interesting  one. This  is  where  we  did  not  quite get  here  at  the  end  of  last  class. We  want  to  measure  how  does  the  loss  change  when we  change  one  of  the  H's  here. Let's  say  H  one  up  until  now  when  I  wanted  to  see  like, let's  say  we  were  looking  at  this  edge  here. When  I  change  this  edge, it  only  influences  one  thing. Changing  this  edge  directly  influences  only  one. Then  I  can  measure  what's  the  effect of  changing  that  edge  on  one. What's  the  effect  of  changing  one  on  the  loss? When  I  change  H  one, I'm  not  in  that  same  situation  anymore. When  I  change  H  one,  I  change  12.3  simultaneously. Let's,  let's  start  with  H  one.  For  right  now. You  may  not  know  the  correct  answer for  how  to  do  this,  yet. We  change  one  part  of  the  model. It  affects  a  bunch  of  other  different  parts  of the  model  which  each  of  them  then  affect  the  loss. How  should  we  take  into account  all  of  these  different  changes? I  believe  that  I've  already  mentioned  that  one  of the  most  important  principles in  deep  learning  is  to  keep  it  dumb. Don't  try  to  do  anything  too  clever. Just  be  dumb.  If  you follow  this  heuristic,  you  will  do  well. I'm  not  going  to  say  what  happens  then. What's  the  dumbest  thing  that  we  could do  to  measure  the  effect  of changing  H  one  on  the  loss  possible  thing? What  would  be  the  simplest  world that  we  were  in  if  it  was  true? Yeah,  Adding  up  all  the  things  we  change. So  tell  me  what  I  should  add  up, This  changing  H  one  directly  change A  is  a  weight  that  we're  learning. There's  a  difference  between two  types  of  things  in  these  models. There's  the  weights  that  we're  learning, and  then  there's  the  activations, which  are  the  things  that  we  compute  using  the  weights. Okay,  so  I'm  seeing  how  is the  loss  changed  with  respect  to  one.  What  do  I  do  now? A  one,  great. Then  just  add  all  three  of  the s.  I  measure  how  much  does  H  one  affect  each, how  much  does  each  affect  the  loss. And  then  sum  all  of  those  things  up. Okay? If  you  were  to  guess  this  simple  formula, the  simple  possible  formula here  was  correct,  you  would  be  right. This  is  it. Let's  see  if  we  can  get  a  little  bit of  intuition  about  what's  going  on  here. Suppose  that  I  have  a  function.  I  have  a  function. Let's  say  I  want to  add  on  a  very  tiny  vector  z  onto  that  function. What's  our  rule?  Remember  Z  is  very  tiny. What's  the  rule  for  computing  this? Getting  an  approximation.  That  is, this  is  approximately  equal  to  what? Yeah,  no,  I  mean, it's  not  like,  it's  not  nothing. It's  just  it's,  it's  pretty  tiny. Yeah.  We've  in  fact talked  in  this  very  class  about  how  we  do  this. Yeah,  exactly. Okay.  Right.  So  what  happens  if  y  and  z  are  just  scalars? What  do  we  do  in  that  case? How  do  you  compute  an  approximation  to a  function  when  you  have  a  scalar  function? Just  takes  real  numbers, you  change  the  function,  You  change the  input  to  the  function  just  by  a  little  bit. What  can  you  do  in  that  case?  Yeah. All  right,  so  this  is  approximately  equal  to f  of  y  plus  what this  gives  us,  the  slope  of  the  function. This  is  taking  a  linear  approximation,  right? So  we  measure  what  was  the  scope  of  the  function  at  y. And  we  just  move  a  little  bit along  that  line  by  the  amount, that's  what  our  approximate  value  for  this  is. What's  the  generalization  of  this  to  the  case where  we  have  vectors  in  the  input? Yeah,  dative  gradient  instead of  the  derivative,  you  use  the  gradient. This  is  approximately  equal. It's  going  to  be  almost  exactly  the  same  formula, except  we  have  a  gradient. Here,  it's  gradient  instead  of  the  derivative, we  use  a  dot  product  instead of  just  normal  multiplication. Okay,  Now suppose  that  instead  of  moving  by  some  random  vector  z, I  instead  choose  a  very  special  vector. Before  we  get  there,  let  me  mention  one  thing  here. Let's  look  at  this  gradient  multiplication. Let's  just  take  this  over  here. Gradient  of  t,  y  dot  product  z. Let's  imagine  that  these are  vectors  with  dimension  three. Each  of  these  vectors  has  three  coordinates  in  it. Is  this  dot  product  equal  to? In  that  case, so we  have  z  is  equal  to  the  vector  z  is  equal  to  z  1z2z3. The  vector  y  is  equal  to  y  1y2y3. What  is  this  p  product  equal  to?  In  that  case? Yeah,  it's  absolutely  equal  to  a  scalar.  Which  scaler? What  is  a  product  In  general, if  I  want  to  express  a  dot  product algebraically  in  an  expanded  way,  what  is  that  thing? Yeah,  summation  of  product. Each  element. Exactly.  So  I  take  the  product  of each  element  and  I  sum that  thing  up.  So  what  do  I  get  here? So  the  gradient's  people having  a  little  bit  of  an  off  date  today. Is  that  midterms? Is  that  a  mid  terms? Okay,  well,  understandable. Yeah,  not  quite. So  what's  the  gradient? Let's  first  think  about  what's  the  gradient? Yeah,  yeah,  yeah,  yeah,  yeah,  at  exactly. So  this  is  equal  to  the  partial  derivative. So  we  collect  up  the  partial  derivatives into  a  vector,  right? Okay,  so  we're  going  to  take the  partial  derivative  of  evaluated at  y  with  respect  to  y  one. We're  going  to  multiply  that  by  z  one. We're  going  to  sum  that  up  with partial  derivative  with  respect  to  y  two, multiply  that  by  two, and  then  sum  that  up  with  partial  derivative  of f  of  y  with  respect  to  y  three, multiply  that  by  z  three,  okay? So  that's  what  happens  when  we, sorry,  that's  what  happens  when  we take  this  function  at  and  change  it  by  a  little  bit, where  that  little  bit  is  z. Okay,  now  let's  go  back  to  this  formula  here  on  top, I  look  at  this  formula  here, I  look  at  this  formula  here. They  look  related  to  each  other,  right? I  mean,  they  should. I'm  summing  up  three  things, where  those  three  things  are at  each  a  product  of  two  things. This  looks  like  a  product, this  is  definitely  a  dot  product. There's  definitely  a  gradient  involved  somewhere. I  feel  like  there's  something  closely  related  going  on. Here's  how  we  can  see  that  relationship. Let's  think  about  the  loss  function as  a  function  of  the  O's. Okay? If  you  give  me  the  O's, I  can  compute  the  loss  from  that  the  loss  is  in  fact, it's  a  function  of  other  stuff  too. There's  other  ways  I  can  also  write the  loss  as  a  function  of  the  Xs,  for  example. But  in  this  case,  I'm  going  to  decide to  write  the  loss  as  a  function  of  the S.  Now  what  I want  to  do  is  measure  what's the  value  of  the  loss  when  I'm  at  O, and  then  I  move  by  a  little  bit. What  is  going  to  be  the  little  bit  that  I  move  by? That's  the  crucial  question. The  little  bit  I  move  by  is  going  to  be  the  effect that  changing  H  one  has  on  each  of  the  O's. Let's  say  that  another  way. Let's  define  a  new  vector. Let's  define  z  to  be  a  collection  of  partial  derivatives. I'm  collecting  up  these  three  partial derivatives  into  a  vector. These  three  partial  derivatives  represent  what  happened to  each  of  the  O's  when  I  change  H  by  a  little  bit. Now  what  I'm  going  to  do  is measure  what  happens  to  my  loss  function. Remember,  my  ultimate  question  is  what happens  to  the  loss  when  I  change  H11? Way  of  thinking  about  that  is  when  I change  one,  the  O's  change, it's  going  to  be  approximately  the  loss plus  the  gradient  of the  loss  at  O  dot  product. What  is  this  thing  here? The  gradient  of  the  loss  evaluated  at  O  product  z. That's  equal  to  that  over  there. It's  the  gradient  of the  loss  is  the  collection of  partial  derivatives  with  respect  to  O. Partial  derivative  of  the  loss  with  respect  to  one  z. What's  the  first  element  of  z? It's  how  does  one  change  when  I  change  one? I  sum  that  up  with  partial  derivative of  the  loss  with  respect to  two  times  the  second  element  of  z. How  does  two  change  when  I  change  H  one? So  this  here  gives  me  a  new  estimate of  my  loss  function  when  I  change  it  by  Z, by  a  very  small  amount  of  z. But  then  if  I  want  to  get  the  actual, how  does  the  loss  function  change, this  tells  me  what's  the  value  of  the  loss. What  I  want  over  here is  how  does  the  loss  function  change, but  how  the  loss  function  changes  is just  the  new  value  of  the  loss minus  the  old  value  of  the  loss,  which  is  this. There's  a  few  steps  here,  the  general  point. Is  that  I  want  to  measure how  does  changing  the  loss  change  when  I  change  H  one. When  I  change  H  one  changes  as  a  result  of  that, I'm  going  to  measure  how  does  change. I'm  going  to  collect  that  up  into  a  vector. I'm  going  to  add  that  vector  onto  the  loss, and  I'm  going  to  get  an  approximation  for  how  the  loss changed  when  I  moved  by  that  amount. That's  why  we  get  this  very  simple  formula where  we  just  sum  everything  up. The  reason  things  are  so  simple  is  because  of this  miraculous  formula  here  in  calculus, Everything  is  linear,  that's  the  assumption. You're  just  taking  linear  approximations  everywhere. Linear  means  you  just  add  stuff. Okay.  Any  questions  about  this? Yeah,  everything  is  an  approximation. Equation  four  is  the  correct  formula for  the  derivative,  if  that's  what  you  mean. Or  the  partial  derivative.  The  partial  derivative is  defined  as  a  limit,  right? If  we  really  want  to  do  this  formally, quite  give  a  formal  proof  of  this, you  could  turn  this  into  a  formal proof  with  a  few  more  steps. What  you  have  to  do  is  take the  difference  between  this  and  this. You're  going  to  get  this  and  then  take  the  limit as  the  magnitude  of  Z  basically  goes  to  zero. You're  not  actually  taking  a  step  size  quite  this  large. You're  going  to  take  a  step  size  in  that  direction, though  you're  going  to  get  a  limit, then  this  turns  out  to  be  exactly  true. But  my  feeling  is  that  if I  were  to  give  like  a  formal  proof  here, it  would  hide  more  than  revealed. I  want  to  give  an  intuition  about  why  this  is  true. It's  because  linear  approximations work  in  some  sense. Look,  the  limiting  explanation doesn't  actually  tell  us  anything,  right? Because  what  the  limit  says  is  literally  like, as  you  get  infinitely  small, what  happens  to  the  function? But  in  practice,  that's  not  what we  do  when  we're  optimizing  these  functions. We  do  not  take  infinitely  small  steps, We  take  finitely  long  steps. It  still  works.  This  is maybe  actually  the  correct  explanation, not  the  limiting  one. Any  other  questions?  It's  a  good  one. Let's  go  back  to  our  diagram  here. What  have  we  done  so  far? We  measured  how  does  the  loss  change when  we  change  this  happy. How  is  the  loss  change  when  we  change the  measured  how  is  changing  change  happy? And  we  already  knew  how  happy  change  the  loss. We  next  measure  how  is  changing this  weight  matrix  W,  two  change  the  loss. We  did  that  by  measuring  how  is  two change  the  O's  change  the  loss. We've  just  measured  how  is changing  H  one  or  any  of  the  HIs  change  the  loss. We  found  out  that  there's  an  additional  subtlety  there which  is  H  one  or  any  of  the  HIs  changes. All  three  of  the  OI's. You  have  to  measure  the  effect  on  all  three  I's and  then  sum  up  all  of  those  effects  on  the  loss. What's  left  here,  we  can  see  how  is  changing one  or  any  of  the  AI's  change the  loss.  How  would  we  do  that? Step  five  is  how  changing  the  loss. How  does  the  loss  change  when  you  change  I? How  do  I  do  that? We  want  to  always  think  what  does each  element  directly  influence? We  want  to  find  the  simplest  thing  that  it  influences, The  most  direct  thing  that  it  influences. What's  the  most  direct  thing  that one  influences  on  one? Right?  So  how  does  it  influence,  what's  the  relationship? Lue  transformation,  right,  I  is  equal  to  rail. I  take  the  partial  derivative  of  HI  with  respect  to  AI. I  multiply  that  by  the  partial  derivative  of the  loss  with  respect  to  HI. I've  already  calculated  this  in  step  four. What  about  this  one?  What  about  that  one?  What  do  we  do? What's  the  relu  function? Again,  measure  how  does  HI  change. When  I  change  AI,  What  was  the  rail  you  function  again? It  looked  like  this.  It  was  zero when  my  input  is  below  zero  and  then  it  looks  like  this. What's  the  derivative  of  this? I'd  like  people  to  think  about  this  question, ponder  this  question  for  30  seconds. Please  give  me  a  formula for  the  derivative  of  this  function. We'll  write  it  in  a  little  box  over  here. It's  not  really  a  partial  derivative  here, it's  just  a  normal  derivative. So  I'm  taking  the  derivative  of  the  rail  you  function. I  want  to  get  some  good  pondering  here. Anyone  want  to  throw  their  hat  into  the  ring  here? Yeah.  Okay.  Yeah.  Okay. Okay.  Okay.  So  when  I  is  less  than  zero, let's  get  there  in  1  second. What  about  when  AI  is  greater  than  zero? Okay, And then  what's  going  on  here? Yeah,  undefined  is  one  way  to  put  it. Our  function  is  not  differentiable. I've  been  talking  about  stochastic gradient  descent  the  whole  class, we're  going  to  be  optimizing functions  by  taking  derivatives. That  you  can  take  the  derivative  of  your  function. It  turns  out  we  can't, yet.  It  still  works. What  is  going  on  there? Basically,  there's  a  few  answers  to  this. There's  one  answer  which  is  tempting, which  I  guess  I've  said  in  previous  years, and  on  reflection  is  not  actually  correct. What's  the  tempting  answer  to  say  here? Yeah,  that  is  what  people  basically  do. Yeah,  yeah,  that  answer, but  what  I  mean  is  like, what's  the  tempting  justification  for  that? You're  right,  we  just  make  it  zero  basically. Okay.  Let's  expand  on  that. Why  is  it  almost  never  matter? You  would  think  that,  yes,  you would  think  you're  working  with  real  numbers. Let's,  let's  forget  about  the  floating  point  issue because  that  does  actually complicate  things  for  this  explanation. But  you're  working  with  floating  real  numbers,  right? Like  a  real  number. What  are  the  chances  that  you'll  actually, that  the  input  to  this  function will  actually  ever  be  zero? What  are  the  chances  of  it? It  seems  like  it  should  be  negligible. Now,  it's  tempting  to  say  this, the  problem  is  that  we  have  here  a  function  Lue  that outputs  zero  a  lot  of the  time  As  a  result  of  it  outputting  zero, subsequent  inputs  to  other  functions  will  also  be  zero. It's  not  like  simply a  theoretical  issue  that  you  encounter, that  this  thing  will  equal  zero. It  happens,  you're  trying to  take  the  derivative  of  a  point. You're  trying  to  take  the  derivative, this  function  at  zero  where  the  derivative  doesn't  exist. People  I  think  just  usually  say  it's zero  there,  The  derivative. The  justification  for  that  is, I  think  basically  that  works. There's  something  called  the  subgradient  that  if  you  take a  upper  division  optimization  class, you  might  learn  about  a  subgradient  for  this  function. Basically,  for  certain  non  differentiable  functions, there's  something  that  you  can  do that's  not  quite  a  gradient. It's  almost  like  a  gradient. It's  a  little  bit  is, it  has  some  nice  mathematical  properties and  zero  is  an  okay  solution  there. I  very  strongly  doubt  like the  theoretical  justification  that  this  has  is actually  the  correct  justification  for  why  it  works. Yeah,  we're  in  the  situation  where  we  are optimizing  functions given  the  assumption  that  they're  differentiable, even  though  they're  not  differentiable. That's  the  entire  foundation  of  deplury, but  it  works.  Okay. Any  questions  about  that? Okay.  What's  next? So  we've  figured  out  how  does changing  the  A's  affect  the  loss. We  have  a  formula  for  that.  Now. What  do  we  have  left  to  do? Yeah.  Oh,  well,  where  were  we  repeat? So,  you  had  a  suggestion  up  there? Yeah.  Okay,  well,  actually  we  skipped  a  step  first. Let's  get  back  to a  very  interesting  idea  that  you  just  had. Why  have  we  not  actually  examined  here? So  that  we  haven't  looked  at  the  input. There's  something  even  sooner than  that  that  we  have  not  looked at  Y  weight  matrix  one. So  we  need  to  see  how  changing this  weight  matrix  changes  the  loss.  That's  nothing  new. Changing  a  particular  weight  changes  one  of  the, as  we've  already  measured, how  changing  the  A's  change the  loss,  there's  nothing  new  there. It's  exactly  the  same  as  what  we  saw with  this  weight  matrix  here. Okay,  now  we're  done  with  this  weight  matrix, we're  done  with  this  weight  matrix. Is  there  anything  else  left  for  us  to  do? You  suggested  measuring  how changing  the  input  changes  the  loss. It's  an  interesting  idea.  Should  we  do  that? Okay,  I'm  asking  a  leading  question, so  you  can  get  You  were  suggesting. No.  Yeah,  why  not? Yeah, exactly. So  this  is  a  fixed  input  that  we  have. This  is  not  something  that  we're  trying  to  learn. The  reason  why  we're doing  back  propagation  and  computing  gradients  is we're  trying  to  find  the  gradient  of the  loss  with  respect to  the  things  that  we're  trying  to  learn. We're  only  trying  to  learn  two  things  here. Weight  matrix  one,  weight  one,  and  weight  matrix  two. We're  not  trying  to  learn  the  input. The  input  is  just  handed  to  us. We  don't  need  to  change  it,  so we  don't  need  to  take  the  gradient. I'll  mention  offhand,  and  we're  not going  to  use  this  idea  for  the  rest  of class  since  you  brought  it  up. I'm  going  to  mention  it,  but  I  just  want  to  know, just  for  confusions  sake,  we do  not  do  this  in  this  class. You  can  take  the  gradient  of the  loss  with  respect  to  these  Xs. One  common  thing  that  people  do  in  practice,  in  fact, is  they  try  to  maximize  the  loss  using  that  gradient. They  try  to  find  small  perturbations  of x  that  will  increase  the  loss  by  a  lot. Why  would  they  do  that? Yeah,  to  take  an  existing  model and  get  a  desired  output  from  it. The  most  common  use  case  for  that  is  something  called an  adversarial  example  where, let's  say  you  take  an  existing  image, the  model  thinks  that  that  image  is  a  cat. It  turns  out  that  basically every  deep  burning  model  for  images, there  exists  a  small  perturbation, very  small  change  to  the  image. Where  if  you  change  the  image  by  that, you  can  get  the  model  to  think  a  cat  is  like a  tree  or  whatever  other  target  that  you  want. They're  called  adversarial  examples. Very  interesting  area,  lots  of  fun  applications  of. One  very  interesting  application  is that  there  are deep  learning  based  face  recognition  systems. As  you  may  know,  when  you're  walking  around  in many  countries  or  places, there  is  a  system underneath  that's  trying  to  figure  out  who  you  are. They  can. Out  where  you've  been. For  example,  you  can buy  T  shirts  or like  masks  where  if  you  wear  those  things, the  T  shirts  are  fun,  they're  just  like  a  random  image. But  if  the  model  sees that  image  and  at  the  same  time  as  it  sees  your  face, it  will  think  that  you're  someone completely  different  or  just  not  a  human  at  all. Yeah,  the  wearable  adversarial  images.  Those  are  fun. Okay,  we're  done  with back  propagation  for  the  multi  layer  perceptron. You  now  know  how  to  optimize the  most  important  neural  network. And  back  propagation  in  general  looks  basically  the  same. There's  nothing  very  special that  happens  in  any  other  case. And  we'll  talk  about  how  we  can  even think  about  the  general  case  right  now. Okay, so  we'll  keep  this  figure  up  here. I'm  going  to  draw  a  parallel  figure that  represents  exactly  the  same  information. This  is  like  the  traditional  diagram  that people  use  informal,  right? If  we  look  at  it,  we  can  figure  out  what  it  means. But  it's  not  something that  exactly  like  we  could  put  into  a  computer  program. You  could,  but  you'd  have  to establish  your  own  convention. I'm  going  to  talk  about  now  is  something approximately  how modern  deep  learning  programming  languages represent  neural  networks. It's  still  going  to  be an  approximation  to  what  they  actually  do. They  do  some  fancy  stuff, but  we'll  have  a  pretty  good  idea  of  what's  going  on. You're  doing  some  pitorch  in your  homework  pitch  and other  deep  learning  frameworks like  there's  two  other  major  ones, there's  tensor  flow  and  jacks. I'd  say  those  are  the  two  major  ones. Besides  Pitorch,  they  do  something  pretty  amazing, which  is  that  they  take  your  program, they're  able  to  do  automatic differentiation  on  that  program. Your  program  defines  a  function, able  to  figure  out  somehow, automatically  from  the  program that  you've  written  out  what  the  gradient  is. That's  exactly  what  on  your  homework  right  now. Yeah.  What's  going  on as  part  of  the  model, Being  able  to  find  the  gradient is  that  it's  taking  your  programming, turning  it  into  something  called  the  computation  graph. The  computation  graph,  it's just  another  representation  of  the  same  information  here. Let's  talk  about,  let's  find an  equivalent  computational  graph  representation  of  this. We're  going  to  have  two  types  of  things. We're  going  to  have  boxes, we're  going  to  have  circles. Boxes  will  represent  input. These  are  things  that  we're  not  computing  actually. We're  going  to  have  three  types  of  things. We'll  see  that  in  just  1  second. We  have  boxes  here, let's  say  they're  diamonds. Diamonds  will  represent  things that  we're  actually  trying  to  learn. Then  we're  going  to  have  circles. Circles  are  going  to  be  things  that  we  compute. Let  me  just  make  things  slightly  simpler  for  myself. I'm  going  to  put  vectors rather  than  individual  elements  into  this. I'm  going  to  just  collect  up  this  x  one  and  x  two  as a  vector  should  be  in  a  square. Okay,  We  have  inputs  in  squares, we  have  weights  in  diamonds. Then  we  have  outputs  of  computations  in  circles. I'm  going  to  draw  an  arrow. Each  circle  is  going  to  have some  number  of  arrows  going  into  it. Circles  are  the  results  of  computations. A  has  two  arrows  going  into  it, X  W.  For  each  result  or  output  of  a  computation, I'm  going  to  indicate  how  was  that  thing  computed. It's  going  to  be  computed  as  the  input  of two  things  in  an  operation  between  them. The  operation  in  this  case  is  multiply. I  just  indicate  that  how  did  I  combine  these  two  things? I  multiplied  them  together. Okay? Then  what's  next? H,  one,  or  h  rather? I  have  my  vector  h  that  I'm  computing. H  takes  as  input  only  one  thing  a, the  hidden  state  h  only  takes  as  input. I  have  to  just  indicate,  I  indicate  what's the  function  of  and  what  operation  is  performed. That  operation  is  the  relu,  what's  after  h. Well,  I'm  going  to  have  a  new  thing  that  I'm  learning, The  weight  matrix  W  two, and  a  new  output  computation  that  I'm  performing, which  is  this  vector  O. How  do  I  get  one  from  the  other? I  multiply.  Finally,  I have  my  probabilities. I'll  just  call  them  the  probs  here. My  probabilities  like  these are  the  output  probabilities  that  I'm  getting. How  do  I  compute  the  probabilities? They  only  take  in  O  as  input, what  operations  performed  on  O. It's  the  softmax. This  diagram  here  represents exactly  the  same  information  as diagram  here  it  is  that  it's  a  lot  more  explicit. I'm  assigning  special  types  of  nodes  to  inputs, and  the  weight  matrices  that  I'm  learning, those  get  indicated  very  in  a  special  way. And  I  explicitly  state  what  operations  are used  in  order  to  combine different  elements  together  to  get  an  output. When  I'm  producing  a,  I  get  that  from  two  inputs, X  and  W,  one. The  way  I  get  them  is  by  multiplying  them  together. This  is  explicit  enough  that  I  can  write it  in  a  computer  program. We'll  learn  a  little  bit  about  this,  I  think  next  class. This  is  basically  what's  happening when  you're  using  pitch. Pitch  looks  at  your  code and  transforms  your  code into  a  representation  that  looks  like  this. It's  a  little  bit  like  that, actually,  maybe  closer  to  what  Jack. Slightly  different  than  that. We're  not  going  to  go  into  the  nuances there  if  you  had  that  picture  in  your  mind. You'll  be  good  for  quite  a  while, you'll  understand  the  fundamental  thing  that's  happening. Now,  one  interesting  consequence  of this  representation  is  that it  gives  us  a  framework  for constructing  new  types  of  neural  networks. Nothing  says  that  my  neural  network has  to  have  like  this  particular  structure  here. I  could  combine  stuff together  here  almost  however  I  want. I  could  add  in  new  nodes  to  the  side  here  if  I  want, and  then  compute  them  from  O  and  A  together. I  can  cross  layers  if  I  want  it. Then  combine  those  things  together  to  get  the  output. As  long  as  my  graph  doesn't  have  cycles  in  it,  I'm  okay. As  long  as  it's  a  directed  graph,  that  is, you  cannot  go  from  any  node  back to  itself.  That's  not  allowed. But  other  than  that  you're  fine, you  can  construct  about.  We  don't  have  quite  enough  time. Today  we'll  talk  about  this  next  class, somewhat  more  elaborate,  things  that  you  could  do  here. Now  there  was  a  lot  of  excitement  about  all  of the  Pytorch  and  very good  programming  language  for  doing  deep  learning. They  came  out  2015, 2016,  sometime  around  then. People  were  extremely  excited. The  reason  they  were  so  excited  is  because  they  no longer  had  to  write  down  gradients  by  hand? No,  that  was  a  major. That  was  how  you  spent  most  of your  time  when  you  were  doing  deep  learning  research. It  was  manually  computing them  on  paper  and  then  coding  them  up? Yeah,  it  was  really  painful. Actually,  it  was  a  little  bit  before  my  time  to  do  that. For  the  very  small  number of  deep  learning  researchers  back  then, that's  how  they  would  spend  their  time. People  were  extremely  excited  when  like pitch  in  tensor  flow  came  out. They  didn't  have  to  do  that  anymore. It's  not  only  that,  but  they  could  write  down  all  of these  really  interesting  architectures that  had  very  unusual  control  flow. One  thing  flowing  into,  now  we're  flowing  into  another. Any  graph  that  you  could  write  down,  all  of  a  sudden, you  could  just  implement  that automatically  in  your  language. And  you  could  compute  gradients  automatically,  right? Because  here,  how  do  you  compute gradients  is  just  back  propagation  on  this  graph. Every  node  we  know  what  goes  into  that  node. We  see  how  does  changing  change  the  props. We  can  do  that.  How  does  changing  H  change? How  changing  W  two  change? We  can  do  all  of  these  things. It's  very  nice.  Now,  the  very  sad  thing has  been  most  of  that  excitement. I'd  say  it  went nowhere  because  we're  just  going  back  to  the  MLP  now. The  field  is  just  returning  back to  make  your  model  a  big  MLP. If  your  model  is  a  big  MLP, you  don't  need  these  very fancy  programming  languages  to  do  stuff. Everyone's  converging  on  a  very  small  number of  architectures.  Maybe  that  will  change. But  there  was  this  big  blossoming  of all  this  weird  stuff  that these  programming  languages  could  allow. Now  it's  all  being  windowed  back  down to  a  small  number  of  things. Let's  end  there.  I'll  see  everyone  next  week.
