Okay,  let's  get  started.  Questions. I'll  mention,  we  gave  back  your  problem  set, people  did,  well,  that  was  good. One  interesting  thing  that  I  found was  that  the  score  distribution  was almost  the  same  as  when there  was  no  GPT  involvement  in  previous  years. Something  very  interesting  about  that, not  what  I  expected. That  indicates  to  me  actually, that  this  was  challenging, or  more  challenging  than  you  might  think, but  also  a  good  level  of challenge  where  people  could  still  do  it. So  I  thought  that  was  good. Hopefully  you're  learning  something about  both  the  systems  and  how  to  use  them and  about  the  course  material  at  the  same  time. Anything  else  before  we  get  started? Yes.  No,  no,  no,  no. I  would  say  same  level  of difficulty  for  like  the  actual  problems  themselves. Okay.  Yeah. Just  from  reviewing  the  submissions, I  think  people  got  more  out  of  the  3.5  part  of  it. Out  of  the  four  parts,  You'll  just  be  clear, you're  allowed  to  use  four  however  you  want. You  can  use  four  to  find  errors  in 3.5  I  don't  know  if  that  works,  but  you  can  try. It  is  not  clear  to  you  yet. Actually,  all  of  the  language  models,  including  four, are  quite  bad  at  finding  errors  in  things  like, yeah,  you're  seeing  some,  yeah. They  either  say  that  there's an  error  when  there  is  not  one. They  say  everything  looks  great. When  there  is  an  error  that  happens  a  lot, I'd  say  that's  one  of  the  weaker  parts  of  the  model. Okay,  so  we're  going to  continue  talking  about  multilayer  perceptrons  today. So  let's  write  down  a  simple  multilayer  perceptron  here. Okay,  so  this  is  a  multi  layer  perceptron. Let's  just  talk  about  some  terminology  first. So  this  is  called  a  multi  layer  perceptron is  generally  the  same  thing  as  a  fully  connected  network. What  do  we  mean  by  a  fully  connected  network? What  we're  saying  is  that  this  is, let's  say  for  right  now, we'll  use  this  terminology. This  is  not  necessarily  the  best,  but  it's  okay. We  have  three  layers  in  this  network. We  notice  each  layer is  fully  connected  with  the  layer  after  it. What  that  means  is  that  there  are basically  arrows  going  from  layer  one, from  each  no  layer  one. Into  each  nodine  layer, two  arrows  going  from each  nodin  layer  two  to  each  nodine  layer  three. This  is  our  input  layer, this  is  our  output  layer, This  is  our  hidden  layer. In  general,  an  MLP  can  have  many  hidden  layers. It  will  have  usually  one  input  layer,  one  output  layer. It's  beginning  and  the  end, and  some  number  of  hidden  layers  at  one. For  it  to  really  be  considered  an  MLP, a  multi  layer  perceptron, how  could  we  have  more  than  one? Well,  we  just  added  another  layer  of hidden  states  above  this  one  and  below  the  output  layer. Okay,  let's  talk  about  the  math  of  what  this  represents. We  talked  about  this  last  time, but  let's  go  over  it  again. We  have  an  input  vector x.  X  is  a  input  vector  with  two  coordinates, x  one  and  x22  features. If  we're  doing  sentiment, classification  is  the  number of  times  that  the  word  happy  occurs, and  this  is  the  number  of  times  that  the  word  sad  occurs. And  we're  going  to  be  trying  to  classify documents  based  on  those  input  features, the  input  features  of  some  sort. When  it  comes  to  language, we'll  learn  in  a  couple  of, maybe  next  week  or  two  weeks exactly  how  we  deal  with the  input  features  for  a  language. Right  now,  we'll  just  assume  we  have them,  they've  been  given  to  us, we  have  our  input  or we're  going  to  have  to  weight  matrix. Basically,  we  can  think  about  this  multi  layer  perceptron as  mapping  one  set of  features  into  a  new  set  of  features. And  then  doing  the  same  thing  again. Each  of  those  mappings  will  be associated  with  a  weight  matrix. We'll  call  this  thing  superscript one  and  superscript  two. I  think  today  I'm  going  to  omit  the  biases. I'll  write  down  the  formula  with  the  biases. And  then  we're  just  going  to  ignore  them today  because  I  don't  want  to  have  to  deal  with  the, our  hidden  layer  vector. So  we  have  a  vector  a  which  consists  of 12.3  Just  collecting up  those  three  things. This  is  equal  to  one  times  x  plus our  bias  vector  B,  one,  that's  a. The  way  that  we  get  a  is  just  by multiplying  our  input  features  x  by  this  matrix. Remind  me  what's  the  connection  between this,  Let's  write  it  over  here. So  we  have  one,  where  are  the  dimensions  of  one? We  know  what  they  have  to  be. Where  are  the  dimensions  again? Three  to  three  by  two  is three  by  23.  Okay? That's  right.  Any  I think  about  these  things  as  vectors, you  can  think  about  them. I  think  about  the  a  and  the  x  as  vectors that  usually  matrices  In  deep  learning, people  think  of  as  the  weights that  are  being  learned  in  the  model. Not  the  things  that  are  being  computed from  the  inputs  and the  learned  weights  as  like  an  activation. We  think  about  those  as  activation,  but  you're  right, the  input  x  two  by  one, the  output  is  three  by  one. That  means  we  had  to  have  a  three  by  two  matrix to  translate  from  one  into  the  other. We're  mapping  into  a  new  feature  space. Here  we  have  three  by  two, it  has  six  elements. We'll  call  this  thing,  look, this  is  terrible,  I'm  very  sorry. But  we  have  to  have  three  scripts associated  with  everything  in  this  matrix. Yeah,  very  unfortunate.  But The  way  that  I  am  always  thinking  about  this  is  that the  first  subscript  indicates what  row  you're  in  and the  second  one  indicates  what  column  you're  in. How  do  I  map  these  things  onto  this  picture? Here I have  my  weights in  my  weight  matrix. What's  the  connection  between the  weight  matrix  and  this  picture?  The  arrows. Yeah, first, fantastic. It's  a  little  hard  to  keep  track  of  that  in  your  head, especially  without  flipping  them. But  basically  each  of  these  arrows  here, we  can  think  about  this  as  an  arrow. I  haven't  drawn  like  the  arrow  head because  we  conventionally  just don't  do  that  for  diagrams  like  this. But  we  think  about  there  as  being  an  arrow  going from  x  one  into  a  one. We  can  think  about  what's the  strength  of  that  connection. The  strength  of  that  connection  is  this  weight  here. That's  how  much  does  this input  feature  influence  this  feature? A  one,  a  larger  number, especially  larger  in  magnitude indicates  a  stronger  connection. That's  where  111  goes. We  could  do  that.  Let's  just  draw  this  one. What's  the  arrow,  how do  we  label  the  arrow  that  goes  from  x two  to  3132 superscript  one, subscript  32  goes  into three  from  two,  just  like  you  said. We  think  about  these  as  the  strength  of connections  between  input  and  output  features. Are  we  done  with  the  hidden  layer? I've  said  how  do  you  compute  the  a  vector  a  123. We've  explained  that.  Just  matrix  multiplication. Are  we  done  with  the  hidden  layer? Yeah,  I  did  that  over  here. Yeah,  I  am  going  to  be  ignoring  them  in  general, but  you're  right,  we  have  to  add  biases.  What  else? Actually,  what  was  the  crucial  thing that  we  learned  about  last  class? Yeah.  Okay,  Non  linearity.  That's  right. It's  going  to  be  the  most  important  thing  is that  it's  not  linear  means basically  matrix  multiplication. It's  something  that's  not  just  matrix multiplication  or  addition  by  a  constant  vector. We're  going  to  have  a  new  vector  h. I'm  indicating  outside  of these  circles,  123. In  our  case  we'll  say  it's  relu. But  really  the  most  important  thing is  that  it's  a  non  linearity. It's  relu  of  a  relu  of  a  vector. Remember  relu  is  this  function  relu  of  a  number. Z  is  equal  to  z. Z  is  greater  than  zero. It's  equal  to  zero. If  z  is  less  than  or  equal  to  zero, that's  our  function  u  that  we  saw  last  time. I'll  just  note  here  the  most  important  thing is  that  this  is  a  non  linearity, or  a  non  linearity  in  general  apply  to  a  vector. What  that  means  is  we  just  apply  it  to each  individual  coordinate  in  the  vector. Any  questions  so  far?  No.  Okay. Then  we  have  one  final  layer  to  deal  with  which is  we're  going  to be  learning  some  more  about this  final  layer  because  it's  very  important. Our  output  layer,  what's  that? That's  going  to  be  consisting of  three  elements,  12.3  output. This  is  equal  to. The  matrix  two  times  H  plus  a  new  bias  vector  two. Now  imagine  that  we're  doing  sentiment  classification. Sentiment  analysis. What  we  want  to  do  is  classify our  input  vector  as happy,  sad,  or  neutral. We  have  three  possible  categories  here. What's  the  relationship  I  get  in  the  vector? I  want  to  say  this  vector  is  extracted  from  a  document. I  summarize  the  document  in  some  way, transformed  it  into  the  vector  x. I  want  to  say,  was  the  document  happy,  neutral? How  do  I  use  this  network  here  to  solve  this  problem? Let  me  ask  a  slightly  different  question. Would  you  know  how  to  use  a  network  like  this  to solve  this  problem  if  I  was  just  trying  to  do binary  classification,  happy  or  sad, let's  say  I  knew  there  were  only  two  possible  classes, how  could  I  use  what  we've  done  so  far  in  the  class and  this  network  or  network like  it  to  do  happy  versus  sad. Binary  classification. Yeah.  Way.  Which  weights? One.  Which  one? Layer  one.  Okay.  I  mean, you're  right  that  there's  going  to  be  the  weights  will be  picking  up  on  stuff  related to  whether  it's  happy  or  sad. Absolutely.  I  think,  I mean  in  a  slightly  more  like  mechanical  way, how  could  I  look  at,  how  could  I run  my  vector  X  through this  network  and  then  get  a  prediction? What  I  want  to  get  is  a  prediction. Is  this  happy  or  sad? It  doesn't  have  to  be  a  good  prediction  yet. We'll  worry  about  that  later. I  just  want  a  prediction. What  does  it  mean  to  get  a  prediction  about whether  something  is  in  one  class  or  another? What's  our  approach  to  doing  that?  In  this  class? Yeah.  Do  you  actually  even  need  to? You  could  use  two.  You're  right. Okay,  so  let's  say  we  had  two  output  notes. What  would  we  do  with  those  two  output  notes? So,  the  output  notes,  let's  say  1.2  right? Is  that  what  you  is  that  what  you  have  in  mind? Okay.  So  we're  going  to  use  those  output  notes? That  that's  correct,  Yeah. We  would  like  to  interpret one  with  the  sigmoid.  Exactly. So  what  is  that  where  you  were  going  to  say? Yeah.  So  why  did  we  use  the  sigmoid  four  before? How  did  we  use  the  sigmoid?  Yeah. Okay.  So  you're  right. That  was  the  whole  logistic  regression, so  the  fact  that  it  was  only  a  line, the  fact  that  it  was  a  line  was because  it  was  logistic  regression and  we  like  it had  to  do  with  the  exact  formula  that  we're  using. So  it's  not  going  to  be  quite  a  line  anymore. It's  a  little,  we'll  get  to that  whether  it's  actually  a  line. But  let's  continue  on  the  sigmoid  point  of  view. Let's  say,  let's  look  at our  sigmoid  function.  We  have  sigmoid  of  Z. This  is  equal  to  either  the  over  one  plus  e  z. What  does  this  function  do? It  does  this.  Why  is  that  good? Yeah,  it  does. It  outputs  values  0-1  The  sigmoid  function transforms  a  number  z  into  a  probability. We  can  use  this  for  binary  classification. We  take  a  network, we  run  out  our  input  vector  through  this  network, we  get  out  a  single  number. We  run  it  through  a  sigmoid. That  sigmoid  has  transformed what  we  gave  it  into  a  probability. We  can  interpret  that  probability  as  what's the  probability  of  being happy  or  what's  the  probability  of  being  in  class  one. Okay,  if  we  only  wanted  to  do classification  of  a  binary  classification, yes  or  no,  true  or  false, we  can  just  use  the  sigmoid. What's  more  complicated  here  is  that  we have  three  classes,  not  two  anymore. This  is  multi  class  classification, it's  no  longer  binary. What  do  we  do  with  multi  class  classification? Let's  introduce  something  new  for  that. It's  called  the  soft  soft  Max  is a  slight  generalization  of  the  sigmoid  function. Softmax  of  a  vector. It  takes  in  a  vector. Let's  assume  for  right  now, let's  say  that  Z  has  three  elements  in  it. We  have  a  three  element  vector  z,  Softmax  of  z. It's  going  to  take  in  this  original  vector and  it's  going  to  return  three  new  numbers. But  you  can  think  about  this  new  thing  as  a  vector  or a  list,  doesn't  really  matter. It's  going  to  be  either  the  one over  either  the  one  plus  e  of  the  Z  two, either  the  z  three,  that's  going  to  be  the  first  element. The  second  element  is  going  to  be  either the  two  over  the  same  thing. The  third  element  will  be  e, the  three  over  the  same  thing. What  do  we  notice? At  these  three  numbers, we  z  as  an  arbitrary  vector. The  numbers  inside  of  z  can  be  any  numbers  whatsoever. We're  taking  an  arbitrary  vector, we're  running  them  through  this  function. We  get  out  a  new  vector  that  looks  like  this. Why  do  we  notice  about  this  new? Yeah,  all  the  values  are  0-1  That's  definitely  true. What  else  do  we  know? Yeah,  all  the  denominators  are  exactly  the  same. That's  an  important  observation that's  connected  to  our  final  one. Yeah,  they  sum  up  to  one. If  you  sum  all  these  numbers  together, well,  they  have  the  same  denominator. That  means  when  you  sum  them  together, you  can  just  add  the  numerator  and then  divide  by  the  denominator. What  happens  when  you  add  all  of  these  numerators? You  get  one  plus  two  plus  three  weight. That's  what's  in  the  denominator. It's  equal  to  one.  Okay? All  of  these  numbers  are  individually  0-1  Together, they  sum  up  to  one.  What  does  that  mean? It  means  we  have  a  probability  distribution  here. We've  transformed  an  arbitrary  vector  of numbers  into  a  probability  distribution. We  can  interpret  this  as the  probability  of  class  one,  probability  of  happy. This  is  the  probability  of  Sat. This  is  the  probability  of  neutral. Any  questions? What's  our  final  step? Here  we  have  our  output  layer, going  back  to  our  MLP. We  have  our  output  layer  where  we  computed  these  numbers, 12.3  We're  then  going to  run  that  vector that  we  just  computed  through  a  soft  max. Let's  just  write  it  down  here. Our  final  step  is  compute  probabilities. We  compute  our  class  probabilities  by running  the  final  layer  through  a  soft  max. There's  different  ways  of  thinking  about  this. One  way  of  thinking  about  this  is that  we  started  off with  three  classes  that were  all  mixed  together  in  some  way. Imagine  that  our  classes  looked  like  this. We  have  O's,  and  Xs  and  stars. Here's  what  our  O's  looked  like, here's  X  looked  like, here's  what  our  stars  look  like. In  practice,  our  data  will  never  be  this  pretty. But  we'll  just  see  the  general  point here.  Okay,  that's  my  star. We  have  a  bunch  of  stars, we  have  a  bunch  of  Xs, we  have  a  bunch  of  S.  We want  to  classify  these  things.  We  have  three  classes. There's  no  set  of  lines  that  we  can write  down  that  will  correctly  classify  this. If  we  throw  our  logistic  regression  at  it, it's  going  to  be  a  chance  we  point  a  line  in any  one  direction.  Doesn't  do  anything. Our  MLP  is  going  to  map. We  have  our  MLP,  MLP, let's  say,  to  the  final  hidden  layer, which  in  this  case  is  the  only  hidden  layer, MLP  through  hidden  layer, we  look  at  the  output  from  the  final  hidden  layer, that  final  tractor  there. What  we're  hopefully  going  to get  is  something  that  looks  like  this. It's  mapped  all  these  points  into  a  new  space. If  it's  done,  then  we  can think  about  trying  to  learn  three  different  lines. One  line  that  points  in  the  direction  of  the  stars, one  line  that  points  in  the  direction  of  the  Xs. One  line  that  points  in  the  direction  of  the  O's. Something  like  that.  We're  going to  learn  three  different  lines. The  final  layer  we  can  think  of, you  see  this  multiplication  here. What's  the  connection  between  these  three  lines? And  that  multiplication  here, where  we're  computing  the  output  layer by  taking  two  times  h,  What's  the  connection? Let  me  let  you  think  about  that  for  a  minute. This  is  an  important  thing  to,  to  think  about. What  does  a  dot  product  represent? We  can  think  about  a  matrix  multiplication  is consisting  of  multiple  dot  products, one  dot  product  per  row. During  matrix  multiplication,  we  take  our  vector, we  do  a  dot  product  with  the  first  row. We  take  our  vector, we  do  a  dot  product  with  the  second  row. We  take  our  vector, we  do  a  dot  product  with  the  third  row. Matrix  multiplication  is  just  a  way  of collecting  up  multiple  dot  products. A  dot  product  basically measures  how  much  are our  two  vectors  pointing  in  the  same  direction. It's  not  quite  that  because there's  like  a  magnitude  thing. But  basically  what  you're  measuring  is  how much  do  these  two  vectors  point  in  the  same  direction? You  get  a  big  positive  number if  they're  pointing  in  the  same  direction. You  get  a  very  negative  number if  they're  pointing  in  opposite directions  and  something  in  between. Otherwise,  we  can  think about  each  row  in  that  matrix  two. We  have  our  matrix  W  two  here. Each  row  in  that  matrix  corresponds  to a  different  line  in this  classification  problem.  One  line. One  arrow  says  this  is  the  direction  of  the  stars. One  arrow  says  this  direction  of  the  Xs, one  arrow  says  this  direction  of  the  Os. We're  taking  dot  products  to  measure  which  of those  directions  does  a  particular  point  go  the  most  in. Then  that's  what  our  numbers  10203  represent. They  measure  the  magnitude, or  they  measure  the  result  of  that  dot  product, the  sine  and  magnitude  of  that  dot  product. Let's  go  one  step  further. They're  measuring  the  sine and  magnitude  of  that  dot  product. That  means  how  much? One  means  how  much  are  you  happy. Two  means  how  much  are  you  sad. Three  means  how  much  are  you  neutral. We  have  these  three  numbers. We  feed  those  three  numbers  through  a  soft  max. What  do  you  notice?  Let's  say the  first  element  we  Z  one  is  how  happy  are  you? It's  a  number  that  represents  that. If  one  is  bigger, what  effect  does  that  have  on  the  soft  max, on  the  output  probabilities? Yeah,  it  gives  a  higher  chance  of  being happy  if  you're  pointing more  in  the  direction  of  the  happy  vector. That  means  the  probability  that you're  happy  ends  up  being  higher. If  you're  pointing  more  in  the  direction  of  a  sad  vector, the  chances  of  you  being  sad  higher. Same  for  neutral.  If  you  understand  this, you  understand  deep  learning, this  is  what's  happening. We've  really  gone  through  everything  here. We're  going  from  a  space  that  is  not  linearly  separable, where  you  cannot  draw  lines  to  classify  things. We're  transforming  that  space  into  a  new  one, where  things  become  linearly  separable. We're  finding  the  correct  lines for  separating  the  categories. Measuring  how  much  each  point  agrees  with  those  lines, like  how  much  it's  pointing in  the  direction  of  those  lines. Then  transforming  those  numbers, transforming  those  measurements  into  probabilities. Any  questions  about  this?  Yeah,  exactly. One  arrow  per  row  of  W  two,  Yes. No,  this  is  the  sigmoid  function. Yeah.  If  you  need  back  a  probability, these  are  the  only  two  functions  that  people  use. There  are  other  functions  that  you  could  use. People  do  not  use  them. They  are  believed  to  have  special  properties. I  mean,  like  you  can  give  theoretical  justifications. I  don't  know  how  much  that  they  work  in  practice. Let  me  put  you  like  that.  Yeah.  When  you need  back  a  probability  distribution, this  is  what  people  do. Any  other  questions?  Yeah,  When  you  have  21. Great  question.  Any  others?  No.  Okay. So  the  final  thing  for  us  to  do  is  define a  loss  function  and our  loss  function  on  a  particular  data  point. So  we  have  a  data point  theta, we'll  think  of  as  basically  something that  collects  up  all  of  our  weight  matrices  and  biases. Those  are  the  things  that  we're  learning. The  things  that  we're  learning  when we're  using  this  network. The  weight  matrices  in  the  biases. Everything.  Once  we  know the  weight  matrices  in  the  biases, our  function  is  fully  defined. There's  nothing  else  to  know. Look  at  the  equations  here.  The  only  things that  you  have  to  fill  in  the  details  theta will  be  basically  a  vector  that  just  collects all  of  those  numbers,  the  loss. Of  theta  on  D. It's  just  going  to  be  the  negative  log  probability of  D  given  theta. We've  seen  this  formula  before. This  is  equal  to  negative  sum  over  our  dataset. I'll  assume  there  are  data  points  in  the  dataset. Log probability given  theta, how  do  we  compute  the  probability of  a  particular  data  point? D  I,  we're  only  looking  at  classification  problems. I  think  almost  everything  we're  going  to  look  at  in this  class  is  ultimately  a  classification  problem. What  we  measure  is what's  the  probability  of  getting  the  correct  class? We  have,  we  have  the  input  x, that's  our  input  features. We  have  an  output  class,  Happy,  sad,  neutral. We  measure  what  was  the  probability of  getting  the  correct  class. We  can  now  compute  that  probability  from  our  softmax. Our  softmax  gives  us  what's  the  probability  of each  class  given  this  input  vector. We  then  measure  what  was  the  probability of  the  actually  correct  class. Okay,  this  is  a  full  pipeline  now. We  can  take  an  input, run  it  through  our  MLP, get  out  a  prediction  of  what  class  you're  in, and  then  measure  the  loss on  that  class  or  on  that  data  point. Is  anyone  here  thinking  of  starting  a  company? I'm  not  like  trying  to  steal  your  ideas  or  something. Uh,  no. I  mean,  there's  never  been  a  better  time  to start  a  software  company  than  right now  because  you  don't have  to  know  very  much  about  software. Look,  I  don't  like  there's all  these  annoying  parts  of  building  stuff, like  having  to  write  lots and  lots  of  code  that's  annoying. Nobody  should  find  that  fun. Some  code  can  be  a  little  fun, but  most  of  it's  annoying. And  now  you  mostly  don't  have  to  do it  like  the  ideas  for  what  you  should  be  writing, those  will  have  to  be  yours,  either. It'll  still  be  hard  work. There's  no  doubt  that  building a  company  is  difficult  no  matter  what. The  ease  of  doing  this  has  never  been  greater. That's  a  great  question.  How  many  of your  peers  know  about  this? Everyone  here  does. Yeah,  yeah,  yeah,  yeah,  not  in  this  class. Look,  the  word  just  has  not  gotten  out  yet. The  world  is  slow  to  adjust. It's  more  than  that,  actually. There's  a  lot  of  legacy  institutions, existing  companies  basically,  that  are  not going  to  be  able  to  adjust  to this  because  they  have  existing  processes. They're  going  to,  every  company  is  trying  to rebrand  itself  right  now  as  an  AI  company. But  very  few  of  them  will  succeed. Just  the  way  the  organizations work  is  they  have  a  way  of  doing things  and  you  cannot  change  it. Way  to  change  organizations  is  to  build  new  ones. There  will  be  a  new  breed  of  organizations that  actually  use  these  technologies  effectively, but  they're  not  going  to  be  existing  ones  out  there. You  can  now  compete  them  right  now. You  can  do  exactly  the  same  job  as  them  for  much  cheaper. Yeah, see  where. Yeah. Yeah.  I  don't  know. Those  companies  are  not  going  to  last. You  might  be  able  to  make  some  money right  now  from  it.  I'm  not  sure. I  think  if  I  was  trying  to  build a  company  that  I  wanted  to  last more  than  six  months  or  a  year,  I  would  not  do  that. It  would  not  just  be  a  thin  wrapper around  GPT  four.  Because  anyone  can  do  that. There's  going  to  have  to  be  like some  details  that  you  get  Right. That  are  a  little  tricky  to  get  right. You  have  to  know  the  area  to  do  that. Yeah,  I  think  all of  the  software  barriers to  doing  a  company  have  gone  away. Doesn't  mean  that,  like  all  the other  stuff  has  gone  away  though. All  that  other  stuff  has  gotten  easier  as  well. But  I'd  say  if  software  was  at  all  an  impediment, you  should  think  about  what  can  you  do  now  that you  would  have  just been  such  a  pain  to  do  six  months  ago? I  don't  know. I'm  curious.  I'm  handing all  of  you  millions  of  dollars  on  a  silver  platter. Yeah.  Example,  office  rust  in  a  few  days. Yeah.  Basically,  just  like  this  Rust  program, the  book  generally,  what  do  you  go  through  the  book  on? Period  of  time  examples? Yeah.  I  just  looked  at  each  of  the  chapters. There's  the  objection  to  explain  that. And  then  I  asked  the  questions, asked  what  happened  to  program  that  works  really  well. Did  you  actually  like  so  you said  you  taught  yourself  rust. And  that's  the  only  place  where  I'd  say  you  went  wrong. Sorry.  But  why  even  bother  learning  it? Okay.  Fine.  Right.  But  that's like  for  your  own  intellectual  stimulation. That's  not  Yeah,  it's  true. No,  no,  you're  right,  you're  right. Okay.  Yeah,  yeah,  yeah, Yeah,  that's  a  great  example. Yeah,  thanks  for  sharing  that. Okay,  so  we've  talked  about  soft, we  talked  about  our  loss  function  here. Okay,  we  have  our  basic  MLP. Now  I  want  to  talk  about  how we  actually  learn  the  weights  in this  MLP  should  be  the  big  mystery  so  far. We  feed  our  vector  through  this  MLP. We  multiply  it  by  a  bunch  of  weight  matrices, do  some  non  linearities, we  get  an  output  probability  for  each  class. What  we  do  in  deep  learning  is  we randomly  initialize  those  weights, we  write  down  our  network, and  we  literally  just  randomly  sample  W  one  and  W  two. And  the  biases,  we  sample  them  from something  similar  to  just  a  normal  distribution. If  you  randomly  sample  your  weights, then  the  predictions  that your  model  will  give  will  be  trash, because  those  weights  don't  know  anything,  just  noise. How  do  we  transform  a  network  that  gives  us trash  into  a  network that  actually  makes  good  predictions? The  way  that  we  do  that  is through  stochastic  gradient  descent. I'll  write  down  the  gradient  descent  formula rather  than  the  stochastic  gradient  descent  formula. Just  because  it's  a,  it's  a  little  easier  to  write. But  we  should  really think  about  this  as  being  about  stochastic  Rad  incent, where  you're  only  sampling  a  subset  of  the  data  SGD. Stochastic  gradient  descent  says optimize  your  weights  and  biases. We've  collected  them  up  in  our  vector  theta. By  minimizing  loss, we  have  theta  is  equal  to  theta  minus  epsilon, our  learning  rate,  times  the  of  our  loss  function. Given  our  dataset, what  do  we  remember? The  gradient  of  the  loss  on the  dataset  is  equal to  a  sum  over  all  of  the  data  points  of. Of  the  loss  on  an  individual  data  point. Sum  up  all  of  the  losses  on  the  individual  data  point. What  this  means  is  that  we  only  have  to  look at  one  data  point  at  a  time  or a  subset  of  data  points  at  a  time in  order  to  compute  this  gradient. The  way  that  we  optimize  the  MLP, it's  going  to  be  exactly  the  same  way  that  we optimize  logistic  regression  or  linear  regression. We're  going  to  minimize  the  loss. Minimizing  the  loss  on  a  sentiment  analysis  task, on  classification  just  means maximize  the  probability  of  the  correct  classes. We're  going  to  incrementally, step  by  step,  find  the  weights, find  the  matrices  that  produce the  most  accurate  classifications on  our  dataset,  on  our  training  set. Now,  what's  the  major  impediment  to  doing  this? What's  preventing  us  from  doing  this?  Right  now? Yeah.  Okay,  Right. Okay.  What  complexity? Just  at  a  high  level, what's  the  thing  that  that  makes  complicated? We  have  these  multiple  weight  matrices  that create  some  complications  finding  the  gradients. How  do  we  compute  this  thing? Here  we  have  a  particular  data  point  D, how  do  we  compute  the  gradient  of  the  loss  function? Okay? The  answer  to  that  is called  the  back  propagation  algorithm. How  the  gradient  that's  through  back  propagation. So  that's  what  we're  going  to  learn  about  right  now. Propagation.  Maybe  I  spelled  that  correctly. Okay,  Back  propagation  was  invented  at  UCSD. May  be  in  this  building, mentioned  the  connection  to  neural  networks  before. But  this  is  the  deep  learning  algorithm. It  was  invented  here.  The  funny  thing  about this  algorithm  was  also  invented simultaneously  in  a  bunch  of  other  places, which  makes  people  very  angry. The  person  who  won  the  major  award  for  back  propagation, he  was  the  one  here,  Jeff  Hinton. You're  going  to  see  you  could  have  learned  about back  propagation  when  you were  taking  calculus  in  high  school. You  just  need  intro  to  calculus, basically  to  do  back  propagation. It's  just  the  chain  rule. So  that  means  that  Isaac  Newton could  have  invented  back  propagation. He  may  have,  for  all  we  know, but  it  wasn't  very  relevant  at  that  time. Let's  talk  about  it.  What  do  we  do  with  back  propagation? Let  me  draw  as  a  little  bit  more  detail  here. We  have  softmax  of  O. We  have  the  output  from  our  MLP. We  pass  that  into  a  softmax,  we  get  the  soft  max. And  then  we  compute  the  loss  on  that. What's  the  probability  of  the  correct class  Negative  log  probability of  what  we  do  in  back  propagation? We  work  from  the  end  of  the  network  to  the  beginning, that's  why  it's  called,  we  work backwards  he  back  propagation. The  first  thing  that  we're  going  to  do  is  see  how changing  the  output  from  the  softmax  changes  the  loss, the  softmax  output  and  probabilities. How  does  changing  those  probabilities  change  the  loss? That's  the  first  thing  that  we're  going  to  measure. Next  thing  that  we  measure, how  does  changing  the  numbers  that  go  into  the  softmax. How  does  that  change  the  loss? The  way  that  we're  going  to  do  that  is  by  first  measuring how  do  changing  the  O's  change the  output  of  the  soft  max. Now  we  already  know  how  changing  the  output  of the  Softmax  changes  the  loss. We're  basically  following  like  the  chain of  effects  that  the  different  parts  of  the  model  have. Every  time  that  we  change  one little  thing  inside  the  model, it  changes  something  else  which  then changes  something  else  which  eventually  changes  the  loss. And  we're  just  trying  to  trace  all  of those  changes  as  they  happen. Let's  start  with  step  one. Step  one,  we're  going  to  measure. Let's, let's  say  we  have  a  data  vector  D, D.  It's  equal  to  a  pair. I'm  abusing  notation  here  because  vectors  are. I  think  we  know  what  I'm  talking  about  is a  pair  that  consists  of  an  input  x  I, and  a  label  class. And  the  class,  let's  say,  is  equal  to  happy. That  means  that  the  loss  of  D  given theta  is  equal  to  the  negative  log  probability  of  happy. Let  me  actually  just  get  rid  of  this  X  I  here,  given  x. I'm  abusing  some  notation  here  because  actually, if  we  wanted  to  make  the  notation  totally  precise, it  will  take  me  twice  as  many  characters. I  don't  want  to  write  it.  You  don't  want  to  write  it. Let's  just  make  sure  we  know  what  we're  saying  here. Probability  of  happy  given x  and  theta.  What  are  we  saying? We  have  a  network  with  weights  given  by  theta. We  input  into  that  network  of  vector  x. That  input  works  its  way  up  to  the  very  top. We  run  it  through  a  soft  max. We  compute  the  probability  of  three  categories, happy,  sad,  and  neutral. We  pick  out  the  probability  of  happy, that's  what  the  probability  of  happy  is, not  necessarily.  They  could  be  perfect. We're  assuming  them  to  be  arbitrary, let  me  put  it  like  that. But  they're  fixed,  but  we  don't  know,  for  us, we  don't  actually  care  about  what  they  are  learning  this, but  they  could  be  any  numbers  whatsoever. Any  other  questions? Okay,  we're  computing  the  loss which  is  negative  log  of  the  probability  of  happy. Step  one,  which  is compute  how  does  the  loss  change when  we  change  the  probability  of  happy. Let  p,  x  and  theta. Let's  just  give  that  a  name.  Let's  call  it. It's  just  a  name.  Now  we're  going  to compute  the  partial  derivative of  the  loss  given  P. If  we  change  a  little  bit, the  probability  of  happy  a  little  bit, how  is  the  loss  change?  So  what's  the  answer  to  that? The  loss  is  negative  log  of, I'm  assuming  is  I  can  just change  this  thing  a  little  bit. I'm  I'm  saying  we'll get  to  how  we  actually  change  it  soon, but  like  mechanically,  how  do  we  change  it? I'm,  I  can  change  this  thing  in  here  a  little  bit, and  I  want  to  know  how  is  my  loss  change? What's  the  answer  to  that? Okay.  Maybe  there's  something  So this  was  not  meant  to  be  a  confusing  question, which  means  there's  something deficient  in  my  explanation.  So  let's  fix  it. So  what's  confusing  about the  way  that  I'm  setting  this  up? Can  someone  help  me  by  saying  what  they're  stuck  on  here? Yeah.  Okay.  Okay,  good. It's  a  derivative  question.  That's  good.  Okay.  Yes. Well,  I  mean,  we're,  we're  assuming  that  we  can change  itself  negative  one  over  p. Yes,  I  had  a  question  about  yes. When  you  say  yes  like  natural, what's,  let's  assume  natural  log  people everyone  uses  natural  log. I  mean,  there's,  I  mean, if  we  didn't  want  to  use  natural  log, then  taking,  you  know, it's  just  a  matter  of  rescaling  by  some  factor. But  yes,  let's  assume  natural  log,  correct? Like  everywhere  you  see  log. Yes,  I  mean,  I  may  have  like, let's  say,  I'm  not  going  to  guarantee that  everywhere  else  you're going  to  look,  you'll  find  base. But  everywhere  in  this  class  where  we  say log  unless  otherwise  stated, it's  as  base  makes  things  very  simple. Okay.  Any  other  questions  about  this? So  there  was  some  memory  issues  about  how  to  take the  derivative  of  log  those.  Okay. But  other,  anything  else  that  came  up  in  this  so  far? Yes,  no,  no. Yeah.  Oh,  okay. I'm  assuming  that.  Okay,  great  question. I'm  assuming  that  our  data point  that  we're  using  right  now, the  correct  class  is  happy. And  what  we're  trying  to  do  is maximize  the  probability  of  the  correct  class. Which  means  minimize the  negative  log  probability  of  the  correct  class. I'm  just  making  if  the  data  point  was  different, if  it  had  a  different  class,  then  we would  have  sad  or  neutral  here. But  it's  going  to  be  some  particular  class that  we're  trying  to  maximize  the  probability  of. Yes. So  we're  going  to  fill  out, it's  just,  I'm  assuming  it's  P  and then  we're  going  to  see  how  to actually  compute  that  probability. Exactly.  Yeah,  I  look, the  correct  formula  for  this  probability is  you  have  to  run  this through  the  entire  MLP  and  we  don't  want  to have  to  worry  about  all  of  that  stuff  at  once. We  want  to  break  it  down  piece  by  piece. Okay,  let's  move  on  to  step  two. We  started  here,  basically  it's  like softmax  outputs  of  probability  that  computes  the  loss. We  asked  how  does  changing the  probability  change  the  loss? Now  what  we  want  to  ask  is  how  does  changing the  stuff  inside  of  change  the  loss? Our  goal,  let  me  just  be  clear, this  was  our  goal  in  step  one. Now  our  goal  is  to  compute  partial  derivative  of the  loss  with  respect  to  I. I.  Change.  One  of  these  is  how  does  that  change  the  loss? We're  going  to  break  that  into  two  steps. We're  first  going  to  see  how  does  changing the  O's  change  the  soft  max, then  how  does  changing  the  soft, the  output  from  the  soft  max change  the  loss? Okay? The  way  that  we're  going  to  do,  think  about  this, is  in  terms  of  function  composition. That's  where  the  chain  rule  is  going  to come  in,  the  loss. I'm  just  going  to  repeat  our formula  that  we  have  up  there. It's  negative  log  probability  of  x, of  happy  given  x  and  theta. Let's  expand  on  this  a  little  bit. This  is  equal  to  negative  of. Let  me  change  our,  let me  make  things  a  little  bit  simpler. I  was  going  to  I  was  going  to  combine  two  steps  together. Let  me  actually  change  this  just  very  slightly. Do  I  want  to?  No,  let's  keep  it  the  same. Thanks. Okay,  we take  our  vector, this  is  like  the  weight  that  each  class  gets. We  run  that  through  a  soft  max. How  do  we  get  the  probability  of  happy  from  that? Well,  we  go  into  this  vector here  and  we  look  at  the  first  coordinate. I'm  using  this  notation, this  little  subscript  one, to  indicate  get  the  first  coordinate  of  the  softmax. Okay?  So  we  have  our  vector  O. We  run  it  through  a  Softmax  to, it  gives  us  back  three  probabilities. To  compute  the  probability  happy, we  only  need  one  of  those, which  is  like  the  probability  of  happy. It's  the  first  coordinate  there, that's  what  we're  pulling  out. Okay?  This  is  equal to  negative  log  of  e  to  the  O1o, the  one  of  the  two  of  the  three. That's  just  the  definition  of  a  soft  max. This  thing  in  here  is the  probability  of  happy and  then  we  take  the  negative  log  of  that. What  is  the  chain  rule  tell  us? We  want  to  take  the  partial  derivative of  the  loss  with  respect  to  one. What  is  the  chain  rule  tell  us  to  do? The  chain  rule  says  you  take the  partial  derivative  of this  thing  and  then  you multiply  it  by  the  partial  derivative  of  negative  O. So  it's  going  to  be  the  partial  derivative  of the  loss  with  respect  to times  the  partial  derivative  of  with  respect  to  one. Let's  just  make  this  clear.  This  is  negative  log  of p  is  equal  to  this  thing. We  see  how  does  changing  change  the  loss, how  does  changing  one  change? And  we  multiply  those  two  things  together. I'm  seeing  some  nods.  Any  questions  about  this? This  is  the  chain  rule. This  is  a  little  bit  nasty  because  of the  form  of  the  soft  max  is  a  little  complicated, it  a  little  bit. Nasa  questions,  Okay. I  actually  don't  want  to  compute  this here  because  it's  like  slightly  painful. It's  not  that  bad,  but  definitely more  than  the  rest  of  this  board  space  here. It's  not  grade  of  our  time. Taking  a  derivative  like  this  is  not a  good  use  of  our  time,  so  I'm  not  going  to  do  it  here. Happy  to  talk  about  it  on  Piazza. If  people  have  questions  about  how  to  Dei, the  point  is  this  thing, right,  is  equal  to  either one  over  that  it's  the  first  coordinate,  the  softmax. How  do  you  take  the  partial  derivative of  that  thing  with  respect  to  one? It's  just  a  little  bit  more  chain  rule.  That's  all  it  is. Okay?  I'll  assume  that  we  can  already  do  this. Let's  think  about  this.  This  is  actually  great. We  did  this  for  one. We  could  do  this  for  2.3  as  well. It's  a  little  bit  different  because 2.3  only  appear  in  the  denominator  words, one  appears  in  both  numerator and  the  math  is  a  little  different. But  it's  the  same  thing. It's  just  the  chain  rule.  Let's  try to  get  an  understanding  of  where  we  are. Now  we  have  now  using  the  chain  rule, we've  measured  how  does  changing  one  change  the  loss. The  way  that  we  did  that  is  by  measuring  how  does one  change  the  output  from  the  softmax. How  does  changing  the  output from  the  soft  max  change  the  loss? Let's  go  one  step  further  now. What  we're  going  to  try  to  understand  is  how  does changing  each  of  these  weights  change  the  OI? Over  here  we  have  211. This  is  from  weight  matrix  two, going  from  the  first  activation here  to  the  first  activation  here, the  first  node,  to  the  first  node. That's  W211.  Step  three  Now  is  to compute  the  partial  derivative  of the  loss  with  respect  to  each  of the  weights in  the  second  weight  matrix  two  IJ. Let's  focus  on  just  the  first  weight, Let's  specialize  to  that. We'll  consider  how  does the  loss  change  when  we  change  211, the  very  first  element  of  the  second  weight  matrix. So  what  do  we  do? I  want  to  know  how  does  the  loss  change when  I  changed  this  thing  here? What  should  my  approach  be? Yeah, okay, What's  the  name  for  the  rule  that  we  use? The  chain  rule.  We're  going  to  use  the  chain  rule  again. We're  going  to  measure  how  does changing  that  weight  change, how  does  changing  one  change  the  loss? We've  already  figured  out  how changing  one  changes  the  loss. That  was  right  here  and  here. Now  we're  just  going  to  figure  out how  changing  the  weight  changes one partial  derivative of the  loss  with  respect  to  211. Let's  actually,  before  we  actually  get  there  directly, what  I  want  to  know  first  is  what  is  one  equal  to? One  is  equal  to. Well,  how  do  I  compute  one? Yeah. Uh,  good.  Thank  you. That's  not  full  one  yet. Not  quite.  How  many  arrows  are  going  into  one? 12 times  h22. Plus  the  bias. It's  going  to  be  21  first  term in  the  bias.  For  the  second  layer. Yeah,  subscripts,  they're  painful, There's  no  way  around  it  if  we want  to  actually  have  something  that  makes  sense. Okay.  I  have  a  very  simple  formula  for  one  here. This  is  a  linear  equation,  right? It's  just  a  bunch  of  stuff  that we  multiply  together  and  sum  up. Yeah,  we  can  absolutely  write  it  as  a  product. That  notation  will  not  be helpful  here  because  now  our  goal  is  to  take the  partial  derivative  of  one  with  respect  to. This  is  actually  a  much  clearer  way  to  see  that. You're  right,  though  we  can  write  it  as a  dot  product.  It  is  a  product. What's this one  partial  derivative of  the  loss  with  respect  to  11211. What's  the  sequel  to?  What's  my  chain  rule  here? Yeah,  question  of  one. Great,  we  just  computed  this. It's  one.  And  what  about  this  one? Well,  that's  what  we  computed  over  here. As  we  work  our  way  down, we  save  our  previous  work. We  compute  all  these  partial  derivatives. We're  going  to  save  those  partial  derivatives and  then  reuse  them  in  the  next  steps. What  would  be  next?  Now  I know  how  to  compute  the  partial  derivative. Here  I  have  a  formula  for  the  partial  derivative of  the  loss  with  respect  to  this  weight. I'm  going  to  repeat  that  for all  of  the  weights  in  this  layer. For  all  of  the  weights  in  this  weight  matrix  W  two, I  can  do  it  for  the  biases  to  the  math  will  all be  similar.  It's  a  little  different. There  are  slight  differences,  but basically  there's  not  going  to be  anything  surprising  about  that. Okay,  what's  my  next  step after  computing  the  partial  derivatives with  respect  to  the  loss?
