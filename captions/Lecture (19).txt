Okay,  let's  start. Any  questions  before  before  we  get  going, anything  about  the  final  project  you should  be  working  on  it  okay  s  some  details. The  final  project,  we  have presentations  in a  week  from  today,  that's  on  the  syllabus. The  presentations  will  be in  price  center  in  one  of  the  ball  room. I'll  send  the  exact  location. Basically,  each  group  should  bring  a  laptop. Make  sure  your  laptop  is  fully  charged. We'll  have  tables  set  up and  you'll  be  able  to  show  demos. What's  going  to  happen  is  that  I'll, most  of  the  groups  here  are  pretty  large. Most  people  from  each  group  can  walk around  and  see  presentations  from  the  other  groups. You  probably  want  to  switch off  who's  doing  the  presentations. It's  up  to  you  and  I'll  go  and  see  all  the  presentations. You  should  see  all  the  presentations. Yeah.  After  that,  you'll  have  another  week  in  the  day, so  another  eight  days  to  finish  things  up. Basically,  you'll  be  turning  in  your  final  project, like  the  final  product  on  Friday  of  finals  week. What  should  you  turn  in  there?  I'll  give specific  instructions  on  Piazza, but  it  should  be  all  of  your  code. And  then  two  other  things  besides  the  code. One  should  be  a  short  video that  demonstrates  what  you've  done. There's  a  video  version  of  demoing  it  and  then a  short  write  up like  a  page  or  two  basically  like  what  did  you, what's  still  outstanding  and  what's  not  working? What  did  you  find  more challenging  than  you  thought  it  would  be? Obviously,  I  want  you  to  be  candid  there because  I  want  to  learn  lessons  about  this. From  the  more  that  I  learn  about  this, you  have  something  that  fails for  an  interesting  reason.  That's  great. You  don't  need  to  cover  up  all  of  your  blemishes. If  you  have  something  that  fails  for  dumb  reasons, that's  bad,  like  because you  didn't  implement  it  or  something. But  like  if  you, if  you  really  tried  something  you're  like, no,  no,  no  GT  cannot  do  this. Or  I,  I  just could  not  figure  out  how  to  do  this  effectively. And  it's  clear  you  actually  tried  it  out.  That's  great. Lesson  learned. Okay.  Any  questions? Yes,  posession. It's  a  poster  session.  Except  without posters  on  your  computer? Yeah,  no  walking  around? Exactly.  Any  other  questions?  Yeah. I  mean,  it's  going  to  be  in  this  time  slot  next  Thursday, so  it's  just,  I  mean  it's  on  a  syllable. I  mean,  our  normal  class  time. It's  just,  it  will  be, instead  of  lecture,  it's  going  to  be  presentations. Any  others?  Okay,  so let's  talk  about  Transformers. So  we  work  through the  full  definition  of  the  transformer. Last  time,  we  have  a  sequence  of  words, so  we  have  a  sequence  of  layers  up  to  layer  K. Each  layer  looks  the  same  as  every  previous  layer, except  they  have  different  weights, different  numbers  in  the  weight  matrices. Then  finally,  we  get  an  output, We'll  call  these  outputs  the  Z's. There  are  different  tasks  that  we  can  solve here  we  can  consider just  auto  regressive  language  model that  is  predicting  the  next  word. We  have  our  target  here. A  target  meaning  like  this  is the  sequence  that  we  want  to  predict. Our  target  sequence  is the  original  sequence  that  we  input  into  the  model, except  shifted  to  the  left  by  one  that  is  given  one, we  want  to  predict  two, given  1.2  we  want  to  predict  three. We  want  to  predict  every  word from  all  of  the  words  that  came  before our  target  sequence  is  two  up  through. Then  we'll  also  say  we  want  to predict  EOS,  end  of  sequence. Es  means  end  of  sequence. This  is  what  we  also  want  to  predict. We're  going  to  be  defining  a  probability  distribution. Probability  of  a  particular  word. Let's  say  probability  of  word  I  plus  one  given. I'm  sorry,  I  made  one  small  error  here. The  target  sequence  are  not  actually  the  vectors. They're  not  the  word  vectors. They  are  the  words  themselves. It's,  it's  a  small  distinction, but  like,  you  know, what  we're  trying  to  predict  is  just  what the  actual  next  word  was  like. This  was  the  word  banana. We  want  to  predict  the  word  banana. Okay,  so  we  want  to find  the  probability  of  word  I  plus  one  given  word, One  up  to  word. How  do  we  do  that?  We  were  given  words  one  through  I. We  want  to  predict  the  next  word. How  do  we  predict  the  next  word? Given  what  we've  done  in  the  transformer  and  the  output, the  transformer,  what  do  we  do? The  transformer  takes  in  a  sequence  of  words. It  outputs  new  representations for  each  word,  one  through  z. We  want  to  use  these  representations in  order  to  predict  what  the  next  word  is  going  to  be. What  do  I  do? Multiply  those  representations  by the  output  matrix  and  then  vector. And  then  the  words  D  sampling  take  high  probability. Well  here  I'm  just  trying  to calculate  what's  the  probability of  what  you're  talking  about  for  generation. So  that's  great.  But  I  do  want  to know  for  the  correct  next  word, how  do  I  compute  this  probability? And  you  gave  me  most  of  the  ingredients. Okay,  so  you're  right  that  I'm  going  to  take  the, run  them  through  an  output  embedding matrix  to  transform  them  into  a  new  vector that  has  a  number  of  dimensions equal  to  the  size  of  the  vocabulary  at one  score  per  element  of  word  in  my  vocabulary. I'm  going  to  do  a  soft  max  that  gives me  the  probability  atribution  over  words. Perfect.  But  I  want  to  compute what's  the  probability  of the  word  that  occurs  in  position  one. How  do  I  do  that? There's  one  important  missing ingredient  in  what  you  said, which  is  which  Z  do  I  look  at  there? I'm  not  going  to  look  at  all  the  Z. I'm  predicting  this  particular  next  word. What  does  a  love  or  who  does  lama  love? I'm  going  to  have  Z  associated  with  this,  right? This  is  Z1z2. Which  Z  do  I  use  to  predict  what  word  comes  after? Loves.  The  three  Really, 222  is  going  to contain  information  about  the  entire  previous  sequence. Why  is  that?  It's  because  of  attention. Because  we  did  attention. Two  contains  information  about  both  loves  and  lama. We're  going  to  use  two  to  predict  the  next  word. In  general,  if  you  give  me words  one  through  I and  you  ask  them  to  predict  the  next  word, which  Z  do  I  look  at? I,  Great.  Okay,  so  this  is  going  to  equal  soft  Ma, embedding  matrix,  Output  embedding  matrix, I'll  call  it  O. My  output  embedding  matrix  times  Z, the  output  representation  for  the  word, that  gives  me  back  a  probability  distribution. And  now  I  want  to  find the  actual  probability  of this  specific  work  like  this  was  mama  here. And  I  now  need  to  look  up, where  did  mama  occur  in  this  distribution? I'll  call  that,  I'll  use  this  notation here  where  what  I  mean  is  this  is  like  a  list  of  numbers. I'm  just  looking  up  what's  the  probability  of this  specific  word,  this  list  of  numbers. Questions  about  this. Okay,  so what  was  it?  Yeah,  go  ahead. Yeah, exactly  because  what we're  inputting  into  the  transformer, Okay,  You  got  to exactly  the  question  I  was  going  to  ask  right  now. So  what  the  transformer  actually  sees is  it  sees  this  full  sequence.  Ma  loves  mama. Is  that  how  you  spell  mama? Probably  not.  That's,  it  doesn't  look  right. Let's,  let's  pretend  it's  right. I  don't  know.  I'm  sure  someone  spelled  it  that  way. So  we  have  the  transformer sees  each  of  these  three  words and  it  sees  them  at  the  same  time. Right?  What  we  get  out  from  this, if  we  input  a  three  word  sequence, we  actually  get  out  three different  next  word  prediction  problems. We  get  out  a  problem  that  corresponds to  predicting  what  comes  after  lava. We're  going  to  use  z  one  to predict  what  comes  after  lava. We  get  a  problem  of  what  comes  after  loves. We're  going  to  use  two  to  predict  what  comes  after  loves. We  get  out  of  the  problem  of  what  comes  after  mama, which  is  end  of  sequence. We  use  three  to  predict  end  of  sequence, We  get  out  three  separate  problems. Now  the  issue  that  we  encounter  at the  very  end  of  last  class  is that  as  we've  defined  attention, attention  for  two  or  for  word  two, looks  at  word  one  and  word  three. If  we  want  to  predict  what  word  three  is, that  will  be  pretty  easy. We  can  just  literally  copy  over  word  three  with  mama. That's  what  two  will  represent. It  already  saw  the  correct  engine. That's  not  what  we  want  the  model  to  learn. We  want  the  model  to  learn  actually  how  to  do  prediction. If  someone  gives  you  a  partial  sequence, you  can  predict  what  comes  after  it. That's  how  GPT  works. You  give  PT  question, it  predicts  what  comes  after  that  question. A  good  answer,  we're going  to  need  to  adjust  what  happens  inside of  our  attention  there.  Let's  talk  about  that. What  we'll  be  talking  about  is  called causal  masking  for  attention. Another  way  of  saying  this  is preventing  the  future  from  influencing  the  past. We  do  not  want  the  word  mama  to directly  influence  our  representation  for  the  word  love. Now  there's  only  one  time  travel  movie that  actually  makes  sense. We  know  where  it  is.  Yeah.  Which  one  was  that? Which  one  was  that?  Who's  in  it? I  think  I  saw  it.  I  think  it  was  terrible. So  none  of  that  makes  sense. Okay.  Because  they  all  have like  the  grandfather  paradox,  right? Which  is,  you  know, go  back  in  time,  kill  your  grandfather. And  how  is  that  possible? Doesn't  make  any  sense. So  there's  only  one  time  travel  movie that  actually  makes  sense. Tenet  is  not  very  loved  is  my  sense. I  think  it  does  not  have  a  great  reputation. Have  people  seen  it?  Yeah,  you  have  seen  it. Wow,  that's  surprising. Okay,  I  think  this  is  the  best  sci  fi  movie  ever  made. I  mean,  not  least  because  it  deals  with time  travel  in  a  coherent  way when  that  seems  to  be  almost  impossible. But  it's  also,  I  just  think  it's  fantastic. This  is  a  controversial  decision. Your  grade  will  be  improved  if  you  share my  opinion  about  this,  joking. Okay.  Now  time  travel  does  not  make  sense. Okay,  that's  the  lesson  here. It's  very,  very  hard  to  get  it  to  make  sense. In  the  context  of  language  modeling, it  makes  no  sense  whatsoever. So  we  want  to  prevent  it. Let's  look  at  attention. Where  does  our  attention  matrix  look  like? So  let's  say  we  have  three  words. We  have  this  sequence  here. Now  where  attention,  I'm  not  going  to  go  through the  Q  KV  computation  here. The  full  product  stuff  that  we  talked  about. Attention  gives  you  out, ultimately  is  a  probability  distribution  for  every  word. What  it  tells  you  is  where  does lama  attention,  which  words? And  it  gives  you  a  probability  distribution over  which  words  do  loves  attend  to, Which  words  does  mama  attend  to? We'll  write  it  in  this  way. This  is  A  11  is  the  probability  that  Ma,  the  first  word, assigns  to  Ama, the  first  word  12  is  the  probability  that  llama, first  word  assigns  to  loves  the  second. We're  going  to  write down  three  different  attention  distributions. Why  are  there  three  distributions? One  distribution  for  each  row. Let's  remember  our  attention  update  formula. This  is  how  we  compute new  word  representations  from our  old  word  representations. What  do  we  do?  Well,  let me  give  a  simplified  version  of  it. Let's  say  this  is the  new  word  representation  for  the  I  word. This  is  equal  to  a  weighted  sum over  j  equals  one  to  three. Why  am  I  summing  up  to  three? Because  in  this  case  we  g  three  words. Okay,  so  let's  just.  So  this  is our  new  word  representation  here. And  here  is  our  old  representation. So  we  have  three  different  old  word  representations. One  word  representation  for  work  in  the  sentence, we're  taking  a  weighted  sum over  those  old  word  representations. That's  going  to  give  us  our  new  word  representation. There  are  some  details  here  I'm  ignoring. In  particular,  we  have,  these are  actually  value  vectors,  right? To  remember,  the  value  matrix and  the  value  transformation  that  we  did. If  you  go  back  in  your  notes, you'll  see  we  talked  about  value  before. Doesn't  really  change  anything  here. The  point  is  we  have  a  vector representation  for  each  word. Our  new  word  representations are  way  to  solve  those  things. Why  is  this  a  problem? Because  now  the  word Ama  here  is  trying  to  predict  the  word  loves,  correct? The  attention  that  the  word  lama assigns  to  the  word  loves  will  be, in  general  non  zero. That  means  that  part  of  the  new  representation  for the  word  lama  will include  information  about  the  word  loves. The  way  the  model  will end  up  solving  the  prediction  problem, what  word  comes  next  is  it  will  just  copy  the  information over  from  the  word  loves  into  the  word  lama. That's  a  trivial  way  to  solve  the  problem. That's  it  involves  no  thinking. It  doesn't  actually  allow  you  to  do  real  prediction  in the  real  world  where  you don't  know  what  the  correct  answer  is. Is  this  problem  clear? Are  there  any  questions  about  this? We  have  leakage  of information  from  the  future  into  the  past. We  need  to  prevent  that  leakage. How  can  we  prevent  that  leakage? We  don't  want  current  word representations  being  influenced  by future  word  representations,  so  what  do  we  do? Yeah,  that  word  representations  perfect. We're,  for  the  word  ma, we're  not  going  to  use  any  of  the  word  representations for  loves  or  mama  for  loves. We  will  not  use  the  word  representation  for  move. We'll  only  use  the  representation  for  loves  and love  In  this  attention  matrix,  what  can  we  do? How  can  we  adjust this  attention  matrix  to enforce  the  requirement  that  you're  suggesting? We're  going  to  transform this  attention  matrix  using  causal  masking. How  do  we  do  that? We're  going  to  do  the  simplest  possible  thing. What's  the  simplest  possible  way to  do  what  you're  suggesting? Yeah,  exactly. We  zero  out  everything  that  occurs  above  the  diagonal. Anything  that  occurs  above  the  diagonal that  involves  a  word looking  at  a  future  word,  we  can't  allow  that. So  we  zero  them  out. There's  only  one  problem  here  with  what  we've  done. One  slight  problem  which  is  what  I  know. I've  zeroed  out  everything  above  the  diagonal.  My  done. Remember  these  are  attention  distributions. Each  row  needs  to  be  a  valid  probability  distribution. We  need  to  make  each  row  up  to  one. We  need  to  make  sure  each  row  sums  up  to  one. The  way  that  we  do  that  is  just  by  taking the  sum  of  each  of each  new  row  and  dividing  each  element  by  that  sum. I'll  call  these  new  things  like  a  twinles. They're  proportional  to  the  old  attentions reweighted  so  that  each  row  sums  up  to  one. Okay,  that's  it,  that's  how  we  solve  this  time travel  problem  for  transformers. We've  now  defined  like  we  have  a  perfectly  valid  way of  predicting  the  future  from  the  past where  we  can't  cheat  and  look  at the  future  in  order  to  solve  the  problem. Any  questions? Okay. So  there  is  one  remaining  problem that  we  mentioned  that  we  mentioned  last  time. What  was  it?  So  we've  solved, we've  solved  this  causal  masking  issue. But  there  was  one  issue  that  we  have  not  addressed  yet. Yeah,  the  word  order  is  the  word  order  issue. So  there's  no  real  way  for  the  model  to know  what  position  each  word  occurs. We  have  word  vectors  here, we  have  the  first  word  vector, the  second  word  vector,  and  so  on. But  if  the  word  llama  occurs  as  the  first  word, that's  the  same  vector  as  if  it  occurs in  the  tenth  word,  right? It's  based  on  the  vocabulary. The  vector  that  you  get  doesn't  depend  at all  on  where  you  are  in  the  sentence. Llama  has  the  same  v,  the  same  input  vector, no  matter  where  it  occurs. From  the  perspective  of  the  model, the  model  doesn't  know  which  word  is  in which  location  we  need  to  solve  that  problem. You  cannot  just  permute the  positions  of  words  and  expect  to  get  the  same  result. Your  model  has  a  problem  if  it  thinks  you  can  do that.  So  what  can  we  do? What's  the  dumbest  possible  thing  that  we  can  do? Yeah,  I  guess  it's  like  have  a  constant  wait  for one  of  the  is  like  first  18.5  and  1.4  on. So  it's  not  going  to  be  in  the  attention. We  want  the  attention  to  be  led. So  we  want  the  mall  to  be able  to  learn  where  to  attend  to. Like,  we  don't  want  that. I  think  we  don't  want  to  screw  up the  model's  ability  to  learn  to  do  attention. Yeah,  when  you  get  the  word embedding  before  you  put  it  in  the  transformer, that  word,  that's  exactly  it. So  those  are  called  positional  embeddings. Each  position one  through n  gets  a  vector. And  we'll  call  it  I. So  we  have  a  distinct  vector  for  each  possible  location. So  remember  all  of  these  models  that  we're  dealing  with. Every  transformer  model  has a  maximum  context  length,  right? For  older  language  models  it  was  like  512  or  1024. For  standard  GPT,  now  it's 4,000  or  8,000  And  then  there  are the  super  long  models that  have  versions  of  model  that  have 128,000  No  matter  what  it  is  though, there's  a  fixed  maximum input  length  that  these  models  can  have. We  just  list  out  what  are all  those  possible  positions  can  be. If  your  maximum  context  length  is  1024, then  you  have  1024  possible  positions that  a  word  could  go  in. You  have  a  separate  vector  for  each  of  those  positions. And  then  your  new  word  representation. Your  new  input  representation  I should  say  is  P, I  plus  I  for  the  word  llama  up  here. It's  the  first  word  that  occurs. So  take  the  word  embedding  for  llama, You  take  the  first  positional  embedding, because  it's  in  the  first  location  in the  sequence,  You  add  them  together. That's  it.  Now,  where do  these  positional  embeddings  come  from? What's  the  simplest  possible  answer you  learn?  It  works  pretty  well. Everything  in  the  model.  Your  word  embeddings, your  weight  matrices,  your  positional  embeddings, they  learned  through  back  propagation. You  have  your  loss  function  which  says, I  want  to  predict  the  next  word. Given  the  previous  words, you  minimize  your  loss. By  adjusting  all  of  these  weights  at  the  same  time. Now  I'll,  there  are a  number  of  different  positional embedding  techniques  in  the  literature. Learned  positional  embeddings, they're  probably  not  the  best  thing. There  are  some  slightly  fancier  things  that  people  do. There  are  things  called  rotary  embeddings, there's  something  called  alibi. There  are  probably  ways  to  improve  it, especially  for  very  long  sequences. If  you  start  from  positional  embeddings, though,  you  will  get  a  pretty  strong  baseline. Not  quite  state  of  the  art,  but  they  work  very  well. Certainly  well  enough  for  like  no, any  beginning  language  model. Okay.  And  that's  it?  Yes. I  mean,  so  just  the  information has  been  smushed  together.  Is  that  what  you're  saying? That  it's  not  necessarily  you're not  necessarily  going  to  be  able to  uniquely  recover  both. That  is  that  is  that  the  concern? So  the  model  will,  these are  extremely  high  dimensional  vectors. There's  not  going  to  be  a  way  to  linearly  decode, but  there  is  going  to  be  a  way  to  non  linearly decode  both  of  those  pieces  of  information  in  general. So  I'm  trying  to think  if  there's  a  clean geometric  way  to  illustrate  this. Let  me,  okay,  let  me  give a very  simplified  example of  what  something  like  this  will  look  like. Imagine  that  our  position  and  word embeddings  are  both  two  dimensional.  Okay? So  we  have  Ama, our  wording,  bedding  for  Llama  over  here, and  we  have  only  two  words  in  our  vocabulary. We  have  over  here, we  have  one  over  here, that  is  the  first  positional  embedding  over  here. We  have  two  over  here. Now  we  can  sum  together  llama  and  one, we  can  sum  together  llama  and  two, Those  will  be  distinct  vector, they'll  also  be  distinct  from 2.1  The  four  possible combinations  here, dimension,  we're not  adding  another  dimension  necessarily. I  mean,  you're  right  here.  Here  we  are. I  mean,  it  didn't  have  to be  necessarily  adding  another  dimension. It  could  be  that  we  have  actually  like, you  know,  we  could  have  had  ten  words, you  know,  each  sort  of  evenly  spaced. As  long  as  the,  as  long as  the  positional  embeddings  are  distinct. Basically  what  we  need  to  make sure  is  that  the  mapping  is  one  to  one. I  believe  that,  that  will, I'm  fairly  sure  that's  achievable  in almost  every  situation  to  have  a  one  to  one  mapping  here. Now,  as  I  mentioned, doesn't  necessarily  mean  that  things will  be  linearly  decodable. If  you  just  do  matrix  multiplication, you  may  not  be  able  to figure  out  which  of  these  you're  in, which  combination  you're  dealing  with. But  if  you  have  the  ability  to  do  non  linear  decoding, which  neural  networks  do  matrix  multiplication followed  by  non  linearities, then  you'll  be  able  to  figure  out which  combination  you're  dealing  with. And  so  everything  is  going  to  be learned  exactly  how  to  position  all  of these  words  in  this  space and  then  how  to  interpret which  words  you're  dealing  with. Any  other  questions?  No.  Okay. So  let  me  talk  now about  a  very  important  engineering  detail, some  very  important  engineering  details  and  all  of this  which  is  it's  very, very  closely  related  to  why  transformers. Took  over  the  world. Have  we  talked  about  the  bitter  lesson? No,  I  haven't  sent  out  this  essay. I'll  have  to  send  out  the  essay  to  everyone. It's  every  year  this  essay becomes  more  and  more  true  when  I  read  it, like  it  came  out  in  like  2018  or  something  or  2019. And  I  first  read  it  and  I  thought, this  is  outrageous,  this  is  insulting. And  my  opinion  about  it has  been  updated  every  year  since  then. So  what  does  the  bitter  lesson say,  U? It  says  that  intelligence  comes  from  lots  of  computation. In  the  primary  way  that  you're  going  to  increase the  intelligence  of  systems  is  by increasing  the  amount  of  computation  in  those  systems. The  reason  why  transformers  have been  so  effective  and  why people  switch  to  them  almost  immediately, and  why  they  really,  really  work, is  that  you  can  process  sequences  really quickly  with  a  transformer  as compared  to  an  RN  current  your  own  network. Let's  talk  about  why  that's  true  at  a  high  level, and  then  we're  going  to  go  into  some  low level  details  about  why  that's  true. Let's  look  at  the  R  and  N  architecture. Remember,  you  do  this  on  your  homework, so  hopefully  this  is  very  familiar  to  all  of  you. If  you  want  to  process  a  sequence of  words,  what  do  you  have  to  do? What  are  these  boxes  telling  us  that  we  need  to  do a  process  First  words? Exactly  everything  is  sequential. In  an  RNN,  we  process  all  of  the  words  sequential, one  word  after  another. In  a  transformer,  we  don't  have  to wait  until  the  first  word  is done  processing  to  process  the  second, we're  processing  everything  in  parallel. The  consequence  of  that  is  in  a  fixed  amount  of  time, we're  able  to  process  many,  many  more  words. The  transformer,  what  that  means is  we  can  do more  computations  in  the  same  amount  of  time. We  can  process  much  more  data  in  the  same  amount  of  time. In  the  time  that  it  took  an  RNN to  process  1  million  words, we  may  be  able  to  process 1  billion  words  with  a  transformer. Maybe  it's,  it  might  be  a  factor  of  100  difference, it's  a  very  large  difference. Nonetheless,  let's  see  why  that's  true. Let's  look  at the  matrix  form  formulation  of  the  transformer. So  we  have  our  sequence  of  words, will  start  from  the  very  top. At  the  very  bottom,  I  should  say  we  have our  sequence  of  words  one  through. What  we're  going  to  do  is  we're  going  to  put this  sequence  of  words  into  a  matrix. Let's  call  it  just  capital  W. So  we  have  a  new  matrix  of words  with  n  columns  in  one  column  per  word, where  we  see  this  before. We  did  this  one  time  previously in  the  course  where  we  see  it. Yeah,  not  quite. The  embedding  matrix  is  because  the embedding  matrix  reserve  for everything  in  the  vocabulary. So  when  we  were  talking  about  MLPs,  right? When  we  were  talking  about  how  you  could process  a  whole  sequence as  how  we  can  think  about MLPs  is  just  a  big  matrix  computation, that  we  saw  something  similar  there. Okay,  now  let's  look  at  the  tension. Can  someone  remind  me,  what are  the  steps  involved  in  attention? So,  you  know,  we  have  our  attention  matrices,  right? We  have  the  attention  distributions. How  did  we  compute  those  attention  distributions? There  were  a  few  high  level  steps  involved. There  were  some  dot  products, right?  What  were  we  do? Why  did  we  do  first? Okay,  great.  So  what  we're  going  to  do  first is  do  a  query  and  key  transformation  on  everything. Right,  so  we  have  a  query  matrix  Q, query  weights.  Key  weights. So  we  then  we're, we're  then  going  to  create  a  new  matrix. We'll  call  it  the  words transformed  into  their  query  vectors. How  do  we  do  that? What  we're  going  to  do  is  take  the  Q  matrix times  what  happens  now. In  the  first  column  of  this  new  matrix, we  have  the  query  vector  for  word  we. In  the  second  column, we  have  the  query  vector  for  word  two. Each  column  contains  a  query  vector. We're  going  to  do  exactly  the  same  thing  for  K. We're  going  to  multiply  the  K  matrix by  our  word  embeddings. That  gives  us  a  key vector in  each  column, rather  than  the  way  that  we  were  presenting  it  before. What's  the  point  of  doing  it  this  way? The  way  that  we  were  presenting  it before  is  that  we  were  sequentially  going through  each  word  vector  and  multiplying  it  by the  key  matrix  and  the  matrix,  the  query  matrix. It  was  very  sequential, the  way  that  we  were  talking  about. It  doesn't  need  to  be  sequential. All  we're  doing  here  is  we're  parallelizing  this. We're  putting  everything  into  a  matrix, doing  all  of  the  computations  at  once.  Why,  why  do  that? It's  because  GPUs,  Graphical  processing  units, they  do  matrix  multiplication  really,  really  quick. So  our  attention  matrix, how  do  we  compute  this? How  do  we  compute  this  in  terms  of these  two  new  matrices  that  we  have  here? It's  going  to  be  a  matrix  multiplication. It's  going  to  be  a  soft  max  and  a  matrix  multiplication. So  what  do  we  do? Remember,  the  queries  are  looking  for  keys. Each  word  has  a  query, and  taking  a  dot  product  with  all  of the  keys  to  see  which  key  should  it  look at,  that  gives  us  a  weight. We're  looking  at  similarity  between  queries  and  keys, giving  us  a  distribution  of  weights  for  each  query. What  I'd  like  is  for  the  queries  to  be  on  the  road. Yeah. So  we're  going  to  take  the  transpose, taking  the  transpose  Q  query  per. Now  we  multiply this  by  K.  Let's, we'll  do  this  in  two  steps. So  we'll  call  this  like  this. So  these  are  the  scores, we'll  call  them  attention  scores. They're  not  the  tension  probability yet  because  it's  not  actually  probability. They  haven't  been  normalized,  we haven't  read  through  soft  back. So  what  does  this  give  us? Let's  actually  look  out,  look at  what  this  matrix  involves. We've  took  the  transpost  now  is  one  query  per, and  we  still  have  one  key  per  column. What  this  looks  like  is  that  we  have  111, 21k,  and  then  on  the  second  row,  it's  2122. Now,  same  one  query  per  row, taking  the  doc  product  with  all  the  keys. Each  row  now  represents the  weight  that  each  word  assigns  to  every  other  word, each  query  assigns  to  all  of  the  keys. Then  our  attention  matrix, the  actual  attention  distribution. So  let's,  let's  call  this  thing. This  is  equal  to  soft  max of  where  the  dimension that  we're  doing  the  soft  max  along  is  equal  to  one. So  we're  doing  it  along  the  road  is  that, let's indicated  like  this, we're  taking  the  softmax  of  each  row, We  have  different  softmaxes,  one  for  each  row. So  what's  the  consequence  of  this? Which  is  we  can  compute  the  attention  distribution. We  got  the  queries  and  the  keys  by  matrix  multiplication. We  got  the  attention  scores  by  matrix  multiplication. And  then  we  got  the  actual  probability  attributions by  taking  the  softmax  of  that. Almost  everything  that  we've  done, almost  all  of  the  computations just  matrix  multiplications. Matrix  multiplications  are  fast  and we've  processed  all  of  the  words  completely  in  parallel. We  also  have  to  apply  causal  masking. Here,  I  left  that  out. It's  a  very  simple  operation, It  just  involves  multiplying  two  matrices. Actually,  you  have  a  causal  mask that  says  everything  above  the diagonal  should  be  zero  and  you  multiply  by  that  matrix. Okay,  what's  left  to  do  for  attention  at  this  point? Multiplied  by  the  value  we  multiply. But  you're  right,  so we  have  to  sum  together  all  the  values,  right? We  have  to  take  a  weighted  sum  of  all  the  values. So  what  we  need  is  a  value  matrix. So  we  have  our  value  weights  V. Then  what  we'll  say  is  that  V, it's  going  to  be  a  matrix  that collects  up  all  the  values  for all  the  words  going  to  be  V  times. So  what  we  have  there  is  one  value, so  the  first  value  in  first  column, the  second  value  in  the  second  column,  and  so  on. So  I  have  my  attention  matrix, I  have  value  matrix, I  have  the  values  for  each  word. How  do  I  combine  those  two  things  in  order to  get  the  new  word  representations? I  want  to  get  my  updated  word  representations. How  do  I  say  I  weighted  some  of  the  value  vets So  we  have  here.  That's  right. So  how  do  I  actually  do  this  to  be  You're  right. We  have  our  attention  matrix  here where  each  row  is  normalized,  right? One  row  corresponds  to  the  distribution, the  probable  distribution  over attention  first  row  is what  does  the  first  word  at  tend  to? So  it's  going  to  be  a  times what  is  it  times  V? The  dimensions  here  actually  tell  us  what  it  has  to  be. A  is  an  n  by  n  matrix, where  n  is  the  sequence  line,  right? Where  are  the  dimensions  of  this matrix,  the  value  matrix. So  okay,  I  think  I  said, I  said  this  in  fact, I  think  when  we  were  doing  the  MLP computations  earlier  in  the  quarter, I  think  I  said,  I'm  acknowledging this  is  not  the  most  exciting  thing  ever. Okay.  I  can  see  from looks  on  people's  faces.  No,  it's  not  thrilling. It  should  seem  remarkably similar  to  what  we  did  there,  because  it  is. However,  this  is  the  actual  reason why  deep  learning  works. If  you  do  not  have  this,  you  don't  have  deep  learning. We  have  all  these  cool  ideas  about  attention, blah,  blah,  blah,  blah,  blah. Human  mind  blah,  blah,  blah,  blah,  blah. None  of  it  matters  if  you  cannot transform  your  problem  into a  matrix  multiplication  problem  and  do  it  really  quickly, None  of  it  works  like  this,  is  why  this  works. That's  why  we're  talking  about  this. The  fact  that  it's  very  boring means  that  most  people  don't  pay  attention  to  it. Therefore,  people  aren't  going  to  be making  money  by  actually understanding  what  works  here,  right? Like  the  stuff  that's  boring, but  important  is  exactly  where  you  should  be  focusing your  attention  if  you  want  to  make  an  impact. What  are  the  dimensions  of  this  matrix  here? Yeah,  the  dimension  of  the  value  matrix.  Right? Okay,  so  what's  the  dimension  of  the  value  matrix? It's  n  here  being  the  sequence  length. Yeah,  Deeping  the  embedding  dimension,  right? This  is  D  a  n, n,  sequence  length  by  sequence  length, because  we  have  words  attending  to  each  other. What's  the  only  thing  that  can  work  here  for doing  matrix  multiplication  transpo, It's  going  to  be  V  transposed. What  we  get  out  here  is it's  a  little  ugly  to  write  out. What  we  get  out  here  is on  every  row  we  have  our  new  word  representations. What's  so  we  have  a  vector  on  the  row. That  is  the  vector  corresponding  to the  updated  representation  of  word  I,  which  is  what? It  is  a  weighted  sum  from  j  equals  one  to n  I  J  times  J. So  what  are  we  saying  here?  I  J. That's  the  attention  that  word  I  pays  to  word  J. We're  summing  over  the  J's, everything  that  word  pays  attention  to. We're  summing  over  that,  We're  multiplying by  the  value  vector  for  word  J. So  we're  taking  a  weighted  sum  over  all  of the  value  vectors  and we're  putting  each  of those  new  representations  on  the  throw. So  that's  it.  Like  ours, we're  done  with  attention,  right? Every  single  attention  layer  can  now  be  broken  down into  just  a  bunch  of  matrix  multilications and  one  little  saw  fats  in  the  middle. Yes,  here,  all  right. You  can  do  it  here  or  here. Basically  the  same  thing,  people actually  do  it  at  this  stage. Basically,  it's  what  you  get your  attention  distribution  and  you  mask, we  will  say  do  causal  masking  here. Okay.  So  we've  taken  care  of  attention. Attention  is  just  a  bunch  of  major  multiplications. What  about  the  MLP? So,  it's  actually  is  a  little  different  from the  MLP  that  we  did  before.  So  let's  talk  about  it. Start  for  Ok. How do  we  paralyze  the  MLP? I  mean,  we've  already  paralyzed MLP  once  it  was  different. So  what  was  different  about the  MLP  that  we  did  before  is  that  it  did, it  did  not  process  each  of  the  words  independent. In  this  case,  the  MLP processes  the  words  completely  parallel. So  what  was  our  definition  of  the  MLP? It  was  that  you  take  in  a  word  representation, we'll  call  it  MLP  of  x. Okay? You  take  in,  let's  call  it  x, I  for  the  word  vector. So  what  happened? You  have  a  weight  matrix. Okay? So  you  took  in  the, the  word  vector  x, you  multiply  it  by  this  weight  matrix,  MLP  one. You  do  a  relu,  you multiply  that  by  another  weight  matrix. You  do  that  for  every  single  word. We  can  frame  this  in  a  completely  sequential  way. You  do  this  for  the  first  word, you  do  this  for  a  second  word, you  do  this  for  the  third  word. Now,  we  don't  want  to  do  that. We  want  to  do  this  all  in  parallel. So  if  I,  let's  talk  about  this  in  terms  of  just  my  word, word  matrix  up  here. So  I'll  change  these  Xs  to S.  You're  not  directly processing  the  word  vectors  you're  processing. Well  actually  let's  do  this  in  terms  of  the  H, just  like  it's  just  what  name  we  give  to  the  executives. That's  all,  that's  all  we're  talking  about  here. But  we'll  do  this  in  terms  of  the H.  H  is  where  I  mean  here. So  let's  call  this  thing  H. So  we  have  this  matrix  of  our  updated  word, embeddings  attention,  right,  One  per. So  how  do  I  apply  the  MLP  to  that  entire  H  matrix? All  of  the  word  embeddings  in  parallel? I  have  h  is  equal to  h  one  the  representation  for  the  first  word, two  the  representation  for  the  second  word, and  so  on,  one  per. So  I  want  to  parallelize  this. Yeah,  Pz  here  basically radiate  and  then  taking in  a  bunch  of  examples  of  raiateie. One  you  give  each  computer,  each  one  example, a  few  numbers  of  examples  and  they  each calculate  their  own  radiating  side and  out  the  radiates  together  to  get. Well,  that's  a  good  idea  if you're  talking  about  independent  sequences, but  here  I'm  talking  about  a  single  sequence. So  is  the  H, I'm  taking  the  H  notation  from  what  I  did  over  here, where  I  have  my  updated  word  representations. So  what  H,  H  I  represents the  updated  word  representation from  attention  for  word  I. So  each  of  the  HIs, they're  all  in  the  same  sequence. Right?  So  I  have  one  sequence  with  1,000  words  in  it. I  have  1,000  words, so  I  have  equal  1,000  I  have  1,000  ages  as  a  result, but  they're  all  from  the  same  text. If  they're  all  from  the  same  text, I  have  to  run  them  through the  transformer  at  the  same  time. I  can't  do  back  propagation  independently  on  them. Otherwise,  that  would  be  like  processing each  word  in  a  text  completely  independently. That's  like  having  a  transformer with  context  length  equal  to  one. The  words  influence  each  other's  meaning. I  need  to  take  that  into  account. How  do  I  turn,  maybe  I  didn't  state  the  problem  clearly. Do  people  understand  what  I'm  asking  for? What  I  want  to  know  is  like  I've given  you  an  algorithm  for  applying  an  MLP  to  a single  vector  H.  How  do  I  parallelize, how  do  I  turn  that  single  vector  computation into  something  that  happens simultaneously  for  all  the  vectors? Into  a  matrix  multiplication? Yeah,  H  transpose,  that's  it. The  thing  I  do  here  is  I  multiply  W  MLP times  H  transfer,  right? Because  now  that  gives  me out  in  one  column  I have  the  result  of  MLP  applied  to  H  one, the  first  word  vector. In  the  second  column  I  have  MLP  applied  to  two. Each  column  now  contains  the  MLP  applied  to, applied  to  each  vector. So  that's  how  we  paralyze  it. It's  literally  just  like  take,  instead  of  doing, instead  of  inserting  a  vector  into  this  MLP  equation, we  insert  a  matrix  into  the  equation. But  nothing  really  changes, just  means  we've  done  more  computations  at  the  same  time. And  that's  it,  right? That's  like  what  happens  in  the  transformer. There's  a  tension,  there's  an  MLP. And  there  were  two  other  things  that  happened. There  was  the  residual  connection, which  is  an  addition  residual  connection just  means  you  add  what  you did  previously  to  what  you  did  now. And  then  we  also  had  the  layer  norm. The  layer  norm  actually  turns  out  to  be  like. Not  very  parallelizable,  but  it's  also very  few  computations  relative  to this  very  few  floating  point  operations. It  doesn't  really  matter. There  are  people  who  care  about fast  layered  normal  representations from  running  on  the  GPU, but  you're  talking  about  a  couple  of  percentage  points of  how  much  time  it  takes  to  run  this. It's  not  a  big  deal,  not a  big  deal  for  you,  It's  a  big  deal. Look  open  the  eye  is spending  like  billions  of  dollars  per year  running  their  GPU's  right  now. They  actually  do  care  about  those. Percentage  point  savings  percentage  point  to $10,000,000  is  $100  million,  right? That's  a  lot  of  money.  So  people  care  about  that  a  lot. But  for  the  purpose  of  this  class, it's  not  that  big  of  a  deal. Yeah. Can  you  what? Yeah,  of  course,  no. Oh,  no,  I  didn't. Maybe  the  message  might  have  been  there, but  I,  you  know,  I  was  too,  like, you  know,  locked  into  what  I  was  doing, I  guess  to  notice  it  has  a  little  birthday. Oh,  has  a  little  birthday.  Had,  maybe  I  saw  it. I  didn't  even  notice  happy  birthday. Yeah.  Does  the  forward pass  paralyzed  way  back  propagate  fantastic  question? Yes,  Everything  in  the  backward  pass can  be  paralyzed  as  well  as  just, it's  just  another  matrix  multiplication. In  fact,  I  believe  in  your  homework you  implemented the  paralyzed  version  of  the  backward  pass. Or  at  least  I  encourage  you  to. So  for  the  MLP,  right, in  problem  set  three,  there  was a  paralyzed  version  of  the  backward  pass. Yeah,  it  looks  very,  very, very  similar.  Great  question.  Yes. The  results  won't  be  quite  as  good. I  mean,  for  one  thing, the  MLP  will  be, let  me  not  even  try  to  give a  justification  for  why  that's  true. People,  I  mean,  basically  every  little  iteration  of this  people  have  tried  and  this  one  works  the  best. It  doesn't  mean  this  is  the  best  architecture, just  among  like  little  tweaks  that  you  can  make. This  is  the  best  one  that  there  is. I  mean,  it's  a  good  question,  just it  just  happens  not  to  work  as well.  Any  other  questions? Okay, Sorry,  one  more  thing. What  I  want  to  ask  now  is  u, how  many  floating  point  operations? By  which  I  mean  by  a  floating  point  operation, I  mean  like  a  multiplication  of two  numbers  or  an  addition  of  two  numbers. I  mean,  I  mean  numbers.  Okay? How  many  floating  point  operations, additions  and  multiplies  happen in  attention  and  the  MLP? So  let's  do  attention  first. And  you  assume  that  it's  like  you're  using  a  very, very  straightforward  algorithm  for doing  matrix  multiplication,  okay? So  what  I  mean  is  just  like  just the  algorithm  you  learned  in  high  school, where  you  take  each  column  and  you  multiply  it by  the  rows,  okay? So  what  were  the  computations  that  we  did  for  attention? I,  I  erased  an  important  one  over  there. So  let's  write  it  right  down  over  here. We  can  ignore  the  Softmax  as  well, but  we  have  V  is  equal  to  V  times, and  then  we  have  h  is  equal  to what  it was  a times  V  transpose,  okay? So  we  actually,  this  is everything  that  happens  for  attention. We  have  everything  that  happens  to  MLP  up  there. So  we  have 12345  matrix  multiplications that  happen  for  doing  attention. Let's  count  up  how  many  floating point  operations  happen  in  each  of  them. So  how  many  floating  point  operations  happen  here? How  many  floating  point  operations? When  we  multiply  two  matrices together,  what  ends  up  happening? So  we  have  like  x1x 2x3x4  times  y1y  2y3y4. Where  do  we  get  out?  We  get  out  X1y1  plus  X2y2, X3y1  plus  X42,  X1y2 plus  X2y4,  X3y2  plus  x4y4. So  the  way  that  I  would  count this  up  is  I  would  basically say  that  we  have  12345678  multiplications. And  then  we  can, we  can  count  this  as  basically,  let, let's  say  that  this  is  two floating  point  operations  for  the  edition. Let's  just,  let's  just,  let's  count  it  like  that. Uh,  so  I  have  eight  plus  eight,  correct? So  I  have  16  floating  point  operations. So  what's  the  general  formula  there? All  right,  We're  out  of  time. I  may  not  decide  to  continue  this  next time  since  it's  going  to  require  a  lot  of  set. Let  me  give  you  the  punch  line  and  you  can confirm  this  for  yourself  in  your  own  time.
