So. Linearly  separable  means. We're  in  a  situation  like  this  where  we  have  two  classes. And  we  can  draw  a  line that  perfectly  separates  the  two  classes. Not  all  problems  are  like  this. In  fact,  no  real  world  problem will  ever  be  like  this  because  of  noise. Let's  make  a  very  simplifying  assumption. What  we're  going  to  do  before the  model  launches  into  anything, I'm  going  to  brainstorm  with  it. Okay,  Let's  brainstorm  about  what  we  should  do. Key. Uh, okay. So  I  proposed  a  bunch  of  steps. We're  not  going  to  do  all  of  them, we're  going  to  do  the  first  few. And  I'm  just  going to  clarify  that  my  students  already  know the  definition  of  logistic  regression. Okay,  so  they're  going  to  initialize  the  weights. Initializing  the  weights  means, and  we  do  this  everywhere  in  deep  learning  is we  just  make  a  random  guess  about  what  the  weights  are. It's  really  just  chosen  that and  when  they  are  training  GPT  four, they  start  from  a  random  initialization  of the  10  trillion  numbers  that  make  up  the  weight  matrices. There  we're  going  to  do  exactly  the  same  thing  here. Okay,  So  they're  going  to  initialize  the  weight, to  initialize  the  decision  boundary, they're  going  to  plot  it. Then  they're  going  to  do  gradient  descent. We  haven't  talked  about  grading  descent  yet, That's  what's  basically  coming  up  today. And  next  class,  gradient  descent, as  the  name  suggests,  involves  taking  the  gradient. We're  going  to  have  a  loss  function for  logistic  regression. The  loss  function  is  going  to  say  how well  we're  doing  it  at  classifying. And  we're  going  to  take  the  gradient, which  means  differentiating. And  then  we're  going  to  use  that  gradient  information. The  gradient  is  going  to  tell us  in  what  direction  we  should move  to  slowly  update  the  weights  to  make  them  better. Okay,  let's  begin  with steps  11.3  by  creating a  synthetic  dataset  and  visualizing  it. People  in  your  generation  may  have  an  easier  time getting  used  to  this  like  a  model, just  writing  code  like  this. I  still  cannot  quite  get  over  it. Oh,  okay. Okay,  this  is  not  good  though. Why  is  this  not  good? We  had  two  classes and  we've  already  separated  them  with  our  line. Now  it  turns  out  this  is  actually  not  the  best  decision. Boundary,  do  you  have  a  sense  about  why  that  is? Yeah,  boundary  still  classifies, it's  true,  but  here  it's  not. I  mean,  in  the  sense  that  we  have  correctly classified  all  of  the  points  here  in  the  sense that  all  of  the  yellow  dots  are  on one  side  and  all  of  the  black  dots  are on  the  other,  says. Not  even  testing.  I  was  saying  training  wise, this  is  still  Okay,  great  distinction. You're  right,  okay.  These  things  are  actually  connected. We  have  an  intuition  looking  at this  that  if  we  see  more  data, some  of  the  yellows  may  very  well  appear  to  the  right. And  then  we  would  classify  them  wrong. But  that  problem  already  shows  up  at  training  time. Yeah,  it  feels  like  there  should  be  more  space. Exactly,  At  training  time, the  model  will  actually  have  a  pressure  to move  this  farther  towards  the  center. That  will  help  at  test  time, making  it  more  robust  to  new  data.  Great  observations. Okay,  so  let's  see if  the  model  can  pick  up  what  the  problem  is. There's  a  problem  with  the  example  you  just  gave. The  initial  weights  already  correctly  classify. So  let's  see  what  it  does  in  response. Okay,  okay,  that's  better. Again,  I've  illustrated  this  already  a few  times  in  class,  in  previous  classes. I  just  want  to  highlight  it  again. When  this  model  gives  you  something  that you  don't  want  tell  it, it's  very  good  at  dealing  with  that  type  of  correction. Okay,  so  we're  going  to  compute the  gradient  and  that's  what we're  going  to  be  spending  the  rest of  class  doing  manually. We're  going  to  see  how  to  do  this computing  the  gradient  though. Let's  talk  about  our  loss  function. What  is  our  loss  function? We  have  a  dataset  D. What's  the  loss  given? Let's  just  say  we  have  our  weights. And  again,  what  is  D? D  is  a  collection  of  vectors. X1y1  is  a  single  number, So  we  have  y  I  is  either  zero  or  one. What's  our  loss  function? It's  equal  to  the  probability  of  the  data. The  loss  function  is  going  to  be the  negative  log  probability of  the  data  given. What's  that  equal  to?  It's  equal  to  a  product. Let  me  actually  say  that  slightly  differently. What's  the  probability  of  the  data  given  equal  to? It's  equal  to  the  product  from  I  equals  one  to  n  of the  probability  of  y  given  x  n. We  look  at  all  of  the  y's,  we  ask, what's  the  probability,  let's  say  y  one  is  equal  to  one. We  ask,  what's  the  probability  of  actually observing  y  one  is  equal  to  one, given  the  particular  x  one  that  we  see. We  see  some  input  features. What's  the  probability  of  them  producing the  corresponding  y  I? So  this  is  a  little  bit  fun,  this  loss  function. So  we've  explained  what  the  probability  of the  dataset  is.  Any  questions  about  that? Actually,  how  we  compute  the  probability  of  the  dataset, is  this  visible  from  over  there? Good.  Okay,  thank  you. Any  questions  about  the  probability  of the  dataset  in  our  definition  of  it? So  we  take  each  individual  data  point  and  we  ask  what's the  probability  of  some  YI equaling  the  value  that  it  got? Okay,  what  about  this  loss  function here?  That's  a  little  fun. Intuitively,  we  should  want  to maximize  the  probability  of  our  dataset. Someone  gives  us  a  dataset. We're  trying  to  find  a  model  that  makes  all  the  data that  we  observed  as  likely  as  possible. Another  way  that  another  loss  function  that  we could  have  chosen  or  not  a  loss  function, our  goal  could  have  been maximize  probability  of  the  data given  the  weight  vector  W.  That's  a, this  is  actually  our  goal. We're  still  doing  maximum  likelihood. We're  trying  to  find  the  model  that maximizes  the  likelihood  of  the  data. But  what  does  that  goal have  to  do  with  this  loss  function  here? Yeah,  minimizing  loss. Okay,  Great.  Small,  uh  huh. Uh  huh.  Correct  loss  is  bad, so  we  want  to  drive  it  down.  That's  correct. Maximizing  a  positive  quantity is  the  same  as  minimizing  a  negative  quantity. If  we  want  to  maximize  the  probability, that's  the  same  as  minimizing  the  negative  probability. That's  one  step.  Loss  is  bad, that's  why  we  put  a  minus  sign in  front  of  the  probability. And  what  about  the  log? Yeah,  Mizuho  maximizing  log  of  x, Perfect  maximizing  maximizing  log  of  x  are equivalent  in  the  situations  that  we're  in  minimizing  x, Minimizing  log  of  x  are  equivalent. Minimizing  log  of  probability is  accomplishing  the  same  goal as  maximizing  the  probability. I  should  have  said  minimize  negative  log  probability. That's  our  loss  function. We  want  to  maximize  the  probability  of  data, minimize  negative  log  probability  of  the  data. We  have  a  loss  function, We're  going  to  compute  the  gradient of  that  loss  function. What  is  computing  the  gradient  mean? Computing  the  gradient  means taking  partial  derivatives  of the  loss  with  respect  to  each  weight  I. We  see  how  does  the  loss  change when  we  change  by  a  little  bit. And  then  we're  going  to  collect  up  all  of those  partial  derivatives  and  we're going  to  move  basically  in  the  opposite  direction. The  gradient  says,  what  direction  do  you  move in  to  increase  the  function  the  most? If  you  move  in  the  opposite  of  that  direction, that's  the  same  as  decreasing  the  function  the  most, and  we  want  to  decrease  our  loss. Let's  see  what  happens  visually. We're  going  to  be  computing  the  gradient, updating  the  weights,  visualizing  these  updates. We're  going  to  do  a  few  visualized  rounds of  gradient  updates. So  we're  defining  a  function,  update  weights. This  does  not  seem  to  be  going  well. Let  me  not  tell  it  what  type  of  error  I  think  it  got. Yeah,  so  my  version  of  the  model  cannot see  yet,  so  you  know, there's  a  version  that  some  of  you  have that  actually  can  understand  visual  inputs. They're  rolling  it  out  in  stages. If  you  have  it, you're  going  to  know  because  you're  going  to  have a  button  down  here  that says  like  upload  image cruel  that  I  have  not  gotten  that  update  yet. It  can't  see  what  went  wrong  here.  We'll  tell  it. Let's  take  a  close  look  at  this. It's  a  little  hard  for  me  to  tell. Do  you  think  the  slope  is  the  same  in  these  two  cases? I  don't  think  so.  I  think  the  slope  is  changing. It's  definitely  moving  in  the  wrong  direction. I  think  there's  two  things  there. So  I  want  them  to  plot  not  only  the  decision  boundary, but  also  the  weight  factor  that  we're  getting. While  it's  doing  this,  let's  look  at  the  code  and  see if  we  can  find  what  went  wrong. I  have  a  feeling  I  know,  but  I'm  not  sure. No,  it  must  be  oh,  you  know  what? It's  there  is  a  possibility  about  what's  going  on  here. It  may  not  actually  be  wrong. Can  someone  give  a  hypothesis about  what  might  actually  be  happening  here? Yeah,  displaying,  I  don't  think  so. I  think  it  is  displaying  the  decision  boundary. You're  right,  that's  a  possibility. I  think  it  is  probably  displaying  the  decision  boundary. But  there's  one  situation  where, because  the  math  looks  okay,  just  on  the  first  glance. So  I  think  there  is  one  possibility where  this  is  actually  correct. Yeah,  that's  interesting. I  think  we  are.  I'm  pretty sure  we're  seeing  at  least  it  would be  they  were  generated from  from  two  Gacians  or  something. So  it's  bimodal. I  think  that  can  explain  it. Yeah. If  the  decision  boundary lets  keep  moving  in  that  direction, eventually  it  to  more  correct, that's  on  the  right  track,  I  think. I  think  that's  what's  going  on. But  this  decision  boundary might  actually  be  extremely  wrong. Right  now,  it  looks  like  it's  correct  starting  out, but  I  think  there's a  possibility  this  is  extremely  wrong. Does  anyone  know  why  that  could  be? Yeah,  it  might  have  to  be  flipped  exactly. If  blacks  are  positive, the  vector  might  be  pointing  this  way. It  might  be  trying  to  rotate  it  around. Let's  see. Okay,  so  actually  we  don't  know  what  is  yellow. Is  yellow  positive  or  negative? I've  been  assuming,  let's  ask. Okay,  so  that's  the  problem. Yellow  is  actually  positive, so  we're  starting  out. So  here's  our  weight  vector. We  want  the  weight  vector  to  point  towards  these  things. As  we  keep  doing  grading  descent, we  see  it  actually  starting  to  rotate. It  looks  like,  you  see  here, it's  starting  to  point  more  and  more  down. If  we  keep  going,  it's  going  to  rotate all  the  way  around  towards  the  yellow  points.  Yes. Classify  100%  It  seems  like  that. Yeah,  we're  close  to  that. Almost  100%  wrong. Yeah,  that  was  funny. Okay,  Mission  accomplished. Let's  do  one  more  thing. Let's  go  until  approximate  convergence. So  I  wanted  to  just  get  all  the  way  around. I'll  give  it  some  nice  feedback. Okay,  So  actually  skipped  a  few  steps. Okay,  So  here's  what  it  looks  like. At  first  it's  pointing  down, everything  is  on  the  same  side  of  the  decision  boundary. And  now  it's  going  to  slowly  start  to  rotate. And  this  is  a  big,  it  took  a  big  jump  here. This  is  probably  the  biggest  jump  it's  taken  so  far. Now  everything  is  classified  correctly, but  we're  still  not  done. All  of  the  ones  are  on  one  side, all  the  zeros  are  on  the  other  side. We're  still  not  done  because the  decision  boundary  is  not  close  to  the  center. So  we're  going  to  keep  moving the  decision  boundary  closer  to  the  center. And  that's  what's  happening  here. And  we  see  it's  finding  something  pretty  good  now. Okay,  so  you  now  know  how  to  play around  with  this  on  your  own if  you  know  the  urge arises  or  you  want  to  better  out  look. I  mean,  this  is  incredibly  useful for  understanding  logistic  regression, I  think.  Yeah,  the  reason. Let  me  double  check  that.  Is  that  true? Is  that  why?  Oh,  yeah. Oh,  I  don't  think  that  fully explains  why  the  weight  vector is  not  fully  orthogonal  though. Let's  ask  that  question.  Will  that  explain  it? I  don't  think  so  because  I  think  they  should both  be  rescaled  in  the  same  way. Thanks  for  pointing  that  out. I  mean,  okay.  It  thinks  it  could  be, I  guess  a  scaling  issue. Is  that  does  that  make  sense? So  that  looks  pretty  good.  Right? So  I  guess  it  was  an  aspect  ratio  issue, not  a  scaling  issue. I,  I  don't  know. It  was  some  visualization  artifact, you  know.  Thanks  for  asking. That  looks  pretty  good. Okay,  so  let's  talk  about the  probability  of  the  data  given  our  weight  factor. And  let  me  just change  my  notation  slightly  here because  I  do  want  to  include  the  B  in  principle, we're  not  going  to  be  really  focusing  on  it, but  this  offset  is  important for  doing  classification  and  practice. I  want  to  include  it.  Let's  define our  new  parameter  vector theta  as  a  vector  that  in  the  weight  vector  along  with  B. It's  just  a  new  vector  that  includes all  the  elements  of  the  weight  vector  plus  B. Okay?  Then  we  have  the  probability  of  our  data  data. It  is  equal  to  this  formula  down  here. It's  products  from  I  equals  one  to  n probability  of  the  output y  given  input  vector  x  I,  along  with  theta. Now  we're  going  to  use  a  little  trick that  will  make  the  math  easier  for  us. We  break  this  into  basically  two  terms. We're  going  to  observe  that raising  anything  to  the  first  power is  same  as  not  doing  anything. And  raising  something  to  the  zero  of  power is  the  same  as  setting  something  equal  to  one. That's  the  trick  that  we're  going  to  be  using. This  is  the  probability  that  YI  is  equal  to  one. We're  going  to  break  it  into  two  cases. If  y  I  is  equal  to  one, we're  going  to  multiply  by  this. We  raise  this  to  the  y  I  and  then  we  multiply  this. This  is  all  inside  of  the  product  still. We  multiply  this  by  the  probability  YI  is  equal  to  zero. We  raise  that  to  the  one  minus  Y  I. Okay,  so  we  broke  this  into  two  cases. We're  multiplying  these  two  things  together. Let's  just  think  about  what  happens. There's  two  situations.  Either  y  I is  equal  to  one  or  YI  is  equal  to  zero. What  happens  when  YI  is  equal  to  one? Yeah,  only  the  first  one  remains. The  second  one  goes  away  because  we  raise  it to  the  one  minus  Y  I, which  is  equal  to  one  minus  one  or  zero. This  whole  term  goes  away  when  Y  I  is  equal  to  one. What  happens  when  YI  is  equal  to  zero? Well  then  this  term  goes  away. Okay,  just  a  little  trick, it's  gonna  make  our  lives  easier. So  let  me  just  write down  a  little  bit  of  notation  and  observations  here. Just  to  two  simple  things. What  should  we  do? Let  me  actually  do  several  things  here. Number  one,  so  this  is  just  going  to  be  notation. Let  me  move  this  over  here,  because  I'm going  to  want  to  keep  these  for  a  little  while. What  we're  doing  right  now, we  saw  how  to  do  grading  descent. We  saw  grading  descent  is  good. If  there's  any  lesson  of  this  class, that's  a  pretty  good  lesson  to  take  home, like  this  entire  course,  Grading  descent.  Good. We  want  to  see  how  to  actually  implement gradient  descent  with  logistic  regression. That's  like  the  goal  of  this  class. We  have  to  compute  a  gradient. Is  that  actually  involved  mechanically? Let's  just  do  some  notation  here. We're  going  to  say  that  the  probability, what  does  this  mean? The  probability  of  x  I like  that  doesn't  really  make  sense. That's  the  probability  of  our  input  vector. What  I  mean  by  that  is  I'm  just  defining  this as  the  probability  that  y  I  is equal  to  one  given our  input  vector  and the  weights  theta.  This  is  just  notation. Now  let  me  write  down  a  few  observations. The  first  is  that  the  probability  y  I  is  equal  to zero  given  x  I  and  theta,  what's  this  equal  to? What's  the  relationship  between  probability  that  YI  is equal  to  zero  and  the probability  that  Y  I  is  equal  to  one? Yeah. One  minus  the  other  one. This  is  equal  to  one  minus  the  probability  YI  is  equal  to one  given  ine  theta. Okay,  first  observation. Second  observation  which  takes a  little  bit  more  work  to  derive, it's  that  the probability  log  probability that  YI  is  equal  to  one  given  theta. This  is  going  to  look  like  a  sort  of  funny  formula, but  it's  going  to  be  useful. So  let  me  just  write  this  in  this  notation. This  is  going  to  be  rewriting  the  notation  here. Okay,  that's  just  notation. And  then  here's  the  actual  thing  that  you  can  prove, which  is  with  a  few  lines  of  work. It's  that  this  is  equal  to  x  times  plus  B. This  is  like  an  interesting  formula. Where  does  this  thing  come  from? Where  have  we  seen  this?  Yeah,  the  distance from  this  is  not  actually  you're  close. You're  close. But  this  is  not  quite  the  distance  from  the. Oh,  oh,  I  see  what  you're  saying. Um,  yeah,  that's  true. Yes,  that's  exactly  right. This  is  the  distance  from  the  decision  boundary for  where  else  have  we  seen  this? Yeah,  it's  from  linear  regression. That's  right,  Yeah,  it's exactly  what  we  put  into  the  sigmoid  function. Let's  just  go  back  to  our  definition of  logistic  regression. We  have  probability  y  I  is  at  one  given  theta. It's  equal  to  e to  times  x  I  plus  B  over  one  plus  either  the times  x  I  plus  basically  what  we're  saying  is  that  if we  take  the  probability  that  y  I  is equal  to  one  and  we  divide  by  one  minus  that  probability, what  we  get  back  is the  thing  that's  inside  of  this  exponent  here, which  is  also  the  distance  from  the  decision  boundary. So  this  is  useful  to  know  for  us. And  then  here's  the  third  observation. I'm  not  going  to  prove  this  here. It's  like  if  someone  is  really  curious, we  can  do  it  but  not  going  to  do  it. Then  just  third  observation  is  that  one  minus the  probability  y  I  is  equal  to  one  given  x  I  and  theta, here's  the  notation,  is  one  minus  probability  of  x  I. That's  just  a  notation  thing  that  this  is  equal to  1/1  plus  either the  I  times,  how  do  we  get  that? If  you  do  one  minus  this  thing  here, it's  equal  to  that,  that's  how  you  do  it. These  are  just  going  to  be  three useful  observations  that  we're  going  to  be  using. Okay,  let's  get  on  to  actually  computing the  gradient  for  logistic  regression, our  loss,  again,  is negative  probability  of  the  data  set  given  theta. Let's  look  at  this  thing.  Log  probability  given  theta. I'm  taking  the  log  of  this, Now  I'm  going  to  get  a  sum. This  is  a  product.  I  take  the  log,  I  get  a  sum. Then  within  this  product, I'm  multiplying  two  things. I'm  multiplying  this  thing up  here  by  this  thing  down  here, and  I  get  actually  two  sums  inside  of  that. I'm  turning  this  product  of  two  terms into  a  two  sum,  two  big  sums. It's  going  to  be  a  sum  from  I  equals  one  to  n  of probability  Y  I  is  equal  to  one given  x  I  and  theta  to  the  y.  I  have  that  term  there, and  this  is  a  new  sum. Sum  from  I  equal  one  to  n log  probability  y  I  is  equal  to  zero given  x  I  theta  and  I  raise  that  to  the  one  minus  y  I. Let's  use  our  new,  not  that  I  introduced this  notation  over  here  just  so  I  don't have  to  keep  writing  out  this  long  expression  here. So  this  is  equal  to  A  sum  I  equals  one  to n  log  probability  of x  I  to  the  Y  I  sum  I  equals one  to  n.  Oops,  one  minus. Okay,  and  what  can  I  do  now? I  have  a  log  of  something  raised  to  a  power. I'm  just  gonna  take  that  power  down. Okay,  so  next  thing  I'm  going  to  do  is  distribute. I  have  one  minus  Y,  I  times  this  thing. I'm  going  to  distribute  those  two  terms. So  now  I'm  going  to  have  two  terms  that  begin  with  y.  I. I'm  going  to  have  log  of  y,  log  of  that  thing. I'm  going  to  pull  out  the  Y. I's,  I  have  a  log  difference  of  two  terms, and  then  I'm  going  to  put  those  together  in  a  division. I'm  going  to  divide  one  term  by  the  other  inside  the  log. I'm  basically  going  to  use  just  properties  of multiplication  and  logs  from  this  point  on. I  don't  want  to  put  those  inside  this  summer. No,  fix  that. So  here's  what  we're  going  to  do. So  I'm  going  to  pull  out  the  Y  I.  I have  log  probability  x  I, and  I'm  going  to  subtract from  that  log  one  minus  probability  of  x  I. That's  all  inside  of  a  single  sum. And  then  I'm  going  to  have  sum  from  I  equals  one  to  n of  one  minus  probability of  x  I.  Okay,  good. I  have  these  two  terms  here. I  have  log  of  one  minus  log  of  another. I'm  going  to  divide  one  term  by  another, sum  from  I  equals  one  to  n  y. Okay? So  why  is  this  useful? Because  of  this  thing.  This  is basically  what  I've  been  building  up  to. I  have,  I  have  this  over  here.  Those  are  the  same  things. What  I  get  log  of  the  probability,  one  minus  probability. So  I  get  this  over  here. Sorry,  y  I  on  the  outside, times  x  I  times  W  plus, plus  I  equals  one  to  n  log  of  one  minus. And  let's  expand  this  a little  bit  over  here. One  minus,  where  have  we  seen  that  before? One  minus  probability  of  x  I.  That's  over  here. It's  this  formula  here. And  I'm  going  to  take  the  log  of  that. Where  do  I  get  back  after  I  take  the  log  of  this? I  can  actually  get  rid  of  the  one  over  thing  just  by turning  that  into  a  minus  sign  on  the  outside. Okay. Any  questions  about  this?  What  we've  been  doing is  we've  just  been  turning  the  loss  function. So  I  took  my  loss  function  over  here. I've  just  been  rewriting  it. Why  am  I  rewriting  it?  Because  I'm  now about  to  take  the  gradient  of  this  loss  function. I  want  to  see  how  does  the  loss  function change  when  I  change  the  weights  by  a  little  bit. In  general,  I  want  the  loss  to  decrease. That's  why  I  want  to  know  how it  changes  when  I  change  the  weights. Because  I  want  to  find  the  direction. When  I  change  the  weights  in  that  direction, the  loss  will  decrease  by  the  most. I'm  going  to  be  taking  very  small  steps. Any  questions  about  this? Nothing  thrilling  about  this, right?  There's  no  pretense. Nothing  at  all  thrilling.  This  is a  little  bit  of  algebra  that  we've  been  doing, and  now  we're  going  to  do  a  little  bit  of  calculus. Deep  Learning  is  composed  out of  lots  and  lots  of  very  non  thrilling  steps. This  is  Deep  Learning,  2014, or  sometime  around  then. This  is  what  your  day  to  day  life  looked like  when  you  were  doing  deep  learning. What  changed  in  2014  is that  instead  of  having to  take  all  of  these  gradients  by  hand, people  develop  programming  languages that  would  take  them  automatically  for  you. You  don't  actually  have  to  know  how  to  do this  in  order  to  do  deep  learning  nowadays. Why  are  we  doing  it  here?  It's  because  you will  never  do  this  for  the  rest  of  your  lives. No.  And  so  it's  going  to  be  completely magical  for  you  if  you  don't see  it  right  now  and  learn  it, just  you're  going  to  be  using  magic. And  it's  good  to  one  time  demystify  the  magic and  see  it's  all  composed  of  really,  really  boring  stuff. Look,  language  models  are  now just  another  layer  of abstraction  that  are  removing  you  from  doing  this. I  used  to  have  to  program  in Pytorch  and  these  other languages  for  doing  deep  learning. And  you  don't  even  have  to  do  that  anymore, you  just  say,  code  me  up  a  neural  network. It's  like  we're  moving to  higher  and  higher  levels  of  abstraction, which  is  a  good  thing  and also  means  you're  very separated  now  from  what's  actually  happening. What  I  want  to  do  is  take the  partial  derivative  of  that  loss  function. Partial  derivative  of  the  loss  with  respect to  the  weight  term  I  I  term  in  the  weight  vector. This  is  the  term  that  we  got  for  the  loss  down  here. So  I'm  just  going  to  take the  partial  derivative  of  this  thing. So  I  have  two  things  to  worry  about  here. So,  how  do  I  take  the  partial  derivative of  this  with  respect  to  WI, what  do  I  do? Factor  in  the,  so  there's  de, I  mean,  you  know  what  happens  to  the  YI? So,  a  factor,  a  factor  in  the  YI,  what  does  that  mean? Uh  huh. Okay.  Uh  huh.  Uh  huh. Oh,  let's  not  worry  about  the  second  half  yet. Let's  just  so  just, I  want  just  a  simple  expression  for  this, for  the  partial  derivative  of  this  part  of  the  expression times  the  partial  derivative  of  WI,  what  does  that  mean? You're  close.  You're  very  close. So  what's  the  times  of  X  I? And  let's  talk  about  that  first. What  is  that  dot  product  equal  to? I  multiply  each  of  the  individual  components, and  I  sum  them  up, x  times  this  is  equal  to a  sum  if  we  assume  that there  are  k  components  in  these  vectors. It's  a  sum  from  I  equals  one  to  'let's, do  j,  j  equals  one  to  k  x  j. That's  the  J  component  of  the  I  vector  x. So  this  is  our,  the  input  vector  x, IJ  J,  the  J  component. Okay,  I  have  that  in  here. The  B  obviously  goes  away. When  I  take  the  partial  derivative of  B  with  respect  to  W, that  goes  away.  So  let's  not  worry  about  this. All  we  have  to  do  is  worry  about  times  this  do  product partial  derivative  of  that.  So  what's  that? Partial  derivative.  Okay,  so  you're  right. That's  because  of  my  bad  notation.  That's  exactly  right. That's  because  of  bad  notation  I  chose. Let's  say  that  this  is the  partial  derivative  with  respect  to  J.  Thank  you. So  we  don't  want  those, those  two  things  to  be  linked  together. Okay,  So  in  the  new  notation,  what  is  it? That's  the  vector  in  our  data  set.  I  J. Exactly.  So  we're  going  to  have  I  times  j. That's  the  partial  derivative  of this  term  with  respect  to  J. Okay,  We  over  here, so  I  heard  the  chain  rule. Great.  We  definitely  want  to  use  the  chain  rule. So  what  does  chain  rule  tell  us  to  do? Function. Okay?  Function.  So  x one  plus  x,  okay? Great.  X  prime. Okay?  So  what's  F  prime  in  this  case?  The  0. Okay.  Great.  I  mean,  that's  and  so  what's  f  prime? How  do  I  do  one  over  x?  One  over  x. Great.  So  it's  going to  be  one  over  this entire  thing  that's  inside  of  the  lock. Okay?  So  that's  going  to  be  one  term  here. And  then  we're  going  to  have  the  partial  derivative  of this  thing  with  respect  to  WI  or  WJ. We  won't  worry  about  that  in  this  step. Okay.  So  this  was  the  chain  rule. We're  basically  done  with  that.  Good.  Any  questions? So,  what's  this?  Yeah,  times  X times  Y  times  X  times Y  I previous  J. So  that's  correct.  But  where  did  the  YI  come  from? Here?  Oh,  there's  no  way I  may  misheard  you.  Sorry  about  that. Okay,  so  that's  correct. So  it's  going  to  be  times  x  IJ. That's  exactly  right.  So  we're just  going  to  copy  this  part  over. So  I  equals  one  to  N  Y  I  times  x  I  j. So  this  is  the  jth  component  of the  input  vector  minus  it's  going  to  be  this  thing, so  it's  going  to  be  J. Where  did  we  get  that  from?  That's  from  taking the  partial  derivative  of  the  top  term  up  here. So  it's  another  application  of  the  chain  rule. We  take  the  partial  derivative  of  the  thing  inside  of the  exponent,  that's  going  to  be  XIJ. Then  this  one  disappears and  all  we're  left  with  is  e  to  that  thing  again. Okay,  so  let's  continue  here. So  we  have  partial  derivative  of loss  with  respect  to  weight  J. So  what  we're  going  to  do  now  is  collect up  these  two  terms  and  what  do  we  notice? So  we  put  the  E  did this  thing,  so  actually,  sorry. I  apologize.  Just  because  I  put  my  parentheses  in a  slightly  incorrect  location. I  mean,  it's  not  I  don't  know  if it's  exactly  wrong  what  I  did, but  we'll  make  it  slightly  clearer,  okay? Uh,  moving  the  parentheses  a  bit. Good.  Everything  here  is  still  inside  of  the  sum. That's  all  I'm  trying  to  say,  okay? We're  going  to  move  this  term inside  of  those  parentheses, so  we're  going  to  have  either  the  times  x  I plus  over  one  plus  the  same  thing. Why  does  that  give  us? We  have  this  term  here. We  move  that  inside the  sum  is  over  the  entire  expression. Yeah,  it gives  her  sigmoid  function.  That's  exactly  right. So  another  way  of  saying  that  is, well,  we  have  that  term, I  equals  one  n  y  times  x  I  J. And  then  minus  sum  from  I  equals  one  to n.  And  let's  move  xj  times, and  then  it's  actually  just  going  to  be the  probability  YI  is  equal  to  one  given  x  I. Again,  the  way  we  got  this  is  we  just  move over  here  to  the  times  x  I  plus  we  move  that  inside. And  now  we  have  x  I  plus  b  over  the  same  thing. That's  the  probability  of  that  y  I  is  equal  to  one. Now  we're  going  to  pull  out  this  x  I  J. This  is  now  equal  to  some  from  I  equals  one  to  n  of x  I  J  times  Y  I  minus  probability. Y  I  is  equal  to  one  given  x  I. This  is  our  partial  derivative. The  gradient,  let's  remember the  gradient  is  just  the  vector that  collects  up  all  of  our  partial  derivatives. That's  our  gradient  I've excluded  is  actually  a  little  bit  different  than  this. We  have  to  take  the  partial  derivative  of  the  loss  with respect  to  the  computation  is  a  little  bit  different. It's  a  little  simpler  but  it's  a  little  different. We're  not  going  to  worry  about  that. This  is  just  with  respect  to  all  of the  terms  inside  of  the  weight for  they're  all  going  to  look  the  same  in the  sense  that  they  all  have  this  form  numerically. Why,  in  terms  of  the  numbers,  are  they  going  to  be different  with  respect  to  one, going  to  be  different  than  two  in  general? How  can  we  tell  that  here? So,  I  mean,  this  is  actually  an  interesting, it's  a  slightly  interesting  question because  this  does  not, Y  does  not,  There's  nothing  there that  says  which  we  component  of  the  weight  vector, we're  looking  at  this  term, There's  nothing  about  which  component of  the  weight  vector  we're  looking  at. The  only  place  where WJ  comes  in  is  in  this  term  here,  the  XIJ. That's  the  only  place  where  you  see  a  J. So  you're  taking  the  partial  derivative with  respect  to  the  J  weight  component. The  place  it  shows  up  is  in  this  XIJ. Let's  look  at  this.  This  is  not just  some  random  expression  we  pulled  out. There's  a  very  natural  interpretation of  what  the  gradient  is  telling  us. Let's  look  at  this  for  a  second. We  look  at  y,  Y  is  equal, is  either  equal  to  one  or  equal  to  zero. We  look  at  the  probability  that YI  is  equal  to  one  according  to  our  current  model. We  take  the  difference  between  those  things. Is  that  difference  telling  us, we're  taking  one  minus  the  other.  Is  that  difference? Tell  us,  let's  say  that  YI  is  equal  to  one. In  fact,  the  correct label  for  Y  I,  it's  like  you're  a  dog. What  is  the  difference?  Tell  us  in  this  case. Yeah.  Tells  us  the  probability. Can  you  say  more  about  that? It's  not  so  this  difference,  you  mean? Yeah,  I  wouldn't  quite  say  it  like  that. So  this  is  we  have  a  current  model. The  current  model  that  we  have  is  our  best  guess. Our  current  best  guess  about  how to  predict  whether  something  is  a  cat  or  a  dog. And  so  it's  telling  us  what's the  probability  that  you're  a  dog. So  you're  taking  one  minus  the  probability  being  a  dog? Yeah,  exactly. This  difference  is  telling  us how  good  our  current  prediction  is. If  you're  one,  if  the  correct  class  is  one, then  you  want  the  probability  that  YI  is  equal  to  one. You  want  that  quantity  to  be  very  close  to  one. If  something  is  a  dog,  your  loss  function  is  telling  you, you  definitely  want  to  predict  that  this  is  a  dog. This  is  measuring  how  wrong  you  are, how  bad  that  prediction  is. Let's  ignore  this  term  out  here  for  just  1  second. This  term  in  here  is  telling  us  how  wrong  we  are. The  gradient  in  general  is  going  to  be  telling us  which  direction  should we  move  in  and  how  big  of  a  step  should  we  take. What  does  the  gradient  tell  us when  we're  making  extremely  wrong  predictions? Let's  say  that  every  time  the  true  label  is  one. Every  time  the  true  label  is, do  we  predict  Cat? Every  time  the  true  label  is  cat,  we  predict. What  does  this  tell  us? Yeah,  grading, it  means  the  gradient  is  going  to  be  very  large, which  means  it  changes  more. If  we're  making  really  incorrect  predictions, the  gradient  will  tell  us, change  your  model  by  a  lot, change  your  weights  by  a  lot. If  you're  only  making  very  slightly  wrong  predictions, your  gradient  tells  you  don't  move  very  far. Okay,  so  that's  one  thing  here. What  about  we  understand  now  that this  is  giving  us  the  prediction  error. This  term  here  is  the  prediction  error. The  gradient  contains  the  prediction  error  in  it. What  about  this  XIJ  term  out there?  How  can  we  interpret  that? Let's,  let's  say  the,  the  Jth  component. Let's  say  that  measures  you're  an  animal, it  measures  how  many  times, how  frequently  you  bark. Trying  to  classify  cats  and  dogs, how  frequently  do  you  bark? We've  encountered  an  object  that  barks  often. Ij,  in  that  case,  is  very  high. We  predicted,  however,  that  this  object  was  a  cat. Yeah, I  won't  say  anything  about  prior  knowledge. This  is  completely  data  driven, so  we  have  a  feature  that's extremely  high  in  a  positive  direction. So  let  me  note  one  thing  here  actually, which  is  all  of  these  computations  actually, so  there's  one  issue  with  these  computations. We  can  fix  that  issue  completely  by  saying  this. Everything  here  that  we  did  was computing  the  gradient  of the  log  probability  of  the  dataset. That's  not  what  the  loss  is. The  loss  is  equal  to what  it's  equal  to  minus  that  quantity. This  is  telling  us  how  do  we maximize  the  log  probability  of  the  dataset, the  minus  that  quantity? Let's  say  that  we're  trying  to  maximize the  log  probability  of  the  dataset. It's  equivalent  to  minimizing  the  loss. Why  do  we  want  to  multiply? We've  observed  an  object  that  barks  very  frequently. We  currently  predict  this  object  as  a  cat. What  should  we  do? Yeah,  well,  you're  right, we  want  to  flip  the  prediction, but  the  question  is  how  do  we  get  there? Like,  I  want  to  know  how  we  get  there  and  the  gradient tells  us  that  yeah, feature  change,  the  weight on  that  feature  in  particular,  that's  exactly  right. If  you  bark  a  lot  and  your  prediction  is  very  wrong, then  what  you  should  do  is make  the  weight  on  that  feature  much  higher. Because  next  time  when  you  take  the  dot  product  or  when you  multiply  that  weight  by  that  feature, it  will  be  a  very  positive  number. And  that  will  make  you  more likely  to  predict  that  this  thing  is  a  dog. When  we're  computing  the  gradient, when  we're  trying  to  understand  how to  maximize  the  probability  of  dataset, there's  two  factors  that  we  take  into  account. One  is  what  the  prediction  error  is. One  is  basically  the  magnitude  and  sine  of  the  features. If  we  have  a  positive  feature,  that's
