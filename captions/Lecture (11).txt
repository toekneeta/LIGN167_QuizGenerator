All  right,  let's  get  started. Happy  Halloween,  this  is  the  cost of  not  attending  your  lecture  in  person. So  we  released  the  project  guidelines yesterday.  Sorry  for  the  delay. With  that,  I  was  finalizing the  details  with  the  ling  one  oh  one  instructor. Basically,  he  has, I  hope  that  you've  read  the  guidelines. So  he's  now  authorized  our  use  of any  of  course  materials. This  is  Will  Styler  in  my  department. He  dumped  everything.  Voice  to  act. All  of  his  podcast  lectures, you  have  all  of  his  notes,  You  have  the  syllabi. There's  absolutely  everything  in  there. It  makes  creating  a  tutor  much  easier. Obviously,  it'll  be  helpful  if  you have  some  familiarity  with  the  class  material, but  a  lot  of  it  probably  you  can  pick  up, it's  an  intro  class  for  this  class, There  will  be  a  little  bit  more  legwork  that  you'll  need to  do  in  terms  of  compiling  the  course  materials. I'm  happy  to  give  some  advice  about  that. My  podcasts  might  be  a  good  source,  for  example, if  you  use  a, the  text  processing  on  that. Any  questions  about  the  project? Yes.  Yeah,  It's  probably not  very  good  though,  is  it?  Yeah. Okay.  I  mean,  you  might  want  to  try. I  don't  know  how  well  we'll  be  able  to pick  up  like  math  and  things  like  that. I  guess  I  would  have  a  question  about  that. Whisper  is  very  good  if  you  want an  open  source  method  for  doing  this. I  would  also  recommend the  textbooks  that  I  have  assigned  for  the  class. They're  all  free,  they're  all  online. I  don't  think  there  are  any  copyright  issues  with  that. We're  not  trying  to  sell  anything. This  is  simply  for  a  class  project. I  think  you  should  feel  free  to  use the  textbook  material  we  have. Three  of  them  are a  lot  of  other  open  source  material  on  the  same  topics. Online,  I  would  say  in terms  of  where  to  get  course  content  for  this  class, I  would  just  look  at  the  class  textbooks  and also  many  sources  online  that  are  high  quality. For  the  same  material,  for  your  proposal, I  basically  need  a  sketch  of  what  you're  going  to  do. I  think  we  said  it  should  be  around  one  page. What  does  one  page  means? I  need  to  know  what  you're  going  to  do,  right? So  give  me  enough  details  and  then  make  me  feel  confident that  nothing  disastrous  will happen  when  you  go  off  and  do  this  thing. Any  other  questions? Let's  talk  about,  just  a  little  bit, how  to  use  GPT  four  for  this  purpose. So  let  me  show  you  how  not  to  use  GPT  four. Okay,  write  the  code for  an  AI  tutor. Okay.  So  this  is  not  what  you  want. I  don't  know  what  it's  doing  here. It  is  giving  you  something. It's  definitely  not  what  you  want. Honestly,  almost  no  matter,  I  think, how  much  text  you  give  this, if  you  just  ask  it  to  directly  go  from your  text  description  to  code, you're  not  going  to  get  what  you  want. What  would  I  do  instead? Start  a  new  chat.  What  I  would  do  instead  is  say, I'm  writing  a  tutor. I'm  creating,  I  should  say, a  tutor  for  an  introduction  to  Linguistics  class. I  have  all  of  the  course  material, notes,  syllabus,  et  cetera,  available  to  me. What  would  I  like  to  do?  I'd  like  to  create an  interactive  textbook  for  this  class. What  would  I  like  the  interactive  textbook  to  do? The  textbook  will  be  powered  by  GPT  four. I'll  be  calling  the  GPT  four  API. I'd  like  to  build  something  using  this  technology. What  I  would  say  at  this  point  is, given  that  I've  stated  my  high  level  goal, let's  brainstorm  about  what  exactly  this  should  look like.  What  are  we  saying? Let's  brainstorm.  We  need  P  four  is  going  to  give  us ideas  that  will  help  it  to  fill  in everything  that  it  needs  to  know in  order  to  create  such  a  textbook. It's  exciting,  Chris,  like all  of  you  should  be  excited  about  this. Okay.  Interface,  it  talks about  content  P,  four  integration  interactive. You  don't  want  a  lot  of  this  stuff. As  I  said  in  the  project  definition, the  guidelines  that  I  gave you  make  something  simple  that  works, that's  much  better  than having  feature  blow  where  nothing  works. Let's  tell  it,  this  is  way  too much  I  need  to  build  a  minimum  viable  product. Okay,  so  I  gave  it  some  clarification. So  these  are  pretty  good  features  by  the  way. I  like  you  might  want  to  focus  on  even just  one  of  them  that  would  be  perfectly  fine. This  is  probably  a  little  bit  too much  like  you  would  want  to  use  your  judgment  here. Maybe  you  don't  need  quizzes  right  now. Let's  just  focus  on  explain text  or  maybe  just  focus  on  the  quiz  part  alone. I  would  choose  like  one  of  those  features. Okay,  this  is  interesting, scalability,  I  mean,  you  can worry  about  that  or  you  cannot  worry  about  that. Again,  the  goal  is  to  get  a  minimum  viable  product. Okay,  we  have  some  ideas. Now,  at  this  point, what  I  would  tell  GPT  four is  exactly  which  features  I  want. Let's  focus  on  auto  automated  quizzes  at the  end  of  chapters. Okay? What  I  would  say  now  I  want  to  think  about a  plan  for  this  that's  very  important. We  don't  want  to  have  GP  four, Just  rush  into  giving  us  code, even  at  this  point  the  problem  is  more  specified. Now  the  code  that  we  get  out  will  be  much  better than  what  I  said  when  just said  like  write  me  an  AI  tutor. But  it's  still  not  going  to  be  what we  want  we  want  to  plan. This  is  related  to  something  that  is known  in  the  field of  language  modeling  or  large  language  modeling, a  chain  of  thought. This  is  a  surprising  and  emergent  behavior of  large  language  models. This  is  something  that  occurred in  language  models  without anyone  having  ever  trained them  for  this.  What  does  it  mean? It  means  that  before  you  get language  models  to  give  you  an  answer, say  write  code  for  you. You  first  have  them  think  in an  incremental  way  about how  they're  going  to  solve  the  problem  for  you. You  have  them  write  down  a  plan, let's  say  only  after  the  plan  is written  do  you  actually have  them  write  the  code.  It's  very  similar. That's  at  least  how  it  works  with  coding. It's  very  similar  to  how  you might  architect  a  software  system  yourselves. When  you're  trying  to  build a  complicated  piece  of  software, you  might  just  rush  into  writing  code. It's  not  terrible  always  to  do  that, but  the  first  thing  that  you  write  is  not  going  to  work. If  you  really  want  to  like  engineer  a  system, you're  going  to  write  down  a  plan, an  architecture  for  what  this  system  looks  like. And  you're  going  to  think  about what  should  the  architecture  look  like. Then  you're  actually  going  to  implement  it. It's  the  same  thing  here. This  actually  improves  GP four's  behavior,  That's  one  thing. The  second  thing  is  that  it allows  you  to  critique  parts  of  it. It  allows  you  to  understand  what's  going  on. You  can  say,  no,  I  don't  want  you  to  build  it  that  way. If  you're  going  to  use  GP  four to  help  you  build  this  system, which  I  strongly  recommend  that  you  do, because  you're  going  to  get something  better  more  quickly. If  you  do  this,  what  I  would suggest  is  you  make  a  collaborative  plan  with  GPT  four, then  you  have  GP  four. Implement  each  step  of  the  plan  one  by one  and  you  can  test  each  step. I'd  like  a  plan. This  is  actually  not  good  in  the  following  sense. What  it's  saying  is,  how  are  we going  to  get  the  content  for  the  quizzes? Extract  key  points  from  each  chapter. This  could  be  done  manually  or  using  NLP  techniques. So  to  me,  manually  may  be  okay. Honestly,  it's  like  textbook  only  has  so  much  material  in it  you  could  potentially  just  go through  and  say  what  you  want  the  quiz  to  be  about. It's  not  the  worst  strategy. Using  NLP  techniques  sounds  to  me  like  it's  saying  not using  GPT  in  general. I  would  not  inject  any  NLP  into this  tutor  that  is  not  GPT  because  it  won't  work. You  know,  it's  a  sad  commentary  on  the  state  of the  field  if  you go  back  more  than  six  months,  but  it's  true. Nothing  that  we  had  six  months  ago  worked  that  well. It  might  work  well  in  limited  circumstances, but  you  don't  have  time  to  do  all the  engineering  necessary  to really  like  hammer  that  out,  just  use  T  four. So  what  I  would  say  here  is  like  if  it  tells you  use  NLP  techniques  at  any  point, push  back  and  say,  no,  no,  no, I  want  to  extract  the  relevant  material using  GPT  Four,  okay, decide  on  various  types  of  questions  four  to help  generate  diverse  and  contextually  relevant  questions based  on  chapter  content. That's  not  bad,  looks  pretty  reasonable  to  me. We'd  have  to  go  through  this.  What  I  would  do then,  This  is  great. Look  at  this  technology  stack. No  JS  with  Express  for  service  logic, Python  with  flask  or  Jangof. Using  more  complex  AI  or  ML  components  database for  storing  questions  is  complicated. P  four  will  help  you  with  all  of  this  stuff. I  don't  know  how  to  do  a  lot  of  this  stuff, I  would  not  be  scared  about  it. Nonetheless,  hosting  you'd  want  to  clarify, we  don't  need  to  host  on  AWS. Hosting  on  your  personal  laptop  is  fine, so  it  can  be  accessed  locally. Compliance  and  security, We  don't  need  to  worry  about  this. It's  going  to  be  making  suggestions  that  you don't  need  to  use  your  judgment. And  I'm  happy  to  discuss  with  you as  what  you  do  or  do  not  need  here. Like  this  stuff  we  don't  need  to  do, even  though  it  would  be  nice. Long  term  deployment  and  maintenance. Interesting  ideas  here  we  probably  don't  need  quite  yet. So  it  doesn't,  but  let's  just  say  it  did, This  looks  like  a  good,  I'd  like  to  write  the  code. For  this  in  an  incremental  manner. Where  do  you  recommend  we  begin? What  are  you  noticing  about  how  I'm  talking  to  this? It's  like  I'm  allowing GP  four  to  do  a  lot  of  the  work  for  me. I  don't  know  where  to  start  here. I'm  saying  where  do  I  start, But  I'm  also  not  telling  it  to just  make  a  decision  for  me  yet. I  want  to  understand  what  the  different  trade  offs, what  the  possible  options  are, what  the  trade  offs  are,  then we'll  make  a  decision  together. I  would  treat  it  like  a  very  helpful  human  assistant. It's  crazy  to  think  of  it  that  way, but  you're  going  to  get  the  most  out  of  it if  you  do,  okay? What  does  it  say  here?  Basic  quiz  framework, a  basic  but  functional  quiz  system  for a  single  chapter  with  predefined  questions  and  answers. It's  not  a  bad  idea. Don't  worry  about  the  GPT  four  stuff  for  a  little  bit. Get  something  up  and  running,  that's  very,  very  simple. Look,  this  is  pretty  smart  database  integration also  not  a  bad  idea  to  do  that. You'll  have  a  place  to  store  your  questions at  this  point  is  where  you  do  your  P  four  integration. Look,  this  seems  like  a  pretty  good  plan. And  then  I  would  say,  okay,  let's  do  step  one. I'll  say  that  for  the  user  interface  stuff, you're  welcome  to  not  do any  of  the  heavy  user  interface  lifting  yourselves, as  long  as  you  credit  them,  use  external  frameworks. And  we  should  actually  ask  T  about  external  frameworks, it  recommends  you  don't  have  to. You  can  make  a  real  nice  looking  user  interface  without actually  having  to  write  all  of  that  code  yourselves. You  probably  don't  want  GPT  four  directly  writing too  much  HTML  because  they  won't  look  that  nice. What  I  would  say  is  like  feel  free  to use  frameworks  that  just  have a  lot  of  stuff  already  built  in  for  you. That's  something  I  would  want  to  discuss  with  P  four. Okay,  I'm  assuming  this  code  is  pretty  good  here. It  gave  us  I  don't  know  how  to  even  use  this  code. I  don't  know  what  to  do  with  it.  I  would ask  what  do  I  do  with  this? Okay,  I  think  we  get  the  idea  though. This  is  how  I  would  approach  and  I  think  it's pretty  clear  I've  just accelerated  the  project  development  by  a  lot. I  would  have  had  no  idea  where to  start  here  without  this. All  of  a  sudden,  we  have  sort  of  the  skeleton  of  an  app. I  think  we  know  how  to  do  this. Any  questions?  Okay.  Yeah,  yeah, yeah,  yeah,  I  would  recommend  it. So  here's  a  reason  why  the  models  might  be  sensitive. I  haven't  seen  any  systematic  investigation of  this  with  GPT  four, but  I  would  recommend these  models  were  initially trained  to  do  next  word  prediction. That's  where  the  core  intelligence  has  come  from. They  look  at  the  previous  contexts. They  predict  what's  the  next  word  after  that. When  the  models  see  misspelled  words, grammar,  what  does  it  think  to itself  about  the  type  of next  word  prediction  that  it's  doing? What  does  it  unprofessional? But  let's  be  more  specific  than  that. It's  unprofessional.  What  does  that  mean? About  the  model  is optimized  to  just  predict  the  next  word,  right? Yeah,  what  does  that  mean? Yeah,  exactly. So  it  might  be  in  a  situation  where the  speakers  don't  know  what  they're  talking  about, that's  why  they  have all  these  grammar  errors  and  spelling  errors. And  what  it's  going  to  be  trying  to  do  is predict  what  type  of  answer  I  would receive  would  get  in response  to  this  question  is going  to  be  an  incorrect  answer. It's  going  to  be  doing  a  very  good job  at  next  word  prediction. It's  just  not  what  you  wanted  to  do. In  principle,  GPT  four  should  not  be sensitive  to  this  anymore,  but  I  suspect  it  is. Any  other  questions?  Yes.  So  is there  a  degree  of  grammar  errors? Errors  that  are  still  similar  Words. Started  letter,  yeah. Words  recognition. Four,  sees  everything  that  you  write, it  sees  any  error  that  you  have. There's  multiple  things  going  on  there. The  way  that  GPT  four  perceives  words  is  actually very  weird  and  it  has  to do  with  an  issue  called  tokenization. We'll  get  to  that  in  a  few  weeks. I'll  just  four  does not  see  spelling  errors  the  same  way  that  we  do. Look,  what  I  would  say  is  to  be  as  safe  as  possible. We  professionally  with  GPT  four. You  could  have  grammar  or  something  in  your  pipeline for  your  input  to  GPT  four  if  you  really  want  to. By  the  way,  grammarly,  wow. That  company  is  in  a  lot  of  trouble. I  remember  for  a  few  years the  only  Youtube  ad  I  would  get  was  for  grammarly, and  they  must  have  blown  like hundreds  of  millions  of  dollars  on  those  things. Yeah.  Like  their  technology  is  basically  irrelevant. Now,  I  feel  very  bad  for  them. Okay.  Okay. I  think  hopefully  people  feel a  little  bit  better  about  the  final  projects  now. Again,  I'm  happy  to  answer  more  questions  about  that. I'm  very  excited  to  see  what  people  produce  today. We're  going  to  at  least  start  off  by  talking  at, let's  say,  a  little  bit  more  about automatic  differentiation  and  also  about  batching. Let's  go  back  to  the  computation  graph  that  we wrote  down  last  time. We  have  an  input  vector  x, we  have  a  weight  matrix  one. Remember,  this  is  a  box  indicates  an  input, a  diamond  indicates  weights  that  learned  parameters, things  that  we're  going  to  be partial  derivatives  with  respect  to. We're  taking  the  gradients  with  respect to  the  stuff  in  the  diamonds, we  have  circles,  these  are  activations. We  draw  arrows  into  the  activations to  indicate  what  depends  on  What  we're saying  here  is  that  this  vector  a  that  we're defining  depends  on  x  in  W.  How  does  it  depend? It's  through  multiplication. Let's  just  call  that  MUL  for  multiplication. So  that's  matrix  vector  multiplication. We  have  a  vector  that  we  get  from  a  alone  by  taking  a. Let's  say  this  is  a  one  layer, multi  layer  perceptron,  one  hidden  layer. We  use  this  directly  to compute  a  vector  that's  through  softmax. I  actually  skipped  a  step  here. Really  what  we  should  have  done, we  have  two  here. This  is  multiplication. Here  we  get  by  multiplying  with  a  weight  matrix  two, and  then  we  get  a  vector  of  probabilities  using  soft. This  was  our  computation graph  formulation  of  the  problem, basically  where  we  just  see what's  the  stuff  that  we're  learning  and  we're taking  partial  derivatives  with  respect to  what  are  our  inputs  for  the  activations. How  are  they  each  computed? Which  activations  are  used  to compute  which  other  activations? What  operations  are  used  to  combine  these  things? Let's  write  down  the  Pytorch  code  for  this. There's  some  version  of  the  Pytorch  code. This  is  like  pirtudocde. Let's  say  x  is  equal  to, let's  call  it  get  sample. I'm  just  going  to  have  a  data  load. What  is  x?  So  we  are  the  dimensions  of  x. Let's  just  say  that  this  is a  tensor with  coordinates  in  it. It's  a  vector,  but  we'll  just  call, I'll  use  this  notation  to  indicate  this  has one  dimension  with  k  coordinates. We  can  think  about  this  as  a  vector  with  terms  in  it. Okay.  What  is  one? One?  This  is  going  to  be  a  matrix. This  was  actual  porch. I  would  need  to  have  a  separate  it for  doing  this.  I'm  not  going  to  do  that. I  just  want  to  say  this  is  truly  pseudo  code  right  now. I  just  want  to  understand  in  a  very  broad  sense, code  and  computation  graphs  are  related  to each  other,  n  parameters. Here's  where  we  can  give  the  dimensions  of  this  thing. What  should  the  dimensions  of  this  weight  matrix  B? We  actually  know  something  about  it. So  what  do  we  know? What's  that?  The  size  of  the  input. So  why  does  that  tell  us  about  this? At  okay,  the  size  of  the  input. That's  going  to,  why  is  it  constrained  in  one? We're  doing  one  times  x. What  has  to  match?  Yeah,  exactly. The  number  of  columns  have  to  match  the  number  of  rows. And  x,  x  has  k  rows. Let's  just  say  that  this  is  a  four  times  K  matrix. This  is  basically  how  we  would  indicate  this. We  give  pitch  the  dimensions  that  we  want  for  this  thing. We've  now  created  a  matrix  that's  four  k. What  do  we  do  now? I'm  trying  to  write  down  a  program  here that  corresponds  to  the  computation  graph  over  here. What  do  I  do  now? Yeah,  okay, We're  going  to  multiply  x  and  W.  There  are several  different  ways  in  pitch  to  do this  I  want  to  use  to  do  this. We'll  see  an  advantage  of, it's  almost  like  taking  out  like a  flame  thrower  in  order  to  light  a  candle  or  something. But  we're  going  to  see  the  advantage  of  ins  bit. We  remember  what  do  I  do  for  Einsum  here. The  way  that  we  write  inside  of the  quotations  basically  works  the  same  as nums  which  you  learned  about  your  homework. So  what  did  we  do? Do  we  remember  einstom  a  few  weeks  ago? Let's  do  a,  let's  do  a  quick  refresher  on  Eins,  Okay? So  let's  say  that  I  have,  uh, two  matrices,  x  times  Y,  okay? So  X  is  a  capital  X.  X  is  equal  to, let's  say  a  ten  by  five  matrix Y  is  equal  to  five  by  seven  matrix. What  we're  going  to  get  out  from  this  is What?  It's  going  to  be  a  ten  by  seven  matrix. When  we  multiply  these  two  things  together, I  can  write  this  as  Einsum  of  what? The  way  I  write  this  is  the  first  index, that  is  the  row  index  in  x, then  the  column  index  in  x, then  the  row  index  in  y, the  column  index  in  y.  Then  I  write  arrow. The  way  I  think  about  this  is  if  you  give me  I  and  J  x  J  and  K  Y, what  do  I  do  with  those  indices? What  I  do  with  them  is  I'm  going  to  sum  over  j, I  sum  over  the  columns  in  x, and  I  sum  over  the  rows  and  y. That's  how  I  define  matrix  multiplication. When  you  do  matrix  multiplication, you  take  an  entire  column  and  y, you  take  the  product  of  that  with  a  row  in  x. That  means  that  you  sum  over  a  single, you  sum  over  all  the  rose  and Y  and  all  of  the  columns  and  X, this  gives  me  I k.  Why  are  the  two  inputs,  they're  x  and  y. Let's  just  remember  exactly  what  this  notation  means. This  means  that  the  result, let's  call  this  thing,  we'll  call  z,  the  result  of  this. Then,  another  way  of  saying  this  is that  K  index  in  this  new  matrix  Z is  equal  to  a  sum  over  j of  x  j  times  y  JK. So  this  is  just  another  formula  for matrix  multiplication  and  we  translated, this  is  a  formula  for  matrix  multiplication. This  formula  says,  how  do  you  get  index  k  in  the  output? It's  by  summing  over  the  J indices  when  you  multiply  x  y  together. Questions  about  this?  Yes,  perform  no. There's  the  main  advantage is  in  being  able  to  understand  your  code. There  can  be  performance  advantage. But  I  would  say  the  main  thing  is  that  it's about  being  able  to  understand  your  code  when things  get  more  complicated  than  just multiplying  two  things  together. Yes, you  mean  so W  and  X,  okay,  great  question. Okay,  so  it  seems like  you  have  an  idea.  Me,  tell  me  what  to  do. I,  I arrow  exactly. Then  what  do  I  give  it?  I  give  it  two  inputs, one  and  let's  call  this  thing. I  don't  know  how  I'd  write  that  in  Python, but  let's  just  call  it  one  for  right  now. And  that's  exactly  right. In  this  case,  over  here  we  have  one  times  x. One  is  a  matrix,  is  a  vector. We  have  a  is  equal  to  that  as  a  new  vector. What  we're  saying  here  is  that  the I  place  in  this  new  vector, I  look  to  the  right  hand  side  of  the  arrow. I  is  equal  to  a  sum  over  all  of  the  terms, J  one J  x  J  J. I  take  the  ith  row  in  one, I  take  the  dot  product  between  that  row  and  X. That  gives  me  the, the  term  in  A. Any  questions  about  this? What's  h? H  is  relu  of  a  pitch, that's  literally  just  how  we  write  it. H  is  equal  to  torch  of  A. What  about  to  get  O? We  also  need  to  find  22  is going  to  be  a  new  matrix  parameter. Where  are  the  dimensions  of  two? What  do  we  know  about  them?  We  know  one  thing  about  them. O  is  going  to  be  a  matrix. It's  going  to  be  something  by  some  Do  we  know? Yeah,  something  fantastic. It's  exactly  right.  We  know that  is  eventually  going to  be  transformed  into  probabilities. The  probability  of  each  class, we're  going  to  have  the  number  of  classes. That's  going  to  be  the  output  dimension  from  this  matrix. And  the  input  dimension  is  going  to  be, now  why  am  I  using this  special  thing  here  and  end  parameter? There's  other  ways  of  doing  it  in  that basically  serve  the  same  function. But  why  am  I  writing  it  Like  what  does this  special  thing  that  I'm  saying here  tell  Pitch?  It  tells  Pytorch. These  are  learned  weights  or  learned  parameters. We  need  to  especially  indicate that  Pytorch  doesn't  know  what's the  stuff  that  we  want  to  learn  and  what's the  stuff  that  we  don't  want  to  learn  that's  a  constant, we  need  to  tell  it  explicitly. Okay? Then  we  have  a  new  vector. It's  going  to  be  exactly  the  same Einsum  that  we  had  over  here. It's  going  to  be  a  matrix  vector  multiplication. Ij  J  goes  to  two  H, and  then  finally,  well  not  quite. Finally,  almost  finally,  we  have  P  equals Torch  Softmax  of  O. We're  going  to  have  one  last  step  here  basically, which  is  we  need  to  compute  the  loss. We  need  to  say  given, we  need  to  compute  the  loss,  what  should  we  do? We  should  actually  fix  this. Let's  say  we're  going  to  get  x  here. We'll  also  get  the  label  when  we  call  get  sample, get  sample  is  going  to  return  to  us  two  things, x  and  the  label  for  x,  what  class  is  it  in? Then  we're  going  to  say  that  loss  is  equal  to, let's  just  say  that  we  have  some  loss  function. I'm  describing  exactly  how  we  have  that, but  we  have  a  loss  function. We,  we  give  the  loss  function, we  give  it,  we  give  it  the  label. Given  those  two  things,  we  can  compute  the  loss. We've  now  reconstructed this  entire  computation  graph  in  Pytorch. We  then  have  a  step  that  you  should  have learned  about  on  your  last  problem  set, which  is  what  we  call  lost  Stop  backwards. What  does  this  do? Okay,  it  does  automatic  differentiation. So  tell  me  about  that. Tell  me  me,  broadly  speaking,  what  happens. That's  right.  So  it  goes  back  and calculates  gradients  for  each  vector. So  another  way  of  saying  that  is, let's  say  over  here. Okay,  let's  start  over  here. When  we  call  lost  stop  backward, what  we're  doing  is  we  end  up  calculating how  the  loss  changes  with  respect  to  each  of  the  inputs. We  basically  a  gradient, partial  derivative  of  the  loss with  respect  to  each  of  its  inputs. That  for  we  take the  partial  derivative  of  the  loss  with  respect  to  one, where  I'm  assuming  that  one is  the  probability  of  the  first  class. How  does  the  loss  change  with  respect  to  that? We  do  that  for  each  of  the  probabilities  in  P.  We  store that  in  a  gradient  associated  with  this  variable. I  said  for  each  of  the  inputs. What  about  label?  Label  is  which  class  you're  in. What  happens  there? Do  I  calculate  how  the  loss changes  when  I  change  the  label? No,  no. Okay.  So  people  who are  strongly  shaking  their  heads,  why  not? Yeah. Okay,  great.  So  that's  one  answer. The  labels  don't  change,  they're not  actually  being  tracked. There's  basically,  there  are  variables  that  are  being tracked  by  the  automatic differentiation  machine  that  Pytorch  has, and  then  there  are  variables  that  are  not  tracked. This  is  not  a  track  variable, We'll  get  back  to  that  in  a  second. What  makes  a  variable  track  or  not? There's  another  reason  why  we're  not  taking the  partial  derivative  or the  gradient  with  respect  to  the  label. Why  is  that  the  labels  are  fixed? That's  one  reason.  It's  a  very  good  reason. What's  the  other  reason? Yeah,  we're  not  trying to  learn  it,  but  that's  sort  of  what  we  mean. But  when  we  say  it's  fixed,  Yeah,  you're  right, we're  not  trying  to  learn  the  label,  let's  say. What  I  was  trying  to  learn  the  label I  would  say  is  that  the  gradient  is  not  even  defined because  the  label  is  a  discrete  thing. Think  about  next  word  prediction. Okay,  the  label  might  be,  what's  the  next  word? Maybe  the  next  word  is  sauce. Okay,  so  the  correct  word  was  sauce. What  does  it  mean  to  try to  change  the  word  sauce  a  little  bit? Like  I  guess  you  could  replace the  E  with  an  A  or  something, but  there's no  mathematical  formula  that  corresponds  to  that. You  can't  change  words  or discrete  categories  by  a  little  bit. We  can't  compute  a  gradient  there. Okay,  Now  we've  computed partial  derivative  of  the  loss with  respect  to  all  of  the  terms. And  at  what  do  we  do  at  this  step? Well,  at  this  step  we  calculate  how does  change  when  we  change  a  bit. Then  we  multiply  that  by  how  the  loss  changes. When  we  change  a  little  bit, we're  keeping  track  of  each  stage of  how  changing  something, let's  say  O  changes  the  loss. And  we  do  that  just  through  our  normal  back  propagation. In  the  process  of  doing  this, we  go  backwards  through  this. In  the  process  of  doing  this, Pitorch  automatically constructs  a  computation  graph  that  looks  like  this. It  says,  okay,  I  had  an  output  variable  here. How  is  constructed? Let  me  see.  Took  as  input  O. After  applying  a  Softmax  to  it, I'm  going  to  insert  into  this  graph  I  had  here. I'm  going  to  insert  into  this  graph  and  I'm  going  to  say, how  are  they  related  by  Softmax? I  look  back  at  how  is  constructed. I  see  is  constructed  using  two  inputs, two  and  h.  I'm  going  to  insert  two  nodes  here  and  two. It  goes  through  basically  your  program. It  goes  backwards  through  your  program  and constructs  a  computation  graph. And  then  it's  going  to  do  back  propagation  according to  what  this  computation  graph  says  that  you  should  do. Go  ahead. I  mean,  that's  how  you  do  it  in. There's  no  other  way  to  Is  there  another  way? I  mean, I  would  say  this  is  like the  standard  way  in  Pytorch  to  take  gradients, like  it  triggers  a  call  to  the  pitch. Automatic  differentiation  machinery. You  mean  specifically? Okay.  Right.  I'm  writing  down  pitch. There's  only  two  other  competitors  that anybody  one  of  them besides  pitch,  one  of  them  is  tensor  flow. One  of  the  other  one  is  Jack. The  history  here  Pitch  was  developed  by  Facebook. Basically  Facebook  and  almost  everyone  else  uses  Pitch. All  other  researchers,  Tensorflow  and  Jacks, were  both  developed  by  Google. If  you're  at  Google,  you  have  to  use  one  of  those. Google  is  shifting  away  from  Tensorflow  into  Jack. Jack  does  have  some  nice  properties for  doing  large  scale  machine  learning. Maybe  a  few  other  companies  that are  training  large  models  that  have shifted  into  using  a  part of  Stanford  is  also  using  Jackson. There  are  some  other  groups  using  Jacks. It  has  some  nice  properties. But  what  can  I  say  about  this? Let  me  say  I,  do  you  want  to? Definitely  not  for  this  class, not  for  simple  projects  that you  have,  unless  you  really  want  to. Do  you  want  to  use  it  for  more  complicated  things  maybe. But  then  you  have  to  think  about  some  trade  offs. Pye,  does  that  answer  your  question?  Okay. Okay.  So  how  do  we  know what  stuff  we  want  to  learn  and  what  stuff  is  an  input? So  we  don't  want  to  take partial  derivatives  with  respect  to  x,  right? So  when  we  say  here  at the  partial  derivative  of  the  loss  with respect  to  x  here,  how  do  we  know  that? The  way  that  we  know  that  is  the  stuff  that  we  want, the  partial  derivative  with  respect to  the  stuff  that  we're  trying  to  learn. We  indicate  that  using special  notation  here  I  used  and  then  that  parameter. There's  other  things  that  you  can  say. They  basically  do  the  same  thing  but  there  are special  flags  that  tell  Pytorch  hey, here's  the  stuff  I  actually  want  to  learn. That's  how  we  know  when  to  draw  a  diamond  basically. Okay,  okay,  I  think  we  have  an  idea  now. It's  a,  hopefully  a  little  bit  less mystical  what's  happening  when  you  write some  Pytorch  code  and  all  of  a  sudden  you  can  take gradients  and  then  do  gradient  descent. It's  just  turning  your  code  into  a  computation  graph, running  back  propagation  on  that  computation  graph. Let  me  mention  one  other  thing  here  which  is, do  you  notice  whenever  I'm  calling  these  operations, I'm  saying  to  sum  Softmax  torch. Why  am  I  doing  that?  I  could  call  NNP, I  could  use  the  NPs  here. What  would  be  the  consequence  of  that? Why  did  I  use  the  torch  one  rather  than  Numpy? It  will  actually  still  keep  track  of  that  stack even  if  I  have  a  num  pi  operation  there. Because  it  can  go  back  and  see  how  was  constructed. Oh,  it  was  using  a  Ni  operation,  it  can  do  that. There's  another  reason  why I'm  using  the  I  think  you're  on  the  right  track. It's  about  allowing  to  keep  track  of  stuff. So  what's  going  on  here? I  saw  another  hand  raised  to  Relu. Why  don't  I  just  write  my  own  Relu  function. I  could  do  it  actually.  And  it  would  run  fine. Like  at  least  the  forward  pass  would  run  totally  fine. And  then  the  backward  pass  would  even  keep track  of  the  fact  that  I  had  that  function  there. But  there  would  be  a  big  problem  that  I  would  encounter. We  know  what  it  is.  Okay.  It's  that  torch would  not  know  how  to  take  the  gradient, in  that  case  it  would  not  know  what the  partial  derivatives  are. When  I  call  Torch  that  relu, it  has  built  into  Lu, here's  how  you  differentiate  through  relu. When  you  change  A  a  little  bit, here's  how  H  will  change  for  Torchin  has  built  in, here's  how  you  calculate  gradients  for  Einsum. That  information  is  just  not  there  if  you don't  use  one  of  these  torch  operations. I  want  to  emphasize  again  this  is pseudo  code  that  you would  actually  need  to  write  like  a  full, you  need  to  write  an  Nn  module  class. If  you're  writing  a  real  model  this  won't, as  I'm  writing  it  there's some  stuff  that's  actually  very  wrong  with  it. But  it's  the  simplest  way that  I  can  give a  general  idea  about  what's  happening  here. You're  going  to  be  using  pitch, good  for  us  to  understand  this,  okay? Let's  talk  about  batching  now. We're  going  to  talk  about  why  we  want  to  do  batching. Going  to  erase  this  because we  have  our  computation  graph in  almost  all  the  code  here. Why  would  we  want  to  do  batching? Let's  go  back  to  our  graph  over  here. A  is  equal  to  one  times  x, Let  me  actually  x is  equal  to  a. I'll  write  this  as  to indicate  a  dimensional  vector  with  k  coordinates  in  it. Dimensional  vector one is a  four  K  by  K  matrix. Then  I  have  a  is  equal  to  one  times  x. When  I  run  my  code  here, what  I  end  up  doing  is running  each  example  through a  forward  pass  of  this  network. One  by  one,  I  get  a  single  example. I  run  it  through  the  network. I  take  the  step,  I  take  the  gradient. Maybe  I  want  another  example  to  add  into. I'm  doing  stochastic  grading  descent. What  do  I  do  in  stochastic  grading  descent? I  add  up  the  gradients  for  a  bunch  of  different  examples. I  can  still  do  that  here.  How  do  I  do  that? Run  one  example,  I  take  the  gradient, I  add  that  gradient  to  the gradient  from  the  next  example. Add  that  to  the  gradient  from  the  next  example. And  so  on  and  so  on  and  so  on  until  I get  to  a  sufficient  number  of  examples. Okay?  I  can  still do  stochastic  gradient  descent  with a  large  batch  size  using  this  code. But  what's  important  to  understand  is  that  in  this  code, I'm  just  running  one  example  through  the  code  at  a  time. There's  another  way  to  not  run the  examples  one  at a  time  through  what  would  I  do  instead? The  batched  version  would  say  X  is  now  a  matrix. X  is  a  matrix  with, let's  say  N  examples  in  it, each  with  K  coordinates. This  is  an  N  by  K  matrix. N  is  now  equal  to  my  batch  size. It's  the  number  number of  samples  that  I'm  running  at  once. Rather  than  process  one  sentence  at  a  time, maybe  I  decided  to  process  1,000  sentences  at  a  time. N  would  equal  1,000  There  X  is  an  Bk  matrix. One  is  still  same  dimension  weight  matrix. I'm  going  to  be  using  the  same  weight  matrix  to process  1,000  sentences,  Webb  A. Now  A  is  no  longer  going  to  be  a  vector. A  is  itself  going  to  be  a  matrix. A  is  going  to  do  is  it's  going  to  just  collect  up the  matrix  multiplications  that  I got  from  all  1,000  different  sentences. So  what  should  a  equal? Yeah,  that's  a  great  question. Let's  say  we  wanted  to  keep  it  by K.  We're  about  to actually  see  the  advantage  of Eins  a  little  bit  for  exactly  this  reason. But  let's  say  we  wanted  to  keep  it  in  by  K  matrix. What  would  we  have  to  do  here? Take  the  transpose,  yeah, we  swap  the  dimensions. A  is  now  going  to  equal  one  times  x  transpose. Some  people  may  not  have  this  picture  in  their  heads. Okay,  let's  write  this  out  explicitly. This  is  equal  to  one  times  it's  x  transpose. Now  we  have  a  k  by  n  matrix. The  way  that  we  think  about  this  is that  if  it's  a  k  by  n  matrix, each  column  is  going  to  be  a  separate  example. This  is  a  new  matrix. Where  example  one  is  in  the  first  column, example  two  is  in  the  second  column, example  is  in  the  last  column. What  we  do  is matrix  multiplication  do  it  says  take  the  first  column, multiply  it  by  one, that's  going  to  be  the  first  column  of  a. Take  the  second  column,  multiply  it  by  one. That's  going  to  be  the  second  column  of  A  we  go  through. And  now  as  it's  going  to  be  four  K  by  N  columns, four  K  rows  by  N  columns, what  we're  doing  here  is  we're collecting  all  of  the  examples  into  the  columns  of  A. Instead  of  processing  all  of  the  examples  one  by  one, we're  processing  them  all  at  once. This  is  not  like  the  rapturous  response that  I  was  hoping  for  from  the  class. I  was  expecting  everyone  to  be  on their  feet.  Thank  you.  Thank  you. Thank  you.  Yeah.  Okay. Look,  this  is  maybe  a  little  bit  linear  algebra. It's  not  like  the  most  exciting  thing, but  understanding  this  is machines  are  able  to  do  deep  learning  really  quickly. This  is  why  deep  learning  has beaten  everything  else.  It's  because  of  this. Let  me  motivate  this  a  little  bit  first  and  we'll  talk about  the  details  of  the  motivation  in  a  little  bit. The  motivation  is  that  there  are  these  things called  graphics  processing  units,  GPUs. What  do  GPU  do? Gp  do,  matrix  multiplication. They  were  invented  for  playing  video  games. For  rendering  graphics  and video  games  that  was  like  in  the  late  '90s. Then  ten  years  later,  in  2009, a  few  different  groups  realized that  the  special  chips  that  you  buy, they're  very  expensive  and  you  put  them  in  your  computer and  they  do  matrix  multiplication  really  quickly. What  people  realized  is  that everything  in  deep  learning is  just  matrix  multiplication. If  you  put  it  onto  a  GPU,  it  runs  really  quickly. You  want  to  turn  all  of your  deep  learning  operations into  matrix  multiplications. What  we're  doing  right  now  is we're  turning  a  bunch  of  different, a  bunch  of  isolated  matrix  multiplications  in the  unbathed  case  into a  single  matrix  multiplication  in  the  batched  case. Whenever  we  can  do  that  almost  always  in  deep  learning, that's  a  good  thing,  turning many  matrix  multiplications  into  a  single  one. Because  GPUs  are  so fast  with  doing  matrix  multiplication, Do  a  large  matrix  multiplication  is  usually  much, much  faster  than  doing  a  bunch  of tiny  individual  matrix  multiplications. So  we're  doing  this  batching  so  that  we can  just  collect  everything up  into  a  single  matrix  multiplication, that's  our  goal.  Okay. Any  questions  about  that?  Is  that  question? No.  Okay,  Anything?  No. Okay,  so  we've  collected up  all  of  the  examples  into  this  matrix. We're  multiplying  them  all  at once  by  this  other  matrix,  W  one. We  collect  up  the  answers  into  A. Now  we  have  a  new  A  that's collected  all  of  these  things  up. What  do  we  do  with  that? Well,  we  define  a  matrix  h.  What  is  h? H  is  equal  to  relu  of  A. Next  we  have  another  matrix  multiplication. We  have  capital,  okay? Let's  actually  keep  track  of  our  dimensions. Here  we  are  the  dimensions  of  He, four  K  by  n,  okay? So  this  is  four  K  by  N. Our  different  features  are  in  the  rows. Each  of  the  independent  examples  are  in  the  columns. In  this  case.  What's  this  equal  to? It's  going  to  be  two times  h.  We  don't have  to  take  the  transpose  in  this  case. Then  finally  we're,  we  can  do the  soft  soft  max is  actually  a  little  f.  Let's talk  about  that  for  a  second. We  have  no  what  is  this? This  is  going  to  be  the  number  of  dimensions  by n  for  each  example  in  the  columns, we  have  all  of  its  weights. In  the  rows  is  the  number  of  classes. Probabilities  is  going  to  be  soft  applied  to  this  matrix. We  want  to  turn  this  matrix  into  probabilities, but  there's  one  wrinkle  there,  actually. It's  going  to  be  soft  of  o,  It's  not  a  zero. It's  what's  the  wrinkle? I  want  each  example to  be  turned  into  the  probabilities  of  the  classes. For  that  example,  I  have  examples. Yeah,  yeah,  exactly.  Don't  want  to  mix  them  up. When  I  do  soft  soft  max turns  a  bunch  of  numbers  into  probabilities,  right? But  I  don't  want  the  probabilities  from example  one  having  anything  to  do with  the  probabilities  for  example  two. They're  just  different  things. What  I  need  to  do  there  is specify  which  dimension  I'm  taking, the  soft  mas  al,  the  way that  you  would  normally  do  this  is  you  would  say which  dimension  that  you  just  say  m  is  equal  to  O. This  is  dimension. That's  what  people  usually  refer to  as  the  road  dimension. This  is  the  Pytorch  convention  for  this. Anyway,  the  zero  dimension  is  the  road  dimension. We're  going  to  be  taking  the  probabilities  there. Okay,  let's  write  out  the  Pytorch  code  for  this. In  general,  you  should  always  be  writing batched  code  or  else  your  code  will  run  really  slowly. How  do  we  write  the  batched  pie  torch  code?  W? One  is  exactly  the  same  as  before, one  that's  end  up  parameter  four  k  by K  x  and  now we'll  call  it  lay  bowls  instead  of lay  bowl  is  equal  to  get  samples, we'll  get  multiple  samples. Then  what's  the  dimensions  of  X,  X? The  convention  in  Pitorch, I  think  other  languages is  to  always  put  the  batch  dimension  first. It's  very  easy  to  get  confused  about  which  dimension  is which.  Let  this  be  a  warning. This  is  an  extremely  common  source of  mistakes  that  I  make  as  well. You  have  these  complicated  objects. They're  called  tensors,  right? They're  tensors  because  they  might  have  three, or  four,  or  five  different  dimensions  in  them. It  might  be  a  five  by  six  by  seven  by  eight  object. Remembering  where  are  the  samples, which  dimension  is  which,  Very  hard  to  do. The  convention  is  you  put  the  batch  dimension  first, so  this  is  going  to  be  N  by  K. We  have  examples  on  the  rows, features  on  the  columns. Now,  here's  the  advantage  of  Einsum,  which  is, remember  over  here  to  compute  A, I  had  to  do  a  transpose  on  x,  right? Let's  just  remember  why  that  is x  is  this  n  by  K  matrix  of  examples, one  is  four  K  by  K  weight  matrix. I  did  A  equals  one  by  times. I  couldn't  do  x  because  N  is  the  batch  dimension, is  the  example  dimension, so  I  had  to  take  the  transpose to  flip  it.  So  that's  by  N. If  I  use  einsum, I  don't  need  to  do  that  anymore. I  just  say  A  is  equal  to  torch  einsum. What's  my  Einsum  here? Yeah. Okay. Let's  try  it  so  I don't  wait. So  you  said  J,  I,  J, K.  Yeah,  let's  not  use K  because  I  have  I'm  using  K  already  here. So  let's  call  it something  different.  What  do  you  want  to  call  it? Great,  J,  L,  is  that  right? So  you  said, and  then  what  do  you  want  to  be? The  input  matrices  X and  one  or?  Okay.  I  see. So  you  want  to  put  x  first,  is  that  right? Okay,  so  actually,  I  don't  know  if  this  is  correct. Let's  see.  The  normal  convention is  to  put  the  weight  matrix  first,  but  that's  fine. I  mean,  let's  actually  see  what  this  does. Okay? So  okay, let's  see. I, no, I  think  this  cannot  be  correct  for  the  following  reason that  you're  calling  the  road  dimension  in  W, one  is  J,  right? So  I'm  looking,  I'm  just  looking  at  this  here. So  we  have  this  corresponds  to, this  is  the  first  term  here, so  it  corresponds  to  the  first  argument  here. That's  going  to  be  the  dimension in  the  J  here  is  going  to  be  the  road  dimension, x4k,  row  dimensions  in  one, but  there  are  row  dimensions  in  X. Those  two  things,  they  can't agree  with  each  other.  They  don't  line  up. Yeah, I  I  J, LJ,  LK.  Okay. Let's  take  a  look  at  this  one. So  the  way  I'm  going  to  understand this  is  by  writing  out  the  formula  that  this  implies. And  let's  see  if  it's  correct. So  what  am  I  saying? So  I'm,  I'm  creating  a. So  how  do  I  create  a  I'm  saying  the  LK  index  in  that. What's  that?  You  think  it's  I  L.  Okay,  let's  try  it. This  is  I  L. So  you  think  that  this  is  equal  to, I'm  summing  over  j  here,  correct? So  it's  going  to  equal  a  sum  over  j. The  reason  it's  summing  over  j,  how  do  I  know  that? By  the  way,  how  do  I  know  that  I'm summing  over  j  in  this  formula? Yeah,  J  is  missing  on  the  right  hand  side. Exactly.  Okay,  so  I'm  summing  over  j. So  it's  going  to  be  one  I  J, that  is  the  indices  here,  times  x  LJ. Okay,  let's  take  a  look,  what's  IJ? That's  basically  what  I'm doing  is  here  is  I'm  summing  over  the  columns, J  indicates  the  columns. I'm  summing  over  the  columns in  one,  that  seems  pretty  good. I  have  X  LJ  here, that's  where  the  example  sits. And  then  I'm  summing  over  all  of  the  features  there. This  is  pretty  good.  What  this  is saying  is  that  I  take  the  example. I  take  the  dot  product  of  that  with  the  I  throw  and  W. That's  exactly  what  I  want  to  do. It'll  take  a  little  bit  of  time  to  make  sure  this  is correct,  but  this  is  correct. It  takes  a  little  time  to  get  used  to  this. But  basically  what  this  meant  is  I  didn't  need to  worry  about  taking  a  transpose  on  x  in  this  case, maybe  it's  actually  not  so  clear  the  payoff  was  worth  it. But  I  promise  you  the  payoff  becomes  worth  it  once  x becomes  even  more  complicated or  becomes  more  complicated. Okay? And  the  other  nice  thing  about  it  is  this  code  here, the  batched  code  that  I  wrote, looks  nearly  identical  to the  unbatched  code  that  I  wrote. The  only  difference  is  that  I  changed what  indices  appear  in  the  sign  sum.  That's  it. That's  the  only  thing  that  changed. Okay,  then  I  have  h.  H  is  equal  to  torch  relu  of  a. I  have  two  is the  same  NN  parameter  and  then  it's  four. I  have  my  variable  that's  going  to  be  ins. Again,  here's  another  nice  thing  about  doing  it  this  way, which  is  the  ins  I  do  here  is  identical  to  the. So  I  did  up  here,  I  had  to  figure  it  out  once  I'm  in the  same  situation  now  because A  what  was  A  had  features  along  the  row, examples  along  the  columns, that's  exactly  the  same  thing  that  I  had  with  x  as  well. The  data  type  did  not  really  change. It's  going  to  be  exactly  the  same  formula  here. I  J  LJ  goes  to L  e  x  p is  equal  to  the  probabilities  are  equal  to  softmax  to. The  only  difference  here  is  I  have  to indicate  what  dimension  the  softmax  is  over. It's  going  to  be  di,  actually, what  dimension  is  it  over? Yeah,  it's  going  to  be  dimension  zero,  I  believe. Let's  make  sure  that's  true. O, no, something  went  slightly  wrong  here  with  my  indices. Let  me  make  sure  about  this. Here  was  the  issue.  This  should  have  been  L  I. Okay,  good.  Now  we want  to  keep  our  examples  along  the  rows.
