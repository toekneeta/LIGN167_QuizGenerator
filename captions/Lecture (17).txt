Okay,  so  I'd  like  to  show  everyone an  app  that  I  built  in  an  hour  last  night. Now  it's  not  such  a  great  app, but  it  was  built  in  an  hour. So  here  it  is.  Chatty  like  clothing  with  text  to  speech, and  well,  you'll  see. Let's say  hello  to  everyone. Hello,  everyone.  Okay,  that's  all  it  does. You  can  talk  to  it  some  more. But  what  this  means  is  that  we  now  have  text  to  speech, speech  to  text  on  a  website. Not  just  on  a  website,  but  in a  chat  interface.  How  did  I  build  this? I  took  a  little  bit  of  existing  code  that already  was  like  a  simple  chat  interface. I  just  found  like  someone  else  had  built  it. But  it's  just  a  few  lines  of  code,  not  very  much. I  copied  and  pasted  that  into GT  four  and  I  said  I'd  like  to extend  this  code  to  use  the  open  text  speech  API. That's  it.  It  gave  me  back  some  code, the  code  did  not  run. All  I  did  is  I  then  Googled  Stream's  the  library that  I'm  using  for  doing  the  chat. You  don't  have  to  use  streamlit,  by  the  way. I  happen  to  be  using  it.  Here  I  found  some  stuff from  the  streamlit  message  board  about  how  to  do  speech. Just  copied  it.  You  can  see like  one  month  later,  this  person  says  this. I  just  literally  went  to the  message  board  and  I  copied  it. I  don't  want  to  have  to  learn  how to  use  streamletsI  don't  care. I  don't  want  to  learn  about these  features.  I  just  copied  it. It  gave  me  some  more  code in  case  we  had  some  back and  forth  because  there  were  errors. Kept  telling  it  what  errors  I  saw. It  was  not  giving  me  the  full  code. I  asked  for  the  full  code. I  got  this  error. I  don't  even  know  what  this  error  is. It  helped  debug  this.  We  had  a  conversation. The  conversation  lasted  for  an  hour  or  so. By  the  end  of  the  conversation, I  had  a  working  chat. I  do  not  know  how  any  of  this  code  works.  I  have  no  clue. I  don't  know  what's  in  it  now, is  that  necessarily  the  smartest  thing? Do  you  want  to  be  running  random  code  on  your  computer? Maybe  not.  Let  me  not  officially  give  you  that  advice. But  if  you  want  to  get  something  done  really quickly  in  order  to  prove  a  point,  for  example, for  your  class,  then  you  can  do  this. What's  the  lesson  here? One  of  the  use  this  technology, it  will  speed  up  your  life. A  I'm  sure  most  of  you  are, but  you  may  not  be  using  it  as  aggressively. As  you  saw  there.  Like  I'm  really deferring  all  of  the  thinking  about  this  to  this  app. You  can't  do  that  for  everything, but  this  is  simple  enough, like  my  intuition  is. All  I'm  doing  is  calling  the  T  four  API, which  this  knows  all  about, using  Streamlet,  which  this  thing  knows  all  about. It's  nothing  like  too  complicated, it's  not  like  a  very  complicated  algorithms  question from  an  upper  division  class, it's  very  routine  stuff. Gp  four  knows  how  to  do it  when  it  does  not  know  how  to  do  it. It's  oftentimes  because  the  libraries  that  it  knows  about are  a  little  bit  out  of  date  when  that  happens, I  go  to  the  API  documentation  or  I go  to  a  message  board  and  I  just  copy  in  what that  documentation  says  and then  GPT  four  can  integrate  that  information. That's  one  reason  why  I'm  telling  you  this. You  should  do  this  very  aggressively.  Can  anyone  guess? The  other  reason  why  I'm  showing  you  this  right  now, I  have  a  more  nefarious  motive. Yeah. You  want  to  do  Exactly, No,  no,  no,  no,  no,  no. My  goal  with  your  final  project is  for  you  to  get  something  done. That's  good.  That's  the  only  thing  I  care  about. If  you  want  to  write  all  the  code yourself,  that's  totally  fine. So  I  don't  care  how  you  do  it. I  want  just  a  good  final  product. Anyone  else  want  to  take  a  guess? I  built  this  in  an  hour  without  knowing  anything. I've  not  looked  at  the  code  whatsoever. What  that  means  is  expectations  for your  final  project  will  be  fairly  high  if  it's  just  this. I  know  now  that  that  only  takes an  hour  from  a  non  expert. Many  of  you  know  much  more  about this  technology  than  I  do. I  will  have  high  expectations for  what  you're  going  to  produce. Any  questions. Okay. This  is a  little  bit  gross,  right? Like  someone  just  leaving  their  open  water  bottle. Like  some  UCSD  professor who  thinks  that  they're  like  above  the  law  or  something. It's  outrageous. Okay,  so  we're  talking  about  transformers. We're  talking  about  the  not  quite  transformer. We're  going  to  be  building  up  the  full  transformer in  stages  today.  We're  still  continuing  on. Something  that's  going  to  be  missing  a lot  of  important  details, but  we're  going  to  see  some  important  details  today. Transformers  consist  of  many  layers  iterated. You  have  your  word  sequence  12, a  sequence  of  word  vectors one  through  n.  This  is  your  text. How  long  is  these are  not  technically  words  by  the  way,  they're  tokens. We  haven't  quite  talked  about all  the  different  types  of  tokenizers  that  are  out  there. They  are  not  quite  words,  they're  approximately  words. How  big  is  smallest N  that  anybody  uses  in  practice  for  transformers, this  is  going  back  five  years, is  like  128,100.28  tokens, maybe  like  100  words  or  so. As  you  know,  the  longest  that  exists  is  128,000  in  GP, what's  used  actually  in  practice,  121024. If  you're  building  something  on  your own  and  you  don't  have  millions  of  dollars, 512  might  be  the  best  that  you would  use  in  practice  if  you're  building  your  own  system. I  just  want  to  give  people  a  sense of  the  scale  of  what  we're  talking  about. What's  512  tokens? Let's  say  it's  like  400  words  or  so. What  can  you  write  in  400  words? Yeah,  approximately  a  page. It's  like  an  essay.  One  page  of  an  essay. Okay.  We're  going  to  be  feeding these  tokens  into  a  sequence  of  layers. When  we  process  the  tokens  through  one  layer, we  take  the  results  and  we  feed  them  into  another  layer. When  we're  done  with  layer  two, we  feed  them  into  another  layer  until  we  get  to the  last  layer  of  the  network  I'm  calling  layer. How  big  is  K  in  practice? For  a  very  small  model, k  will  be  equal  to, let's  say  88  layer  transformer, is  the  shallowest  transformer that  someone  would  probably  use  in  practice. I  think  the  smallest  versions  of  burt  are  eight  layers. We'll  talk  about  maybe  next  class, or  in  two  classes,  largest  that  people  use. We  don't  know  what  the  K  is  for  GPT  four, but  I  would  say  the  is publicly  documented  is  somewhere  on  the  order  of  100. I  think  something  like  that. I  believe  GPT  three  is  around  100. You're  going  to  be  doing  iterative  processing. Each  layer  is  going  to  be extract  new  information  from  this  sequence  of  words. Layer  one,  extra  information, it  produces  new  word  representations. Layer  two  takes  the  represents  new  information  from  them. And  we  repeat  that  process until  we  get  to  the  final  layer. Then  the  final  layer,  produtput. Typically  we're  either  going to  be  doing  language  modeling, which  means  next  word  prediction. Or  we're  going  to  be  doing  some  sort  of  classification. Those  would  be  the  two  most common  types  of  output  that  we  want. Predictions  about  the  next  word, or  predictions  about  what  class  this  text  falls  into. Any  questions  about  this?  Okay,  let's look  at  this  in  a  little  bit  more  detail. We  feed  these  words  into  layer  one. After  layer  one  is  done  with  processing, it's  going  to  be  producing new  representations  for  each  word. If  we  have  vectors  going  into  layer  one, we  will  have  n  vectors  going  out. Let's  call  them  h  for  right  now, one  H  for  each  input  word. We  can  think  about  these  H's  as new  representations  of  each  of  the  input  words. Last  class,  we  started  talking about  what's  going  on  inside  of  layer  one. We  got  our  very  first  taste  of  attention. Attention  is  something  that happens  in  each  of  the  layers. We'll  learn  more  about  that  today. What's  the  size  of  these  new  vectors,  the  dimensionality? It's  exactly  the  same  as  the  input  words. Nothing  changes  in  terms  of  the  size  of  these  vectors. What  do  we  do  with  them? We  take  each  of  these  vectors and  we  feed  them  into  layer  two. Layer  two  is  going  to  produce  a  new  sequence  of  vectors. We  need  a  good  consistent  name  for  them. I'm  going  to  use  superscripts  to  indicate  that. I'm  going  to  use  superscript  one  over here  to  indicate  these  are the  hidden  states  for the  first  layer  that  are  output  by  the  first  layer. The  hidden  states  output  by  the  second  layer  will have  a  two  in  their  superscript. And  we  keep  repeating  this  process until  we  get  to  layer  K, where  we  get  an  output  prediction. Let's  now  look  at  what's  happening inside  of  these  layers. Inside  of  a  layer,  there  are  two  main  components. At  the  very  bottom  of  the  layer, we  do  attention,  I'll  put  that  inside  of  a  little  box. I'll  assume.  Let's  say  this  is  layer  one. What  we're  getting  in  are  the  input  words. The  same  thing  will  hold  for  the  layer  layers. Let's  focus  on  layer  one. For  right  now,  we  have  our  input  words  here  one. All  of  these  go  into  layer  one. Basically  the  first  thing  that  we're  going  to  do, I  say  basically  because  we're  not quite  learning  about  transformers  yet. In  our  simplified  model, the  first  thing  that  this  layer  does  is  attention. You  go  in,  you  do  attention  on  all  of  these  vectors. We'll  learn  more  about  that.  The  second  thing that  you're  going  to  do  is  run  all  of the  results  through  a  multi  layer  perceptron  layer. We  first  do  attention  on  all  of  the  vectors. We  take  the  results,  we  run  them  through  an  MLP. That's  what  layer  one  spits  out  at  the  end. So  there's  two  core  components  to  each  layer  attention. And  the  multi  layer  perceptron. Yes, let's  get  back  to  that. After  we  see  there's something  more  precise  that  we  can  say  about  that, we'll  go  back  and  actually  do  a  formal  analysis  of this  once  we've  gone  through  the  components. It's  a  great  question. You  had  a  question? Yeah, we'll  see  what the  role  of  attention,  they  each  play  a  role. And  I  mentioned  this  last  time, the  name  of  the  original  transformer  paper. Attention  is  all  you  need. So  that  would  suggest that  at  really  important  part  of  this  network, right,  that  says  it  in  the  name. That  was  definitely  the  view, I  would  say,  when  this  model  was  invented  in  2017. So  it's  not  very  long  ago,  right? It's  ancient  history  for me  and  also  for  you  because  you're  all  young. But  thinking  about  really  in  the  grand  scheme  of  things, it's  six  years  ago  that  this  was  invented. The  field  had  nothing  before  the  right  in  terms of  anything  that  you  could  use  for  analyzing  language. It  was  just  completely  worthless. All  the  progress  happened after  transformers  were  introduced. You  would  think  attention is  actually  the  important  part  of  this. I  would  say  that  the  current  view  about  this, and  we  have  some  evidence  that  maybe we'll  talk  about  later  in  the  course, is  that,  look, you  can't  just  get  rid  of  attention and  then  your  model  will  work, that's  actually  very  clear. But  you  can  replace  attention  with  a  lot  of  other  things. There  are  other  things  that  can  serve the  same  function  as  attention. Attention  does  not  look  very  special  at  this  point, but  it  doesn't  mean  that  if  you  remove  attention from  the  network  and  you  just  run  the  network, that  it  will  work.  That's  not  true. Okay.  Each  of  these  components  is  extremely  important. Like  given,  if  you fix  all  the  other  components  of  the  network, you  cannot  take  the  network, remove  one  component,  and  expect  it  to  work. If  you  want  to  remove  something  from  the  transformer, you  have  to  do  something  clever. This  is,  it's  a  bit,  it's  like  a  jewel. They  discovered  a  jewel  just  by  accident. Basically,  you  have  to  be  really, really  clever  to  change  the  components  of  it. It's  possible. Okay,  we  do  this, then  what  happens  in  layer  two? Layer  two  is  exactly  the  same  as  layer  one, except  that  the  weights inside  of  each  of  these  components  is  different. Each  layer  structurally  looks  identical. You  just  keep  doing  the  same  thing over  and  over  and  over  again. It  doesn't  mean  that  the  computations  done  are  the  same because  an  MLP  has  weights  in  its  attention. By  the  way,  we'll  learn  about  that, both  of  these  things  have  learned  weight  matrices, the  learned  weight  matrices  in  layer  one  are different  than  the  learned  weight  matrices  in  layer  two. So  this  is  the  high  level  architecture of  something  that's  approximately  a  transformer. You  keep  doing  the  same  thing over  and  over  and  over  again. By  the  way,  there  is an  interesting  biological  analogy here  for  the  cognitive  scientists  in  the  room. Are  you  familiar  with  this? There's  an  interesting  biological  analogy, the  cortex,  which  is  like. You  know,  what  people  normally  say is  where  intelligence  resides? That  human  intelligence  resides  in  the  cortex. The  human  cortex  is  the  main  thing  that's different  from  other  animals  that  are  not  as  smart  as  us. The  cortex  is  composed  of  a  bunch  of  layers, and  each  of  those  layers  looks  the  same. There's  no  differentiating  structure  across  the  layers. It  looks  like  you  have the  same  operations  being  performed  over  and  over  and over  again  in  the  cortex  before  you get  whatever  intelligent  behavior that  you're  looking  for. It's  an  interesting  analogy. It's  thought  provoking,  Is  it? Is  there  really  any  fundamental  connection to  what's  happening  here? I  would  say  I  have  no  clue. I  don't  think  anyone  has  an  idea. But  there  may  be  something  there. Okay,  so  let's  talk  about  attention. We  have  four  words.  Four  input  words. Should  we  do  four  or  three? Let's  do  43  is  a  little  too  simple. So  remember  last  time  what  we said  is  what  attention  does. We're  going  into  the  attention  layer  right  now. What  attention  does  is  it  computes  pair wise  comparisons  between  every  word. It  computes  the  similarity  of  words 1.1 So  we  compute  para, these  are  similarities  that  we're  computing. And  we  continue  on  in  this  way  for  all  of  the  words. What  I  want  to  do  right  now  is  look  at  how  we're  going  to compute  a  new  representation  for  word  one. We're  going  to  be  going  row  by  row. This  row,  the  first  row, contains  all  of  the  similarity  comparisons between  W  one  and  each  of  the  other  words. These  similarity  comparisons  produce  scalars. It's  going  to  be  some  number  between  negative  infinity and  infinity  that  measures  how  similar  you  are. So  what  do  we  do  with  this? Let's  go  back  to  the  formula  that we  learned  about  last  class. Let's  talk  about  one, the  first  probability  distribution  that  we  compute. We're  going  to  have  one  probability  distribution for  every  word  in  this  sequence. Let's  talk  about  the  first  one.  I  mean, the  formula  stays  basically  the  same  for  all  the  others. One  is  equal  to  soft  max  of  all  of  the  similarities, similarity  between  word  one  and  word  one  itself. Similarity  between  word  one  and  word  two. We  compare  word  one to  all  of  the  other  words  in  the  sentence. We  collect  them  up,  we  run  them  through  a  soft, what's  the  probability  we're  going  to  be  thinking about  this  as  a  probability  distribution  over  words. Another  way  of  saying  this  is  this  is the  probability  that  word, one  word,  one  attends,  let's  say, pays  attention  to  each  word. Yes,  great  question. There's  a  technical  reason  for it  that  we'll  get  to  in  a  little  bit. I  think  there's  a  more  interesting  reason, which  it  could  be that  none  of  the  other  words  are  relevant  at  this  point. Maybe  we're  at  a  higher  layer  in  the  network. We  already  know  this  word  has  already gotten  as  much  information  from the  rest  of  the  sequence  as  it  can. There's  no  other  relevant information  from  the  rest  of  the  sequence. It  needs  to  pay  attention  to  something, so  it'll  pay  attention  to  itself. Another  way  of  thinking  about  this  is  that  it's the  model  telling  it that  maybe  it's  not  done processing  all  of  the  information from  the  previous  layers. So  it  needs  to  do,  it  may  be  the  case that  information  from  the  rest of  the  sequence  will  become  relevant, but  we  need  more  processing  here  at  this  particular  word. Okay, So  what  does this  one  look  like? We  can  think  about  P  one. It's  a  sequence,  A  vector. What's  in  the  vector? Let's  call  it  111,213.1  411, 112,  let's  say,  is  that  one pays  attention  to  word  two. So  for  each  word, we're  getting  out  of  probability  distribution over  the  entire  sequence. Any  questions  about  this?  No.  Okay. The  new  representation  for word  one  is  equal  to, we're  going  to  call  it  a  vector,  a  one, which  is  a  weighted  sum  of  all  of the  other  words  in the  sequence  where  the  weights are  given  by  these  probabilities, it's  going  to  be  equal  one. In  this  case  it's  because  that's  our  sequence  length. Sum  from  I  equals  one  to  four  of  how  much? At  word  one  word  I, We  multiply  that  by  the  current  representation  Forward, we  take  all  of  the  words  in  the  sequence, we  weight  them  by  these  attention  probabilities. Attention  is  saying,  you  can  almost  think  about it  does  have  to  do  with  human  attention,  probably  not. But  there  is  definitely, I  think,  a  very  clear  analogy, which  is  when  you're  paying  attention  to  something, your  current  representation  of the  world  mostly  depends  on  that  thing. If  I'm  looking  at  the  green water  bottle  and  paying  attention  to  it, that's  what  I'm  thinking  about  right  now. My  attention  probability  is mostly  on  that  green  water  bottle. Same  thing.  Here  we  have  a  formula  for calculating  how  much  attention  probability should  be  on  each  of  the  words  in  the  sequence. Each  word  in  the  sequence  gets  weighted  up  or down  basically  by  that  attention  probability. Any  questions? Did  anybody see  the  movie  Oppenheimer? Yeah.  Oh,  people  saw  it.  It's  good. It's  worth  seeing.  Yeah. It's  really,  especially  after  taking  this  class, I  think  it's  worth  seeing. I  thought  would  spend  way  too  much  time on  the  trial  stuff. I  thought  that  was  really,  really  boring. It  was  like  an  hour  and  a  half of  I  don't  know what  they  were  thinking  when  they  were  doing  that, but  I  like  the  scenes  in  Los  Alamos. They  really  should  have  spent  more  time  like, what  was  it  actually  like  building  this  bomb? It  was  like  the  biggest  thing  that  anybody  had  ever, the  biggest  organizational  task that  anyone  had  ever  done. Like,  what  was  involved  there? They,  they  could've  done  it,  they  didn't  really  do  it. But  like,  you  know,  even  when  you  got glimpses  of  it,  I  thought  that  was  pretty  cool. Okay,  so  what's  the  main  there? You  know,  there  probably  are  multiple  questions that  you  may  have  about  this, but  there's  one  main  question  here  that  I've left  completely  open  similarity. What's  the  similarity  function? How  do  we  compute  the  similarity  between  pairs  of  words? And  this  requires  some  thought. So  can  someone  give  me  some  ideas  for  what  we  can  do? Yeah,  in  the  first  place, you  do  something  with  the  distance  h. Okay,  Give  me,  give  me  an  idea. The  vectors,  what  can  I  do  with  two  big  vectors? That's  not  going  to  be  similarity,  right? Let's  say  that  here's  a  constraint  that  I  have, which  is  I  want  vector,  I  want  to  be possible  for  vectors  to  be  more  similar  or  less  similar. I  like  those  things  to  be  both positive  and  negative  numbers. You're  on  the  right  track.  What  can  I  do? Yeah,  Okay,  great. Cosine  similarity  and  dot  product. They're  very  closely  related  to  each  other. In  fact,  because  of some  other  details  I'm  not  talking  about  right  now, they  almost  turn  out  to  be the  same  thing  in  this  situation. Let's  talk  about  dot  product that  we'll  be  more on  the  right  track  if  we  think  about  that. Okay,  let's  take  the  dot  product, we'll  call  it  proposal  one. Similarity  between  word  one  and word  two  or  between  any  pair  of  words. It's  equal  to  the  dot  product between  word  one  and  word  two. When  you  take  the  dot  product, if  that  thing  is  big,  that means  they  really  point  in  the  same  direction. If  it's  very  negative, that  means  they  point  in  opposite  directions. If  it's  close  to  zero,  it  means that  they're  approximately  orthogonal. Okay,  so  we  could  do  that. Let's,  so  here's  our  new  attention  matrix. Let's  do,  okay,  Here's  our  new  attention  matrix. Bigger  numbers  here,  we're  doing  a  soft  max  on  this. So  we  take  row  one  to get  the  attention  distribution  for  word  one, we  look  at  row  one,  we  do  a  soft  max,  right? What  do  we,  we've  seen  softmax  a  bunch  of  times. Remember  softmax  means  what  you  do  is  it's like  let's  say  11 over  a  sum  from  I  equals  one. I'm  saying  what's  the  probability, what's  the  attention  probability  for  the  first  word? Sum  from  I  equals  one  to  four  of  the  one  I. Now  let's  assume  for  the  moment that  all  of  the  vectors have  basically  the  same  magnitude, because  that  basically  ends  up  being  true. These  input  vectors  are  all  the  same  size. What's  the  consequence  of  that? For  this  attention  distribution  that  we're  computing? What  can  we  say  about  the  Softmax  in  general? Someone  gives  me  these  products. What  are  some  things  that  I  can  say? There  are  some  very  general  things and  then  there  are  some  things I  can  say  in  this  situation. Anyone  want  to  what  do  we  know  about  softmaxs  in  general? Just  someone  tell  me. If  this,  then  this,  yeah,  it gives  us  the  probability  of  a  bunch  of  events. That's  right.  But  what  else? I  mean  from  this  formula,  what  do  we  know? Yeah,  they're  all  normalized,  they  should  add  to  one. So  it  gives  us  back  a  probability  distribution  there. They're  all  positive,  right? All  the  numbers  that  we  get  out  are  positive. I  mean,  some  up  to  one.  What  else  do  we  know? Yeah,  it  is  differentiable.  That's  definitely  true. Okay. I  want  you  to  think  about,  you should  work  on  this  on  paper  for  a  second. Assuming  that  these  vectors  are  all approximately  the  same  size,  what  do  we  know? Let's  say  the  magnitude  of  the  vectors  are  all  one. Let's  just  say  that  for  simplicity, they  all  lie  along  a  sphere,  right? The  product,  higher  probability. Okay,  that's  absolutely  true. The  dot  product  will  be  higher if  they're  pointing  more  in  the  same  direction. A  higher  dot  product  means  a  higher  probability. The  Softmax  is  monotonic. If  you  give  it  a  bigger  number, the  output  is  a  bigger  number,  okay? Things  with  higher  products  will  have higher  probabilities,  more  attention  probability. What's  the  consequence  of  that?  For  this? Yeah. Yeah,  The  elements  along  the diagonal  will  get  the  most  attention because  you  can  be assuming  that  they're  all approximately  the  same  magnitude. The  best  way  to  get  a  product  is by  the  way  to  get the  highest  product  is  by taking  a  dot  product  with  yourself. What  we're  saying  is  that  the  things along  the  diagonal  will have  the  highest  probability,  22. That  means  every  word  will pay  the  most  attention  to  itself. Now  here's  another  fact. When  you  get  into  higher  dimensions, if  you  take  two  random  vectors, their  product  is  very close  to  zero  between  two  random  vectors. As  a  result  of  that,  you  go  to  be, each  word  will  mostly  pay  attention  to  itself. That  defeats  the  purpose  of  attention. We  want  the  words  to  learn  from  each  other. That's  one  problem  with  a  set  up  here, are  there  any  other  problems? Yeah,  there's  a  related  problem, but  it's  a  little  bit  more  solved  than  that. Yeah, Well,  I  mean,  that's  absolutely  right. I  think  that's  a  good  way  of  stating  the  problem, which  is  that  words  will  be mostly  paying  attention  to themselves  and  so  you're  not  to  be, the  words  will  not  be  getting  the  information that  they  need  from  the  rest  of  the  sequence. Good,  Yeah,  exactly. If  it's  dot  products, then  this  attention  matrix  is  symmetric. That's  a  problem. Why  is  it  a  problem?  Three  is  the  same  as  31. That's  it's  symmetric. If  you  flip  it  over  the  diagonal,  it'll  be  the  same. Why  is  that  a  problem?  We  want each  word  to  pay  the  same  amount  of  one  word. I  pay  a  certain  amount  of  attention  to  another  word. Why  we  want  the  other  word to  pay  as  much  attention  to  me? Can  someone  think  of  any  examples where  that  might  not  be  so  good? Yeah. Okay,  Give  me  an  example. Huh?  So  the  boy went  to  the  park.  Okay. So  I  think  there's a  decent  intuition  that  you  may  not want  boy  to  pay  as  much  attention  to  the, as  the  pays  attention  to  boy  or  vice  versa. Let's,  let's  see. I  can  make  up  a  story  about  that  that  I think  seems,  seems  reasonable. So  it's  not  so clear  to  me  actually,  that  there's  going  to  be. There  probably  is  in  practice, but  I  don't  immediately  see  what  the  asymmetry  is  here. There  are  sort  of  related  examples that  we  can  think  about, but  I  think  you're  on  the  right  track  and  think about  a  concrete  example  where  there's an  asymmetry  where  one  thing  does  not  need to  pay  as  much  attention  to  another  thing  as  vice  versa. Can  anyone  think  of  some  examples  here? Think  about,  about  language  is  ambiguous. The  model  is  going  to  be  trying  to  figure  out the  ambiguities  in  a  sentence. What  can  we  do?  Where  will  this  arise  there? We're  doing  a  little  bit  of  linguistics  right  now. We're  thinking  like  linguists  y, Okay,  this  is  a  little  bit more  grim  than  what  I  was  expecting, but  we'll  go  with  it. I  ate  grandma  versus  I. Okay,  so  where  is the  asymmetry  here? Right? Okay,  so  let's  say that  we  can  think  about  eight as  needing  to  get  disambiguated  because  eight can  either  be  an  intransitive  verb  that, that  does  not  take  another  argument. Or  could  be  a  transitive  verb  that  does  another  argument. Grandma  could  be  an  argument  of  eight  or  it  may  not be  depending  on  whether  grandma  in  fact. Yeah. So  the  way  that eight  will  figure  out  how  to  resolve this  ambiguity  will  be  by  looking at  whether  there's  a  right  next  to  it. Okay,  there  is,  it  will  pay  attention  to  that. Great.  It  now  knows  it's  not  doing, we're  not  specifying  what's  being  eaten. Okay. Pay  attention  to  eight,  maybe you  want  the  comma  to  pay  equal  pi, should  pay  equal  attention  to  the  things  on  both  sides. Uh,  so  you  don't necessarily  want  there  to  be  something  symmetric  there. Okay?  I  think  this  is  getting  closer. So  the examples  that I  have  in  mind  are  things like I  went  swimming. With  at  the  bank. What  type  of  bank  am  I  talking  about  here? Not  the  financial  institution talking  about  a  river  bank  or  something. Bank  will  figure  that  out by  looking  at  the  word  swimming. There's  an  ambiguity  here. What  does  bank?  The  way  that  bank  will  figure out  what  it  means  is  by  looking at  all  the  other  words  in  the  sentence, seeing,  hey,  I  occur  with  the  word  swimming, I  should  pay  attention  to  this. That's  going  to  tell  me  what  type  of  bank  I  am. The  word  swimming  on  the  other  hand, it  can  pay  a  little  bit  of  attention  to  the  word  bank. That  specifies  where  the  location of  the  swimming  happened. But  it's  not  the  only  thing  that  should  pay  attention to  the  word  swimming. Should  also  pay  attention  to  who's  doing  the  swimming. Who  went  swimming  with  who? Yeah.  Then  in  addition  to  that,  where  did  it  happen? The  word  swimming  has  a  bunch  of  stuff  to pay  attention  to  to  figure  out  what  it  means. What  type  of  swimming  was  it? The  word  bank  really  has  one  thing  to  figure  out. This  is  the  reason  why  we want  attention  to  be  asymmetric. Certainly  we  don't  want  to  tell the  model  you  have  to  assign. It  can  happen.  Maybe  two  words pay  equal  attention  to  each  other.  That's  totally  fine. If  it  happens,  we  don't want  to  force  the  model  to  do  that. Maybe  that's  the  true  deep  learning  reason  why  we are  going  to  get  rid  of  this  do product  and  make  attention  nonsymmetric. Because  we  want  to  tell  the  model  as  little  as possible  about  how  it should  do  the  thing  that  we  wanted  to  do. We  want  to  give  it  as  much  flexibility  to learn  the  correct  attention  distributions. How  can  we  do  that?  So  this was  proposal  two.  Proposal  one. Now  we're  going  to  see  proposal  two, which  is  a  way  of  eliminating  both  problems. Making  sure  that  words  don't necessarily  pay  the  most  attention  to  themselves. That  is,  things  along  the  diagonal at  the  highest  similarity  scores. And  making  sure  that  the  attention  weights are  not  symmetric  in  this  matrix. We're  going  to  solve  both  of  those  issues with  using  a  single  technique  is  called  Q  KV. Attention, this  is  key and  value,  that's  what  these  stand  for. We  can  think  of  query  there  are  going  to  be  three  types of  things  that  will appear  when  we're  doing  our  attention  computations. These  are  three  types  of objects  that,  that  we're  going  to  be  using. And  we  can  think about  them  as  each  answering a  different  type  of  question  or each  asking  a  different  type  of  question. Okay,  the  query  represents, what  type  of  information  am  I  looking  for? The  key  represents,  do I  have  the  information  that  you're  looking  for? The  value  represents  what  information  I  have. So  we're  going  to  be  breaking  the  problem into  these  three  parts. Each  word  in  the  sequence  will  have  a, it  will  have  a  key,  and  we'll  have  a  value. When  you  think  about  these  are  tags. We're  putting  on  each  word, the  tags  basically  answer  these  questions. If  I'm  word  one  query  tag  says, here's  the  information  that  I'm  looking  for. My  key  tag  will  tell other  words  whether  I have  the  information  that  they  are  looking  for. My  value  tag  will  actually contain  the  information  that  I'm going  to  be  sharing  with  the  other  words, any  questions  about  this  so  far? I  have  a  question  for  everyone. What  are  your  thoughts  on  virtual  friends  and virtual  know  how  to  put  it  otherwise  than  companions. Romantic  partners  is  like a  normal  idea  to  people.  Is  this  like? Okay,  no,  no,  not  like  humans. I'm  not  going  to  make  a  judgment  about whether  they  have  minds  or  not, but  not  human  minds. Yeah. Interpret  it  broadly.  I  don't  know. Does  it  seem  like  a  good  thing  for the  world  to  have  more  and  more  of  these  things? I  don't  think  so  because relationships  to  serve  that  person. Yeah,  that's  the  relationship. So  that  it's  an  interesting  thought. I  think  the  reality  might  be  even  more  grim, which  is  that  the  primary  purpose  of your  virtual  friend,  or  whatever  it  is, is  actually  to  serve  Facebook  and  extract  money  from  you. In  some  way  is  true. There  will  be  open  source  virtual  friends which  are  more  along  the  lines  of  what  you're  describing. Yeah,  like  a  alongside  what  they  said. I've  also  seen  like  it  happened  a  lot  of these  companies  where  you're  like  their  profit  focused. Yeah. They'll  often  shut  down.  Less  profitable. Yeah. Ai  girlfriend  or  whatever. Yeah,  friends.  And  then  like I've  seen  like  posts  from  like people  like  morning  costs  of  their  girlfriend, like  the  servers  shutdown  and  then  like hopelessly  trying  to  recreate  them  on  some  other  service. That  sounds  like  most  of  these  are  there.  Yeah,  right. But  there  will  be open  source  versions  of  these  assuming  that the  open  source  movement  in large  language  models  and  similar  technology  continues. Right  now  it's  thriving, assuming  it's  not  made  illegal. There  are  going  to  be  lot  and  lots  of open  source  models  that  will  do  this. Maybe  not  as  good  as  Facebook. There  will  have  some  inferior  features, but  they'll  be  pretty  good. My  prediction  a  few  years  ago  was  that  this  was  going to  be  the  biggest  application  of language  models  when  they  started  to  work. I'm  not  sure  whether  this  is  quite  true  yet. It's  like  it  may  not  end  up  being  true. There  are  a  lot  of  good  business  cases  as  well. It  is  disturbing  to  think  about  that. This  is  one  of  the  90%  of  people, some  very  large  fraction  of  people. This  will  be  like  their main  interaction  with  language  models. Maybe  that's  a  high  number,  but  it's  going  to  be a  disturbingly  large  number  of  people,  I  think. Yeah.  No. As  long  as  if  it's behaviorally  indistinguishable  from  a  human. Look,  I  I  don't  know  if  you're  conscious,  right? I  talked  to  you.  You  say some  stuff,  you  know,  you  have  response. So  yeah,  I  don't  know. It's  going  to  be  very  easy. Maybe  these  models  will  be  conscious.  I  don't  know. I,  I  don't  know  how  far  off  we  are  from  that. And  it's  going  to be  very  easy  for  people  if  that's  important  to  them. It's  going  to  be  very  easy  for  people  to  trick  themselves into  believing  that  these  models  are  conscious. Yeah. Situation.  Yeah. I  feel  like,  Yeah, I  know  it  be  be  it's  going  to  be almost  like  an  irresistible  experience because  it's  going  to  be  like,  you  know,  a  friend. That's  perfectly  tailored  to  you, whether  it  cares  about  you,  who  knows, but  like  it's  going  to  seem  like  it  is  tailored  to  you. It's  going  to  tell  you  exactly  what  you  want  to  hear. That's  bad  for  many  reasons. Yeah.  Thank  you. Now,  access  to. Yeah,  yeah. Yeah,  that's  true. That's  a  disturbing  thought.  That's  true,  Yeah. So  I  wonder  a  lot  area  because  like  for  example, as  I  have  OC  and  as  such  I  situation getting  really  bad  about  like  Yeah,  it's  reasonable. I  do  recently  is  I've  been  asking  GPT  to  go. Should  I  be  worried  about  this? I  have  OCD  that  Plaid  mechanic, and  then  Ott,  Very  helpful. Genuine.  But  then  I  worry  what  I  tell  O. Yeah,  there's  a  formal  term  for  this  in  the  field, there's  two  related  concepts. One  of  them  is  sycophancy  being  a  sycophant, and  then  the  other  one  is  reward  model  over  optimization. Look,  this  corresponds  to a  very  hard  to  solve  technical  problem. Which  is  basically  the  model telling  you  what  you  want  to  hear  rather than  like  what  you  think  you want  to  hear  rather  than  what  you  actually  want  to  hear. You  have  a  question? Yeah,  yeah. Uh  huh. I  mean,  you  can  still  say,  yeah, I  think  this  has  to do  with  the  personality  that  they've given  to  GPT  four  or  chat  GPT, You  can  prompt  it  better  and  it won't  act  like  that  anymore. So  that's  first  of  all,  I think  it's  a  failure  of  prompting. Like  you  have  to  give  it  very  detailed  instructions  about the  type  of  friend  that  you want  and  then  it  will  be  that  friend. Maybe  that's  not  so effective  if  you're  the  one  saying  it, but  other  people  will  just  create  wrappers around  that  with  better  prompts and  then  it  will  be  your  friend. The  other  thing  that  some of  it  has  to  do  with  the  post  training  that  it  did, called  RLHF,  reinforcement  learning  from  human  feedback. It  was  optimized  for  personality. You  can  easily  optimize these  models  that  have different  personalities  that  are  not as  cold  or  whatever  it is  that  you're  picking  up  on,  which  is  real. Those  models  have  already  been  trained. They're  not  quite  as  good  as  P  four. They  will  be  soon,  they're  going  to  be  even  better  than GT  four  in  the  sense  of  smarter. And  they  will  act  like  real  friends. This  is  Facebook's  business  model from  this  point  on,  right? It's  to  suck  you  into their  website  by  providing  virtual  friends  for you  where  it  knows  a  lot  about  what  type of  virtual  friend  that  you  want based  on  your  real  friends. Uh,  they  do  not want  to  provide  a  GP  four  like  experience. They  want  something  that  acts  like the  friend  that  you  want  or  that  you  think  that  you  want. Okay,  so  let's  learn  how  we  can  make this  terrible  piece  of  technology  right  now. We're  going  to  introduce  three  new  matrices, The  query  matrix,  the  key  matrix, and  the  value  matrix  V. Now,  each  layer  in  the  neural  network  in the  transformer  will  have its  own  triplet  of  Q  KV  matrices. These  are  going  to  be  learned  matrices. We  learn  them  the  same  way  that  we  learn every  other  set  of  parameters  in  our  model, which  is  through  back  propagation. All  of  these  three  matrices will  be  optimized  through  back  propagation. We  have  separate  matrices  for  each  layer. The  total  number  of  layers  matrices, different  ones,  number  of  layers, K  matrices,  number  of  layers  matrices. Okay,  we  have  these  three  matrices  for  each  layer. What  do  we  do  with  them?  Let's  talk about  what  we  do  with  word  one. It's  going  to  be  the  same. We'll  talk  a  little  bit  more  generally. Actually,  for  each  word  I, we're  going  to  compute  three  quantities. The  value  vectors, we're  going  to  compute  three  vectors  for  each  word. Lower  case,  that's  going  to  be  the  query  vector  for  word is  the  Matt  times  word  I. The  key  vector  for  word  I  is  equal to  the  K  matrix  times  word  I. Finally,  the  value  vector  for  word I  is  equal  to  the  value  matrix  times  word  I. We  have  three  new  vectors that  we  computed  for  every  word. These  three  vectors  represent  these  three  quantities  for that  word  is  going  to say  what  type  of  information  is  word  looking  for. I  will  represent  what  type  of  information  word  has, the  information  that  other  words  are  looking  for, V.  I  will  represent  the  information  that  word  has. What  do  we  do  with  all  of this? The  first  row. The  first  row  represents  word  one, looking  for  information  from  the  other  words,  right? The  first  row  is  going  to  be the  similarity  scores  between word  one  and  all  the  other  words  in  the  sequence. Word  one  is  looking  for  something. Here  it's  looking  for  what  should  they  pay  attention  to? That's  what  the  first  row  represents. We're  going  to  use  one, the  query,  which  is telling  us  what  word  one  is  looking  for. It's  going  to  be  one. And  we're  going  to  take  the  dot  product  of one  with  K  one, which  is  word  one telling  us  whether  it has  the  information  that  we're  looking  for. This  dot  product  tells  us the  Q  is  what  information  is  word  one  looking  for? The  K  is,  does  word  one  contain  that  information? The  product  says,  Are those  things  compatible  with  each  other? Do  we  have  a  match?  If  it's  high, then  word  one  has the  information  that  word  one  is  looking  for. If  it's  low,  it  doesn't  for  this  one. How  much  attention  should  word  one  pay  to  word  two? We  have,  what  information  is  word  one  looking  for? We  take  the  dot  product  with  two  which  will  tell  us, does  word  two  have the  information  that  word  one  is  looking  for? The  dot  product  is the  information  that  word  one  is  looking  for, similar  to  the  information  that  word  two has  then 1314  altogether, this  says,  I  have  some  information, I'm  looking  for  11  stays  the  same  across  this  row. Which  of  these  words  do  I have  the  highest  dot  product  with? That  indicates  to  me  which  of these  words  has  the  information  I'm  looking  for. Okay,  here  on  the  second  row, we  have  22  represents the  information  that  word  two  is  looking  for. This  row  will  tell  us  which  word has  the  information  that  word  two  is  looking  for. We  do  two  times  K  one. Does  word  one  have  that  information? Two  times  two,  Does  word  two  have  it? Two  times,  32, times  four  every  row, there's  some  information  that  that  word  is  looking for  that's  represented  by  the  query  vector. The  key  vectors  tell  us whether  those  words  have  the  information  that's being  sought  after  any  questions  about  this. Okay,  so  then  one, the  probability  distribution  for the  first  word  for  the  first  row, that's  equal  now  to  Softmax  of  the  first  row, so  we'll  write  it  out, 1112,  131f. This  tells  us  what  probability  will the  first  word  assigned  to all  the  other  words  in  the  sentence? Now  we  have  our  new  representation for  the  first  word,  one. And  there's  a  twist,  which  is  that instead  of  a  one  previously,  do  I  have  it  on  the  board? Yes,  up  there.  One  is a  sum  over  all  of  the  words  in  the  sentence. Correct.  There's  a  twist  here, which  is  that  instead  of  summing over  all  of  the  words  in  the  sentence, we're  going  to  sum  over  all  of the  values  in  the  sentence. All  the  value  vectors  one  is  a  sum. It's  the  same  formula  as  that  I  equals  one  to  four. The  probability  that  word  on  e  assigns  to each  word  P  one  times V.  I  is  the  information  that  word  has  to  contribute  to that  word difference between  I  and  I. That's  it. It's  a  transfer.  Think  about  it  this  way. What  does  multiplying  by a  matrix  do  to  a  vector  in  general? It  does  two  main  things. It  rotates  the  vector  around  and  it stretches  out  some  dimensions  and  collapses  others. One  way  to  think  about  it is  word  contains  all  of  the  information. There's  no  new  information  being  introduced. The  what's  being  done  by these  matrices  is  highlighting certain  information  from  the  existing  word  vectors. We  take  the  word  vectors. Some  information  that the  word  vectors  have  will  be  useful  for determining  what  information  is  that  vector  looking  for. Some  other  information  from the  word  vector  will  be  useful  for  computing what  information  it  has  that  others  are  looking  for. The  other,  what  information it  has  to  actually  contribute  to  the  new  representations. Yeah,  Yeah,  how  do  the  structure. That's  great,  that's  a  fantastic  question. And  that's  the  magic  of  back  propagation. We  have  a  loss  function, it's  very  complicated  system  that  we're  building. We're  not  saying  exactly  how  you  should  work, we're  just  saying  which  matrices  should  multiply, which  other  matrices,  right?  That's  the  only  thing. We're  saying,  the  actual  contents  of  the  matrices, the  numbers  inside  those  are  all  going  to  be  learned. We  have  a  loss  function,  we're  saying we  want  you  to  do  next  word  prediction. Well,  we're  then  telling the  model  matrices  V  K  that  do  the  job  well, however  you  want  to  do  it. This  look,  you're  touching  on  the  magic. It  seems  like  there's  all  sorts  of  ways  it  could  fail. They  don't  coordinate  with  each  other,  right? Like  they're  doing  incompatible  things. One  thing's  highlighting  this,  one  thing's highlighting  that  no  one's  talking  to  each  other. Well,  it  seems  like  it could  end  up  in  bad  situations  like  that,  and  it  doesn't. I  don't  know  why  that's  the  entire  Yeah. Yeah.  What's  that? But  this  number  may  be  very high  or  low  for  different  values. So  if  I'm  word  one  and word  four  is  completely  irrelevant  for  me, then  the  probability  that  I assigned  to  word  four  will  be  very  low. It's  about  yes.  These  probabilities  are  crucial  part. This  is  the  attention,  the  probabilities  are upweighting  and  downweighting  different  values. So  it's  saying  which  information from  the  sentence  is  important  for  me. Any  other  questions?  All  right,  let's  end  here. I'll  see  everyone  next  week.
