All  right. All  right,  let's  get  started. I  have  an  announcement  that  will  make  people  very  happy. There  will  be  no  problem  set five  and  everyone  will  get  an  automatic  100  on  it. Now,  why  am  I  being  so  nice  to  you? There's  a  catch,  which is  you  should  spend  this  time  on  your  final  project. My  expectations  for  your  final  project  will go  up  by  the  right  amount. Any  questions  about  the  final  project  before  we  start? No.  Okay,  so  we're  going to  continue  talking  about Transformers  today  and  how the  transformer  architecture  works. We  have  a  sequence  of  words. We've  transformed  all  of  the  words  into  word  vectors. We  pass  them  through a  sequence  of  transformer  layers. Where  is  the  transformer  layer  consists  of? Each  transformer  layer  consists  mainly  of  two  parts. We  pass  our  sequence  of  words  through  one  layer, then  through  another  identically  structured  layer. We  do  this  k  times  until  we  get to  the  final  output  layer  where  we're going  to  do  some  stuff  with it  like  predict  the  next  word, or  just  a  classification, a  sequence  level  classification. Like  what  type  of  document  is  this? We'll  talk  more  about  what happens  after  we're  done  with  the  final  layer. But  what  we're  going  to  be  focusing  on  to  begin with  today  is  what's happening  inside  of  each  individual  layer. Now  we've  covered  attention. We  spent  1.5,  I  think, lectures  talking  about  how  attention  works. What  attention  says  is  that each  word  becomes  a  combination. All  of  the  other  words  in  the  sentence. The  most  important  thing  about  attention  is  that  it mixes  words  together  after  you  are  done  with  attention. Each  word  representation  is  now a  mixture  of  all  of  the  other  word  representations. We  define  mathematically  how  that  happens. Let's  look,  this  is  Gross, Who's  leaving  the  dosing? If  it  was  a  smaller  water  bottle  that someone  had  left  here,  that  would  be  okay. I  don't  know  how  long  was that  person  carrying  that  thing  around  for? Okay,  let's  ignore  that. All  right,  let's  look  inside  of a  single  layer  of the  transformer  and  what's  happening  there. What  is  the  single  layer  say? You  have  attention,  we  have  our  words  going  in. We  have  n  words  going  in. That's  the  full  length  of  our  sequence  n  vectors. We  have  n  vectors  coming  out. We've  learned  about  how  we  compute  each  of  those  vectors. Let's  new  representations,  we'll  call  them  as, as  output  from  attention. We're  then  going  to  feed  these  vectors  into  an  MLP  layer. Again,  this  is  actually,  we're  still  in the  not  quite  transformer  stage  of  things. I'm  teaching  you  right  now. We  actually,  we're  going  to build  up  to  the  full  model  today, but  I'm  going  to  leave  out  a  few  details  for  right  now. Okay,  we  take  all  of  these  vectors, we  feed  them  through  the  MLP  layer  multilayer  percept. We've  learned  a  lot  about  multilayer  perceptrons. What  does  this  mean?  This  is  a little  bit  ambiguous,  right? How  exactly  do  this  MLP  layer  process  these  vectors? Let's  first  define  what  the  MLP  layer  is. Because  the  MLP  layer  in transformers  has  a  specific  structure, the  MLP  layer  says, we'll  call  them,  let's  just  start  with  the  output  vector. One  is  going  to  correspond  to the  first  input  vectors  equal  to,  it's  equal  to, so  this  is  the  full  formula. This  is  the  full  MLP  layer. It's  this  little  equation  here. What  we're  saying  is  the  MLP  layer  takes  in  vector  one, the  vector  corresponding  to  the  first  word. We  multiply  one  by  a  weight  matrix. Here,  I'm  calling  it  MLP  one. It's  just  a  normal  matrix,  vector  multiplication. We  take  a  relu,  we apply  a  non  linearity  to  the  result  of  that. This  is  a  vector.  Now  we  multiply  that  by  MLP  two, the  second  MLP  weight  matrix. That's  it,  that's  the  MLP  layer. The  general  formula  is  that  I  just  to  be  clear, after  we  apply  the  MLP  layer, we  get  out  a  sequence  of  vectors  12  through  HN  one. For  every  word  in  the  sequence, HI  is  just  equal  to MLP  times MLP  one  I. That's  the  general  formula  here. What  do  we  notice  about  what  the  MLP  is  doing? The  MLP  is  processing  the  attention  layer. And  the  MLP  layer,  they  take  in  a  sequence  of  n  vectors, But  there's  a  lot of  differences  between  these  two  layers, the  attention  layer  and  the  MLP  layer. But  there's  one  big  difference  between  them. What's  the  big  difference? If  you  get  the  answer  to  this, we  can  talk  about  some  spicy  stuff. Otherwise  no, Yeah,  that's  a  little  difference. You're  right,  there's  mathematical  differences. But  there's  a  very  important  conceptual  difference between  how  these  two  layers  operate. Why  did  I  say  at  the  beginning  of  the  lecture  was the  most  important  property  of  the  attention  layer? Yeah,  it's  not  that  takes, let's  say  that  a  little  bit  more precisely,  but  you're  right. It's  that  the  output  representation  for each  word  contains  information about  all  of  the  other  words  in  the  sequence. Let's  contrast  that  with  the  MLP  layer  and  what  it  does, what  do  we  notice  about  what  the  MLP  layer  does? I  have  a  definition  here. H  I  is  equal  to  this  formula. What  do  you  see  happening  in  that  formula? Yeah,  it  doesn't  look  at  the  other  words. Like  there's  a  slightly  more  precise  way that  we  can  say  that  it  doesn't  look  at  the  other  words. Hi  only  depends  on  AI,  what's  that? I  think  that  the  MLP is  a  function  that  only  looks at  one  word  embedding  at  a  time, that  one  word  representation  at  a  time. Rather  than  mixing  together all  of  the  word  representations, the  function  that  only  takes  in it  processes  each  word  separately. The  MLP  very  different  than  attention which  processes  all  the  words  together. The  MLP  is  this  fully  parallel  thing. It  says  apply  exactly  the  same  function  to all  words  independently  in  the  transformer. If  you  remember  one  thing  about the  internals  of  the  transformer  after  this  class, this  is  the  thing  to  remember. We  alternate  between  doing  two  things. The  attention  layer,  we  mix  words  together. In  the  MLP  layer, we  process  words  independently. One  reason  why  I'm  emphasizing  this  is  that  there  are  now non  transformer  architectures  that work  very  well  for  language  modeling. And  other  similar  tasks have  been  discovered  in  the  last  six  months  or  so. They're  new,  but  they  work  very  well. But  this  property  of  alternating  between  mixing  and then  doing  processing  on  individual  words  in  parallel, alternating  between  those  two  things, that's  conserved  across  all  existing  models. Let  me  give  a  bit  of  an  interpretation  of  the  MLP, what  we  think  it's  doing  now. This  is  an  emergent  property  of  the  MLP  layer, but  I'd  say  this is  sort  of  the  consensus  in  the  field  now  that this  is  what  the  MLP  layer  is,  in  fact  doing. This  weight  matrix  here. This  is  the  first  MLP  matrix. And  by  the  way,  the  MLP, each  layer  has  its  own  MLP  layer, and  the  weights  for  the  MLP  layer in  layer  one  are  different  than  the  weights  in  layer  two. We  have  a  tension  here  followed  by  an  MLP. This  MLP  is  different  than  this  MLP. The  structure  is  the  same, but  the  actual  numbers  in the  weight  matrices  are  completely  different. They're  learned  separately  from  each  other. Okay,  let's  look  at  this  MLP  here. So  let's  say  that  the  dimension  of  I  is  equal  to  d.  Okay? So  we  have  D  dimensional  vectors. Each  word  is  represented  by  a  D  dimensional  vector. What  are  the  dimensions  of  the  MLP? This  is  a  convention  that  is  used  every  order. How  many  columns  does  this  matrix  has  D,  right? Because  we  know  that this  matrix  has  to  be  multiplying  this  vector, so  we  know  there  have  to  be  D  columns. The  convention  is  that  there  are  four  rows. This  is  a  huge  matrix. You  could  use  another  number  besides  four  D, but  this  is  what  all  models  use. It's  a  hyper  parameter  that's  been  optimized. These  are  huge  matrices. Why  are  these  huge  matrices? Gpt  three  previous  generation  of  GPT's  PT, three  D  was  equal  to  10,000  or  ten  or  20,000  let's  say 10,000  that  means  four  D  is 40,000  So  this  is  a  10,000  by  40,000  matrix. So  how  big  is  that  matrix? So 400  million. So  just  one  matrix  here  has 400  million  parameters  in  it,  okay? We  multiply  one  MLP times  A.  What's  that  doing? That  is  giving  us  weights. We  can  think  about  that  as  giving  us a  single  number  for  every  row. Here  we  have  four  D  rows, it's  giving  us  a  single  number  for  each  row. What  we  get  a,  we  can  think  about  these as by  a  weight  here. I  just  mean  like  how  important  is  this  row? A  higher  number  will  indicate  more  important. Okay,  We  have  this  vector  here. Let's  call  this  vector. This  is  our  vector.  We  then  do  a  L  on  this. We're  then  going  to  multiply  two  MLP. This  is  a  vector. Relu  of  G  is  a  vector. We  multiply  by  two  MLP. What's  two  MLP? This  is  another  matrix. What  are  the  dimensions  of  this  matrix? What  do  we  know  by  the  number  of  columns? Number  of  columns.  Four  D,  okay? So  there's  four  D  columns, and  then  the  number  of  rows  is  equal  to  d, because  we  want  to  output  a  vector, it's  the  same  dimensionality  as  the  original  vector. We're  not  going  to  be  changing dimensionality  of  the  vectors  in  this  layer. Okay?  So  we  have  this  new  matrix  here. What  we've  done  is  we've  said  contains  in  it  or, or  lue  of  whichever  one  you  want  to  look  at. These  contain  weights  in  them  that  say how  important  are  each  of  the  columns  in  here. The  higher  the  weight  a  column  gets, the  larger  its  contribution  to the  output  of  this  matrix  multiplication. I'm  just  making  an  observation about  how  matrix  multiplication  works  right  now. The  second  column  gets  a  very  high  weight. A  higher  weight  than  anything  else. That  the  second  column  will  contribute  more than  any  other  column  in  this  matrix  to  the  output. Based  on  this,  the  interpretation of  the  M  the  MLP  is  doing  is  the  following. The  first  multiplication  here  is  telling the  model  what  information should  be  looked  up  about  this  vector. The  second  multiplication  is  then retrieving  the  information  that  needs  to  be  looked  up, essentially  from  a  database. Each  column  in  this  second  matrix, each  column  here  contains  facts  about  the  world, facts  about  language,  facts  about  the  world. For  example,  let's  say  that  the  model  is  trying  to, Rome  is  the  capital  of  blank. We're  in  the  process  of  predicting  the  blank  here,  okay? What  the  model  will  do  is use  this  first  matrix  multiplication  to  look  up,  okay? I'm  looking  for  the  capitol  of  me. There  will  be  a  particular  index  that  says, here's  where  the  Capitol  of  Rome  information  is  located. It's  going  to  be  located  in the  third  column  of  this  matrix. We  then  do  a  matrix  multiplication. Retrieve  the  third  column. That  third  column  contains  information  about  Italy. We  look  up,  we  first  say, what  information  do  we  need  to  retrieve, essentially,  where  is  that  information  located? And  then  we  go  into  a  database, which  is  just  a  bunch  of  vectors  of  numbers, and  we  pick  out  the  correct  vector. This  really  actually  looks  like, I'm  not  going  to  say  this  is  the  only  thing  the  MLP  does, but  among  other  things, the  MLP  does  do  this. This  has  been  demonstrated,  you  can  actually  go  into the  model  and  find  out  where the  information  about  the  capital  of  Italy  is  located. It's  like  this  column  in  this  MLP  matrix. If  you  edit  that  column  in  that  matrix, the  model  will  now  predict  Rome  is  the  capital  of France  or  wherever  else  you  want  to  perturb  it  to. Yes,  it  would, it  would  need  to  be  what  it does  because  the  model  needs  to, model  doesn't  come  equipped  with  this  world  knowledge. It  needs  to  learn  it.  And  the  way  that  it learns  it  is  through  back  propagation. Look,  this  is  wild,  okay. When  you  actually  think  about  this  when  the  model is  reading  like  giant  amounts  of  text  and trying  to  predict  the  next  word it's  actually  doing  is  populating these  enormous  weight  matrices with  all  of  these  different  facts  about  the  world. It's  a  complicated  library, it's  a  database,  we  have  like one  vector  per  factor  entity. Yeah,  I'm  describing  right,  these  equations  here. This  is  how  the  model,  this  is  the  forward  pass of  the  model  training. And  then  during  testing,  you  just  run the  model  using  these  equations  on  the  forward  pass. What's  different  between  training and  testing  is  that  training, you're  also  doing  a  backward  pass. You  compute  the  gradient  and  you  update  the  weights. But  the  equations  are  the  same. Whether  the  forward  passes  during  training or  any  other  questions. Yeah,  the  positions  of  the  word  vector, the  positions  of  the  word  vectors, what  do  you  mean  by  that? These  are  not  word  vectors  in this  database.  These  are  not  word  vectors. These  are  vectors  that  describe  facts about  the  world,  right? The  word  vectors  are  the  things  that  are input  into  the  model. Does  that. I'm  so  yeah. So  ation  I  mean, everything  I  mean,  just  like,  you  know, the  columns  end  up  just  being  like  numbers,  right? And  so  those  numbers  all  have  to  be  learned. And  then  like  the  model  being  able  to  like, maybe  there's  a  question  here  like  how  do  we  know  that? How  do  we  know  this  column represents  what  the  capitol  of  Italy  is? How  do  we  as  humans  know  that  that's what  that  information  ends  up  representing? It  was  very  hard  to  figure  that  out. And  the  way  that  we  did  it  is that  we  discovered  that  if  we  swap this  particular  column  with  another  column that  we  think  represents  the  capitol  of  France, then  the  model  starts  making  incorrect  predictions. It's  through  basically,  we  do surgery  on  the  model  and then  we  observe  the  consequences  of  that. That's  how  we  figure  out  that here's  the  information  that  is  contained  here. Yeah,  they're  not  tied  between  layers. Every  layer  has  its  own  pair  of  MLP  weight  matrices. Well,  how  many  parameters  does  GPT  Three. Okay,  let's  do  it  back  of the  envelope  calculation  right  now. P  three,  what  is  it? 100.  And  I'm  embarrassed that  this  number  is  not  just rolling  off  the  tip  of  my  tongue. Is  it  135  billion  parameters? You  should  be  ashamed  of  your  professor. I  believe  it's  135  billion. I'm  within  10  billion  parameters. 175  billion,  sorry. It  has  175  billion  parameters. It  has  around  100  layers  in  it. Okay. So  this  is  a  good  calculation  for  us  to  do. Let's  talk  about  how  many  parameters there  are  in  one  layer. We've  basically  talked  about  all of  the  weight  matrices  in  the  model. Now  there's  a  few  others  that  I'm  not  including, but  to  a  first  approximation, we've  talked  about  all  of  the  weight  matrices that  exist  in  the  model  per  layer. So  how  many  parameters  in  one  layer? What  are  all  of, what  are  all  of  our  weight  matrices? How  many  in  attention? Can  someone  remind  me  how matrices  are  there  in  the  attention  layer? Yeah,  three. Okay,  what  are  the  great, we  have  K  and  how  large  are  the  Q, K  and  V  matrices? They're  all  d  by  d  matrices. D  is  the  size  of  the  word  embeddings. Each,  none  of  the  matrices change  the  size  of  the  word  embeddings. They  see  a  word,  the  output  vector  of  the  same  size. These  are  d  by  d  matrices. That  implies  that  we  have D  squared  parameters  per  matrix. Okay?  Let's  say  the  MLP  matrices. We  have  two  of  them,  right? We  have,  what  was  my  notation  here? Okay?  One  MLP  and  W  two  MLP. These  are  either  by  four  or  four  D  by  D  matrices. It  doesn't  really  matter.  Let's  say  they  are  four. By  that  implies  that we  have  four  D  squared  parameters  matrix. Let's  say  that  D  is  equal  to  104,  okay? So  that's  10,000  Let's  see  what  happens. Suppose,  let  me  do  one  calculation  first. Before  we  get  to  that  assumption, let's  add  up  total  number  of  parameters,  okay,  total. Is  equal  to  what?  It's  equal  to  three  D squared  plus  eight  d  squared, or  11  D  squared. This  is  true  basically  every  transformer, every  transformer  per  layer has  11  D  squared  parameters  in  it. Where  D  is  the  size  of  the  word  embeddings. Where  do  we  see  the  relative contribution  of  the  parameters? 811  of  them  are  in  the  MLP. That  tells  you  something  it  says. I'm  trying  to  emphasize  that  deep  learning  is  stupid. Stupid  intuitions  get  you  very  far. Mlps  811  of  the  parameters probably  means  it's  twice  as  important  as, approximately  twice  as  important  as  the  detention  layer. That's  a  pretty  good  intuition  to  have. Okay. We  have  11  squared  parameters  per  layer. Let's  say  number  of  layers  and  number  of  layers  in  GPT. Three  is  around  100  slayers, let's  set  it  to  100. I  could  set  this  to  some  other  number, let's  just  say  it's  100  for  right  now. And  let's  say  D  is  equal  to  ten  to  the  four. With  these  assumptions, how  many  parameters  do  we  have  in  our  model, okay?  So  let's  see  what  you  do. You  just  basically  multiply  everything  together,  right? You  insert  D  into this  formula  and  you  multiply  it  by  100. So  it's  going  to  be  110  to  2100  times  11  times  D  squared. So  that's  going  to  be  108. That  11  is  equal  to  ten, that's  1010  squared  times  ten  times  ten  to  the  eight. That's  around  ten  to  11,  it's  100  billion. We  just  said  that  GPT  three  has 175  billion  parameters  in  it. We  got  pretty  close,  we're  off  by  factor  of  two. I  think  my  guess  is  that the  D  is  slightly  larger  for  a  GPT  three. It's  like  12,000  or  something, but  that'll  get  us  pretty  close  at that  point  to  175  billion. Yeah, it was  trained  on  a  huge  day. So  all  of  these  parameters, every  one  of  these  hundred  billion  parameters has  to  be  learned.  Every  single  one. The  model,  we  initialize  all  of  these  numbers just  randomly model for one  sentence  or  one  sequence. The  model  sees  n  times  ten  to  the  four. N  times  D  is  the  amount  of information  that's  passed  into  the  model  at  any  one  time. It's  actually  a  lot  less  than  that  because  it's actually  the  number  of  bits  that  are  in  the  words.  Right? Like  we  have  a  vector  representation, but  that  vector  representation itself  needs  to  be  learned. We  pass  in  words  into  the  model. We  pass  in  some  number  of  words  at  the  time, let's  say  1,000  words  at  the  time. A  small  essay,  that's  what  gets  passed  in. Then  the  model  does  back  propagation  on  that. But  the  point  is  every single  one  of  these  100  billion numbers  needs  to  be  learned. The  way  that  it's  learned  is  by processing  an  enormous  amount  of  text. Any  questions  about  that? Did  people  follow  what  happened  to  open  the  eye Last  week  we  almost  didn't  have  our  final  projects. I  was  honestly  pretty  concerned  for  a  little  while  there. Yeah,  it's  pretty  interesting  there  were rumors  that  to  some  extent,  like  I  don't  know  rumors. Let's  say  that  the events  for  those  of  you  who  don't  know, like  open  the  eye  nearly  collapsed  the  company. Last  week,  the  directors fired  the  CEO  for  confusing  reasons. Reasons  that  they  did  not  actually say  just  that  the  CEO  was  being  misleading. Of  the  employees, 90%  of  the  employees  threatened  to  quit. Then  the  CEO  was  going  to  start  something  at  Microsoft. Yeah,  the  company  almost  imploded  directors  back  down. They  ended  up  losing  to  the  CEO. They  all  got  replaced. Now  the  CEO  is  back  in charge  and  everyone  there  is happy  except  for  a  few  people. There  are  rumors  that and  be  complete, I'm  sure  you're  all  delighted  about  that. No.  I  mean,  it  would  have  been  very sad  if  GPT  four  had  been  shut  down. Okay,  we  did  that  calculation. Let's  actually,  let's  finish up  the  transformer  equations. Then  we've  seen  approximately  what the  goal  of  the  MLP  is. The  goal  of  the  MLP  is  to  process  words,  individual. After  the  attention  layer, each  word  knows  about  its  role  in  the  sentence. It's  look  at  all  the  other  words  it's  said, okay,  here's  the  role  that I'm  playing  in  the  sentence  right  now. I've  gotten  all  the  information that  I  need  from  all  of  the  other  words. What  I  need  now  is  to  be  processed  some  more. That's  what  the  MLP  does. The  MLP  processes  each  word  individually, and  a  lot  of  what  it's  doing  amounts  to recalling  world  knowledge  from  its  database. Okay,  let's  look  more  in  detail  though  at  what is  actually  happening  inside  of the  layers  and  we'll  finish  up basically  some  missing  ingredients. Two  missing  ingredients  so far,  maybe  that's  okay. So  the  first  one  is  residual  connections, which  is  a  fancy  name  for  addition. The  second  is  layer  norm, which  is  also  a  fancy  name,  something  very  simple. It's  incredible  that  all  of these  special  ingredients  in  deep  learning amounted  to  either  all  matrix  multiplication, The  MLP  is  literally  just  matrix  multiplication. Or  it's  in  this  case, this  is  a  100  year  old  idea  from  statistics, but  they  gave  it  its  own  name  and then  it's  like  they  made  a  major  discovery. It's  a  very  funny  situation. Okay,  what  happens  here? I'm  going  to  just  change  our  notation  slightly  like  we have  a  one  through. That's  the  output  from  attention. Here's  what  I'll  say,  this  is  equal  to  attention. We  apply  an  attention  layer  to  the  input  vectors. One  through, we're  thinking  about  attention  is  a  function  now. Okay,  attention  is  a  function  that  takes  in a  sequence  of  vectors  and spits  out  a  new  sequence  of  vectors. What  we're  then  going  to  do  is define  a  new  sequence  of  vectors,  we'll  call  them. What  we're  going  to  say  is  that  for  I equals  one  to  n  is  equal to  I  plus  this is  called  the  residual  layer. As  I  said,  residual  layer  is  literally  just  addition. We're  defining  this  new  vector  GI  is  going  to  equal  I, the  input  to  at  the  input  to  attention  plus  the, the  output  from  attention. What's  this  layer  doing? Yeah, okay,  that's  a  great  intuition. The  original  input  word  had  some  important  content  there. We  want  to  make  sure  it's  not  lost.  We  did  attention. We  want  to  make  sure  that  we're  keeping the,  I  really  like  that. A  slightly  more  mathematical  way  of  thinking about  what  this  definition  is  saying. What  it's  saying  is  that  when  we're  doing  attention, we're  learning  a  difference. We're  not  trying  to  fully recompute  the  representation  for this  word.  We  have  the  ith  word. What  we're  going  to  do  is  with  attention, we're  learning  the  difference  between the  original  representation  and  the  new  representation. This  AI  here,  we  think that  this  is  a  learned  difference  or residual  between  original  and  new  representation. Okay,  Again, rather  than  completely  learning a  new  representation  from  scratch, we  learned  a  difference  between  the  old  and  new  one. We  may  go  into  a  little  bit  more  depth  there  before. Right  now  I'm  going  to  leave  it  there. Okay,  so  we  then  have,  let's  call  it, let's  call  it  I  is  equal to  layer  norm  of  GI. After  doing  this  residual  layer, which  is  just  adding the  old  representation  to  the  new  one, we  perform  an  operation  called  the  layer  norm. What  is  the  layer  norm?  Let's  talk  about  that. The  layer  is  a  slightly  fancy  way  of just  taking  a  vector  and  mean  centering  it,  that  is. Or  centering  it  at  zero and  then  making  sure  it  has  a  fixed  magnitude. So  what's  the  first  thing  that  we're  doing? We'll  just  talk  about,  we  take a  vector  and  we  apply  a  layer  norm  to  it. Here's  the  formula.  It's  going  to  be  x  minus  e  of x  divided  by  the  variance  of  x. This  is  a  statistical  language. If  you've  taken  some  statistics  courses, you've  learned  about  the  mean  and  variance. Let's  talk  about  what  these  quantities  are, the  average  value  of  x. Is  equal  to  one  over  D. We  have  a  D  dimensional  vector. We  take  the  average  value of  all  of  the  dimensions  of  that  vector. That's  what  we  mean  by  the  mean. Here  we're  doing  is  one  vector  at  a  time. Looking  at  the  mean  value  within  that  vector, there's  just  an  average,  right? What  can  we  say  about  this  quantity  here? X  minus  e  of  x. What  do  we  know  about  this? Not  necessarily.  No.  They  could  be  very  far  away  from zero.  You're  on  the  right  track. What  will  the  mean  of  this  new  V  zero. If  we  take  the  mean  of  x  minus  E  of  x, we'll  get  zero  because  the  mean  is  going  to  be the  average  value  of  x minus  the  average  value  of  the  average  value  of  x, which  is  the  same  as  the  average  value  of  x. This  is  a  new  vector with  mean  zero.  Okay. What  about  the  variance  of  x? How  many  people  remember  the  definition  of  the  variance of  a  vector?  A  distribution. When  we're  talking  about  the  variance  of  a  vector, we're  treating  the  vector  as a  distribution  or  it's  like  samples  from  a  distribution. I'm  certain  all  of  the  data  science  majors here  have  seen  this  definition  before. I  suspect  many  other. Let's  remind  ourselves  what  the  definition  is. The  variance  of  x  is  equal  to. There's  a  question  of  whether  we  divide  by  for  right  now, let's  just  divide  D rather  than  like  D  minus  one  or  something. So sorry, this  should  be  a  square  root  here. Sorry  about  that. Okay,  so what  is  the  variance  telling  us? Telling  us  basically  how  variable, how  much  on  average does  each  position  in  the  vector  vary  from  the  mean. That's  the  first  thing  it's  telling  us. But  there's  an  even  simpler  interpretation  here, If  we  ignore  this  one  over  D  over  here. Okay,  let's  just  ignore this  one  over  D  term  for  right  now. Where  have  you  seen  a  formula  like  this  before? Let's,  let's  talk  about  a  vector. How  is  the  magnitude  of  a  vector  y  defined? We  want  to  compute  the  size of  a  vector.  How  do  we  do  that? Yeah,  square. Okay.  So  we  take  all  the  entries  in  y, we  square  them,  we  sum  them  up, and  we  take  the  square  root  of  that. Correct.  Okay.  So  this  is  a  sum  from  I  equals one  to  D  of  y  I  squared. And  then  we  take  the  square  root  of  this  whole  thing. This  is  just  like,  this  is  basically  the  pager  theorem. So  what's  the  relationship  between  this  and  this? Again,  ignore  the  one  over  D  outfront. Let's  say  I'm  taking  the  square  root of  the  variance  of  x. Yes.  Okay,  good. So  what  this  is  doing  is  this  is measuring  the  size  of  this  vector, the  variance  of  x  is  measuring the  magnitude  of  x  minus  e  of  x, this  mean  centered  vector  x. This  is  equal  to,  we're just  taking  the  square  root  of  this  thing. It's  going  to  be  one  over  square  root  of D  times  the  square  root  of  this  thing. The  square  root  of  this  thing  is  just  the  magnitude  of the  vector  x  minus  x. Let's  go  back  to  layer  norm.  What's  going  on  here? We're  mean  centering  this  vector  x, then  we're  dividing  by the  magnitude  of  that  mean  centered  vector. Then  we  also  have  this  additional  one  over square  root  of  D  there  that  we  have  to  fill  in. What's  the  point  of  this?  I'm  writing out  the  mass  so  that  we can  eventually  get  to  the  intuition. Intuition  is  that  we're  taking  what may  have  been  a  vector  that may  have  been  very  large  or  very  small. Then  by  dividing  by  the  magnitude  of  that  vector, we  now  make  its  size  constant,  its  magnitude  constant. We're  transforming  an  arbitrarily  large  or  small  vector into  a  vector  of  known  magnitude. That's  what  the  layer  normal  is  doing. What  it  means  is  that  when you're  passing  through  all  of  these  layers, the  attention  layer  and  then  subsequently  the  MLP  layer, you're  not  stretching  out the  vectors  too  much  or  making  them  too  small. Now  there's  a  slight  twist  here  which is  this  is  not  quite  the  full  equation. There's  another  scaling  term  that  we have  people  use  Greek  layers  for  it. We  don't  need  to  use  Greek  layers  here, we'll  just  call  it,  we actually  multiply  this  by  a  vector. This  is  not  a  product. Multiply  each  component  individually  and then  we  add  a  constant  that  I'll  call  here. These  are  both  learned.  This  is  a  minor  detail. What  these  two  terms  are  basically doing  is  they're  just  rescaling individual  components  of  the  vector  in  a  fixed  way. This,  note  that  B  and  C  don't  depend  on  x. Every  vector  has  this  applied  in  the  same  way. It  just  gives  the  model  a  little  bit  more  flexibility  but basically  overall  summary  of layer  norm  though  one  thing  to take  away  is  it's  taking  a  vector  of unknown  or  possibly  very  variable  magnitude. It's  reshaping  it  that  it  now  has  fixed  magnitude. Any  questions? Yes. You  standardize  the  vectors? Yes.  This  is  something  that  people  in  statistics before  you  ever  run  your  PCA,  okay? You  have  to  do  something  very  similar  to  this. If  you  ever  run  PCA, you  have  to  standardize  your  vectors. Basically  the  same  thing  going  on  here. That's  like  a  trick  that's  literally  100  years  old. It's  a  little  different  than  what  they  do  in  PCA. But  like  this  type  of standardization  of  your  data is  something  that  occurs  everywhere. And  the  reason  why  you standardize  is  that  it  makes  comparisons  easy. You  don't  want  to  have  to  compare really  huge  vectors  to  really  small  ones. For  example,  when  you're  doing  attention, I  called  this  thing  n  Here. Let  me  go  back  to  my  previous  notation. We'll  actually  call  this  A.  I want  to  remain  consistent  here. This  N  will  become  an  A. What  do  we  do  then?  No,  it  does  not. Never  mind.  We're  going  to  keep  using  n. Okay,  we  got  in  our  original  word, performed  an  attention  layer  on  them. We  got  out  a  new  sequence  of  word  vectors  a, one  through  N  on  each  individual  word  vector. We  applied  this  residual  layer,  just  means  adding, basically  computing  a  difference between  the  old  word  vector  and  the  new  one. Attention  forms  that  difference. We  then  did  a  layer  norm  on  each  individual  vector. Now  all  the  vectors  are  basically  the  same  size. We're  then  going  to  use  these  new  vectors and  those  are  going  to  be  the  input  into  the  MLP. We'll  have  H  one  through H  N  is  equal  to  MLP  of  the  I's  one. Up  to  Well,  that's  unfortunate. What  is  the  MLP  do? It  took  in  each  of  these  individual  vectors  and applied  those  matrices  to  each  vector  in  parallel. The  vectors  there  don't  mix. We're  processing  each  of  the  vectors  there  in  parallel. We're  then  going  to  apply  exactly  the  same  residual  layer and  layer  to  the  output  from  the  MLP. Let's  call  these  new  vectors  B. So  we  have  I  is  equal  to  what? I  plus  H.  Where  is  that  saying? I  was  the,  the  input  into  the  MLP. The  word  I  is  the, the  word  that  went  into  the  MLP. Hi  is  the,  the  output  from  the  MLP. We're  getting  our  new  word  representation  for  word  by summing  together  the  old  representation  for word  I  and  the  MLP  representation  for  word  I, which  is  another  residual  layer. Then  finally,  we'll  call  them  I  is  equal  to layer  norm  of  the  B  I. We  just  do  another  layer  norm  to  make sure  all  of  these  outputs  are  the  same  size. This  is  now  the  transformer in  terms  of  basically  all of  the  weights  that  need  to  be  learned. We've  introduced  them,  the  major mathematical  equations,  we've  introduced  them. This  is  something  that  I  can  honestly tell  you  now  is  a  transformer. There  are  other  details  that  are  important, but  this  is  the  transformer. Now  you  may  ask,  we now  have  actually  this  is  like  the  core. This  is  what  happens  inside  of  a  transformer  layer. We  have  the  sequence  of  equations  here. I'm  looking  at  this,  it's like  a  bunch  of  stuff  is  happening. We've  tried  to  develop  some  intuitions  for  this, but  it's  happening  in  a  very  specific  order. You  first  do  a  residual  layer,  then  do  a  layer. You're  sequencing  that  twice  and it's  in  between  detention  and  MLP. All  the  stuff  is  happening  in  a  very  specific  order. How  was  this  discovered,  right? That  you  could  ask  yourself,  that would  be  a  very  reasonable  question. Does  anyone  want  to  make  a  guess about  how  this  was  discovered? Yeah,  yeah. Basically,  they  just  tried  a  bunch  of stuff  and  this  one  happened  to  work. It  turns  out  there  are  a  couple  of other  variations  in  the  literature that  people  sometimes  use. Sometimes  people  put  the  layer  norm before  attention  and  before  the  MLP  rather  than  after. That's  a  very  common  variation. There's  a  couple  of  variations, but  to  a  first  approximation, there's  no  local  change  to  this that  you  can  make  and  still  get  it  to  work  as  well. It's  a  local  optimum. It  doesn't  mean  that  it's  the  best  model  that  exists. But  what  it  means  that  there's no  straightforward  modification  that  you can  make  to  these  equations  that  will  not  hurt  things. The  way  that  was  discovered  is  just by  they  threw  a  bunch  of  stuff  at  the  wall. They  were  able  to  throw  a  bunch  of stuff  at  the  wall  because  it  was discovered  at  Google  where  they had  huge  amounts  of  compute. They're  able  to  call  up  huge  computer  clusters, try  a  bunch  of  stuff  in  parallel.  They  found  what  worked. That's  where  the  genius  came  in. Yeah,  it's  a  good  question. How  do  they  know  where  to  look  for  all  of the  individual  components  had  been  discovered  previously, was  discovered  in  2014  in  the  context  of  RNN. People  first  used  attention  for  RNNs  three  years  before the  transformer  residual  layers  were  being used  for  vision  stuff  in the  context  of  convolutional  neural  networks. For  a  couple  of  years  before  that, the  layer  was  quite  new  but  had  already  been  discovered. Mlps  are  70  years  old, all  the  individual  components  were. Yeah,  there  was  some  genius involved  in  knowing  what  to  throw  at  the  wall. But  like  the  exact  sequence  of  how  to  order  these  things, that  was  people  just  guessing  and  then  doing  experiments. This  is  fun.  Some  very  important  details that  we  still  need  to  discuss. However,  two  extremely  important details  where  if  you  did, if  you  don't  have  these  details like  you  still  have  a  transformer, but  what  you're  going  to  end  up  producing  is  nonsense. So  let's  learn  about  why  that  is. We  take  our  word  vectors  from  our  initial  sequence, we  feed  them  through  a  transformer. And  this  is  not  just  a  single  layer, this  is  our  entire  transformer. Let's  say  we  get  out  a  sequence of  vectors,  not  call  them  H  is  here. Let's  just  say  that  the  full, it  spits  out  a  sequence, h1h2  up  to  H. Let's  say  that  what  I'm  trying  to do  is  predict  the  next  word. That's  my  goal.  I'm  going  to  be  using a  transformer  in  order  to  do  next  word  prediction. Now  the  transformer  has  a  very  remarkable  property, which  is  that  every  word  was processed  in  parallel,  right? I  did  all  of  these  computations. I  didn't  have  to  wait  until  word one  was  finished  processing  in  order  to  process  word  two, or  until  the  first  two  words were  done  to  process  word  three. I  could  just  throw  them  all  into  the  system  at  once. And  then  the  system  will  feed into  each  layer  goes  in  vectors. Out  of  each  layer  comes  in  vectors. What  that  means  is  that  I  can  simultaneously predict  the  next  word  that  comes  after  every  other  word. My  target  here. So  my  target  is  to  predict  the  sequence 23  up  to  followed  by, let's  just  call  it  like  end  or  stop. What  I'm  saying  here  is  that  the  first, my  goal  is  to  predict  what  word  two  is. Given  the  first  two  words, my  goal  is  to  predict  what  word  three  is. Given  the  first  words,  my  goal is  to  predict  word  K  plus  one. And  I  get  to  the  very  end,  the  final  word. There's  no  more  words  to  predict. So  what  I'm  going  to  try  to  do  is  just predict  the  sequence  is  over. That's  why  I  mean  by  end  here. Did  we  use  EOS?  Is  that  what  we  have previously  used  to  indicate  the  end  of  the  sequence? I  just  want  to,  maybe  we  haven't  talked  much  about  this. Okay,  I  want  to  predict  the  final  down  here. Let  me  make  clear  that  there's  n minus  one  and  N  minus  one  here. From  words  one  through  N  minus  one, I'm  going  to  be  trying  to  predict  the  last  word, N.  Okay,  Any  questions  about  this  set  up  here? I  take  in  each  word  I'm  trying  to  predict,  the  next  word, transformer  is  remarkable  because  I can  try  to  solve  all  of  these  problems, prediction  problems  I'm  trying  to  solve, and  I  get  to  do  it  all  at  once. So  how  do  I  do  that?  What  I'm  going  to  do  is  I take  each  of  these  vectors  I  and  I'm  going to  run  them  through  just  a  linear  layer. I'm  going  to  transform  each  of  the  vectors into  a  vector  that's  the  size  of  my  Vocab. Let's  say  that  V  is  equal  to  size  of  my  Vocab. Let's  define  a  matrix. We've  seen  this  before,  but  let's just  remind  ourselves  about  how  this  works. I'm  going  to  define  a  matrix  output. This  is  a  matrix. Then  the  probability, the  probability  distribution  for  next  word  for, let's  say  for  word  two. What's  that  equal  to?  It's  equal  to soft  max  of  out  times  h. One  out  transforms  the  first  vector into  a  new  vector  of  dimensionality V.  Those  give  me  the  weights  for  every  vocabulary  item. I  do  a  soft  max  that gives  me  a  probability  distribution over  every  output  word. I'm  going  to  be  using  one in  order  to  predict  what  word  two  is. Hopefully  I  get  the  right  answer. My  loss  is  my  normal  negative  log  likelihood  loss. In  general,  the  probability  distribution  for  word or  let's  say  for  one, probabilitybution  for  the  word  plus  one. It's  going  to  be  soft  max  out  times  which  factor, how  do  I  predict  I  plus  one? Hi,  great. I  get  the  probability  of  the  correct  word  From  this, I  do  negative  log  of  that  probability. I  sum  up  all  of  those  losses. Then  my  full  loss  function, my  loss  for  the  sequence  equals the  sum of  word  I. I'm  not  worrying  about  the  boundaries  here. There's  a  little  bit  of  fuzziness  in  my  definition. I  don't  want  to  take  up  too  much  time  though, with  this.  We  have. A  bunch  of  predictions  that  we're  making. We  sum  up  the  loss  from  each  of  those  predictions. I  think  we  know  how  to  do  that. Okay,  everything  is  defined. Now  we  have  next  word  prediction  tasks. If  you  give  me  a  sequence  of  length  N, I  have  different  next word  prediction  tasks  that  I've  defined predict  word  that  comes after  all  of  the  previous  I  minus  one  words. I  have  a  loss  function. I  have  all  of  my  formulas. I  can  do  back  propagation. If  you  do  this,  it  will  be  nonsense. And  there  are  two  reasons why  it  will  be  complete  nonsense. Yeah,  Well,  no,  no. I  mean,  there's,  there's  no  problem. I  mean,  just  think  about  if there's  more  than  two  weight  matrices. I'm  not  sure  exactly  what  you  mean. There's  there's  five  weight  matrices  per  layer,  right? There's  the  three  attention  matrices, two  MLP  matrices,  and  no,  there's, I  mean,  it's,  it's  the  same  as  any  MLP,  right? So  on  problem  set  three, I  believe  there  were  two  weight  matrices  there. And  yet  you  define  back  propagation  the  same  exact  thing. There's  nothing  funny  about  back  propagation  here. Nothing  technical  like  that. This  is  a  conceptual  error  that  we  have  made. Two  conceptual  errors  that  we  have  made?  Yes. Okay.  You're  absolutely  on  the  right  track  here  at. Let's  focus  on  attention. Let's  say  that  I'm  the  first  word trying  to  predict  the  second  word. What  information  did  the  first  word  get  during  attention? You  smelled  something  fishy  going  on  here? Yeah,  all  the  following  words. What  do  the  following  words  include? Information  about  the  second  word. If  we  just  optimize  the  model  as  it  stands, the  first  word  will  learn  to  pay  attention  to the  second  word  of it  will  know  how  to  predict  the  second  words. It  already  saw  it.  That's  a  violation  of  causality. I  think  you  thought about  that  a  little  bit  on  problems  set for  we  have  now  the  future  influencing  the  past. That's  a  big  problem.  We  need  to  fix  that. There's  a  second  problem  that  we're  going  to  encounter. How  is  word  order  represented  in  the  model? Right  now,  let's  think  about  the  equations  for  attention.
