All  right,  that's all  right,  let's  start. So  I  want  to  talk about  one  new  feature  that  was  released  in  the  Open  API, very  relevant  for  your  final  projects  that  you  now  you can  use  the  overall. It's  under  this  tool  section  under  documentation, but  it's  something  called  assistance. You  see  it's  talking  about  assistance  here. Let's  see,  playground  assistance  over  here. What  you  can  see  is  that  I've actually  already  created  an  assistant. Let's  create  a  new  assistant.  What  is  the  assistant? You're  able  to  give  it  instructions. This  will  be  the  prompt  that  you  give, able  to  also  talk  to  it  like  a  normal  chat. Very  special  here  is  that  you  can  upload  files. What  files  would  be  relevant  for  you  to  By  the  way, you  can  do  this  either  on  playground  or  through  the  API. You'll  want  to  do  it  through the  API  for  your  final  project. If  you  decide  to  use  this  tool, what  files  would  be  relevant  for  your  final  project? Well,  they  would  be  class  files. Any  information  that  you  want  to inject  into  the  model.  Let's  load. I'm  just  going  to  upload  this  random  piece  of  code  here, tolerant  sampler, I  actually  don't  even  know  what  it  does, which  makes  it  perfect  test, we're  going  to  turn  on  retrieval  here. What  else  do  we  need  to  do?  We  need  to  select  the  model. We'll  select  41106. Preview.  I  think  that's  actually the  only  model  that  works  right  now  with  retrieval. I'm  not  sure  it's  the  safest  one  to  go  with. We  save  it,  then. I'm  going  to  test  this  in  the  playground. Test  in  playground.  Okay. I'm  going  to  talk  to  it  now. I'm  going  to  tell  it  I've  uploaded  a  file. Tell  me  what  it  does. Okay. It's  still  having  some  growing  pains. I  think  I've  encountered  No,  no,  no. I  uploaded  the  file. You  can  see  it.  You're  a  retrieval  augmented  model. Hopefully,  some  of  these  issues get  worked  out  in  the  next  couple  of  weeks. Okay,  good. So  it  does  actually  have  access  to  it  and it  was  able  to  read  it  and  tell me  what's  going  on  here.  I  don't  know. It  looks  pretty  plausible  way  it's  saying  right  now, okay,  what's  the  relevance  here? What's  the  difference  between  this,  doing  this, and  just  copy  and pasting  the  code  from  this  file  into  the  pro? There's  one  very  important  difference. As  you  know,  these  models  have  a  limited  context  length. We  saw  last  time,  maybe  theoretically, one  of  the  models  now  has  a  context  length of  like  100,000  tokens,  which  is  a  lot, probably  actually  enough  to  upload  everything into  the  prompt  for  the  tutor, but  there's  an  issue  with  doing  that. First  of  all,  I'm  not  sure  it  works  so well  yet  uploading  that  long  of  a  context, I  would  recommend  probably  not  having  a  prompt that's  basically  not  putting more  than  a  few  thousand  words into  the  context  at  a  time. After  that,  my  experience is  that  performance  starts  to  degrade. Okay,  You  don't  want  to  put  more, let's  say  4,000  words  or  8,000  words, something  like  that,  into  the  prompt. So  that  means  you  cannot  fit an  entire  course  textbook  or all  of  the  lecture  notes  for a  class  into  the  prompt  at  once. Not  only  that,  but  putting  all  that into  the  prompt  is  very  expensive. Every  time  that  you  call  the  model  after putting  in  100,000  words  into  the  prompt, it  charges  you  for  each  and every  one  of  those  100,000  words. It  will  start  to  add  up  if  you  use  a  prompt  that  long, you  don't  want  to  do  that  for  your  tutor. Basically,  retrieval  augmented,  this is  a  term  that  people  use  a  lot. Now,  it  has  a  fairly It's  a  general  class  of  models. They're  called  retrieval  augmented  generation. You'll  sometimes  see  it  called  Rag  for  short,  retrieval. Augmented  generation  means  basically, you've  uploaded  a  bunch  of  files  that  the  model  has access  to  at  any  given  point, the  model  can  decide  to  essentially, it's  almost  like  a  Google  search. It's  called  like  a  vector  search, but  it's  almost  like  how  Google  works  at  any  given  time. The  model  can  say,  oh,  I'm looking  for  a  certain  information, I'm  going  to  do  a  very  fast  search in  the  files  that  I  have  access  to. I'm  going  to  pull  down  the  relevant  parts  of those  files  and  then  stick  that  into  my  context. Rather  than  putting  everything into  your  context  all  at  once. The  model  only  looks  for the  relevant  stuff  When  it  becomes  relevant, there's  a  complicated  set  of  tradeoffs  here. You'll  have  to  actually experiment  to  see  how  it  will  work. As  you  can  imagine,  the  fact  that  the  model  does  not  have it  in  context  means the  model  doesn't  know  what  it's  looking  for. It  doesn't  know  what  it's  going  to find  before  it  actually  finds  it. The  search  for  the  information  may  not be  perfect  because  the  model doesn't  even  know  what's  there. It's  like  when  you're  searching  for  something  on  Google, sometimes  you  don't  find, you  don't  know  what  to  look  for. It's  going  to  be  the  same  problem  for the  model  under  some  conditions. It  will  basically,  if  the  models, if  you  tell  the  model  ahead  of  time,  hey, I'm  giving  you  access  to  15  files, here's  what's  in  each  of  the  files, you  should  like  search for  this  information  when  it's  relevant, then  it  will  be  able  to  retrieve  effectively, we  don't  know  exactly  how  this  is  working  under  the  hood. It's  something  along  the  lines  of  what  I  just  described. My  guess  is  that  it will  be  a  useful  tool  for  many  projects. Not  necessarily  all  but  many  projects. Any  questions  about  this? Okay.  Yeah,  you  can play  around  with  it  in  playground  and then  access  it  through  the  API. I  wanted  to  show  off  one  other  thing. So  want  show  a  demo for  something  else  that  is  happening  right  now. There's  one  company  doing  this  called  Adept. We'll  see  another  one  in  a  second. There's  weird  stuff  that is  going  to  be  coming  to  the  world. Who  is  doing  this  navigation  of  door  dash  right  now? Does  anyone  want  to?  We're  seeing  a  mouse  click, searching  for  Burma,  love. We  see  it  clicking  on  group  order,  okay? Creating  this  order  and  doing  a  bunch  of  stuff. Who's  doing  it?  Okay,  What  is  this  AI? What's  going  on?  Yeah,  but  how  is  it  doing  that? It  turns  out  just  what we've  seen  from  four  is basically  sufficient  to  do  this.  How  does  it  work? It's  literally  looking  at  the  website. We  know  that  GP  four  and other  language  models  now  have  vision. These  models  are  looking  at  the  website. They  may  also  be  looking  at  some of  the  source  code  from  the  website. You're  going  to  need  vision  and  maybe  an  analysis of  the  source  code  I. It's  looking  at  the  website. It's  then  the  next  to  where  the  tokens  are. Now  where  to  move  your  mouse  and  what  to  click  on, rather  than  telling  you  something. It's  just  another  next  word  prediction  problem, where  words  are  thought  of  as now  tool  use  or  actions  to  take. This  is  a  company  that has  gotten  tons  and  tons  and  tons  of  money. This  is  the  first  thing  that  they've  put  out  in  the  late, they  got  tons  of  money  in. Like  February  or  something  of  this  year. I  saw  her  demo  and  I  was like  blown  away  by  it  at  the  time. But  now  there  are  open  source  versions of  it  just  running  on  GPT  four. Like  nothing  propriety  that  seemed  to  work  pretty  well. This  is  an  open  source  gethubpo  that  just  uses P  vision  capabilities  because they're  now  accessible  through  the  API. You  can  build  that  thing  that  adept  got hundreds  of  millions  of  dollars  to  build in  a  few  seconds  basically. I  guess  it's  skip  the  ad. Okay.  Yeah. I'll  send  a  link  to  this  on  Piazza. If  I  forget  someone  to  remind  me. Yeah.  I  mean  look a  T  four  image  API was  released  like  two  days  ago.  This  is  already  up. Lots  of  companies  that are  building  tooling  involving language  models  are  in  trouble. If  you  thought  that  what  your  technology  was  special. It  turns  out  that  just  having  a smarter  basically  what  dept  has  been doing  is  building  a  special purpose  language  model  for  this  task. The  fact  that  special  purpose  means  it's  very  tiny  and faster  and  cheaper  than  what  open  they  has. But  on  the  other  hand,  as  you get  smarter  and  smarter  and  smarter language  models  that  are  bigger  and  trained  on  more  data, what  it  means  is  that  those smarter  language  models  can  do all  of  the  things  that  these  special purpose  models  were  able  to  do. It's  hard  to  know  who's  going  to  win. But  I  would  say  this  is  a  pretty  serious  threat for  companies  building  in  this  space. Now  that  is  to  say  if  you're  language  model  technology, if  you're  building  on  top  of  the  technology  and  you have  a  product  that  is  deeper  than  just  like, oh,  here's  a  new  language  model, here's  a,  here's  a  new  tech. It's  like  it's  doing  something  for a  particular  customer  base  that's  more  defensible. But  if  you're  thinking  about starting  a  company  in  this  area, you  have  to  think  openye just  releases  a  smarter  model  tomorrow. Will  my  company  be  out  of  business? What  is  going  on  here? So  let's  continue  to  talking  about  RNNs, recurrent  neural  networks. So  we're  going  to  spend  some  time  on  them  today, then  we'll  probably  talk about  them  a  little  bit  more  next  C  as  well. Okay,  so  we  have, so  we  have  our  initial  hidden  states. We  have  hidden  states  corresponding  to  each,  each  word. We  have  formulas  for  computing hidden  states  from previous  hidden  states  in  the  current  words. We  also  have  word  vectors  corresponding  to  each  word. Let's  just  remember  the  formula  for  this. H  one  is  equal  to  what? It's  equal  to,  h  times. Let's  not  forget. So  we  see  there's  a  general  formula  here. The  next  hidden  state  is  computed  as  a or  some  combination  of the  previous  hidden  state  and  the  current  input. So  you're  combining  previous  hidden  states, current  inputs  at  each  time step  to  get  the  next  hidden  states. There's  various  different  formulas  that  people  use  here. This  is  just  about  the  simplest  thing  that you  can  do  similar, but  not  quite  the  same  as something  called  the  Elman  network. Basically,  the  first  recurrent neural  network  was  from  UCSD. Jeff  Ellman  who  was  in  this  department. People  came  up  with  much  better  RNNs. We'll  talk  about  some  reasons  why this  particular  one  doesn't  work  so  well. They  all  share  in  common  this  general  property. Like  what's  the  general  property that  they  share  in  common? Common  across  all  RNNs. Rnns.  What's  common? Let's  say  one. The  I  plus  one  hidden  state  is some  function  of  the  previous  hidden  state, the  current  word  word  W  plus  one. This  is  the  common  property  across  all  RNNs. We're  instantiating  this  in  a  particular  way  here. That's  very  simple. Further  developments  in  RNNs, they  basically  involved,  what  exactly should  this  function  look  like?  This  function? Any  questions  about  this? Okay,  We  need  to  get  an  output  from  this  network, and  we'll  talk  about  some  different  things that  we  can  do  to  get  that  output. Let's  talk  about  one  particularly  simple  thing that  we  can  do  with  this, which  is  what  if  we  want  to  do  sentiment  classification, Any  binary  classification  problem, any  multiclass  classification  problem,  That's  fine. What  we  need  to  do  for sequence  level  classification  problem, that  means  we  have  our  text, we  want  to  assign  a  single  class  to  that  text, happy  or  sad,  or  is  it  spam  or  not? Spam  classification  problem  that  we  want  to  do, we  need  to  be  able  to  look  at  the  entire, there's  only  one  hidden  state  in  this  sequence that  has  information  about  all  of the  what  hidden  state  is  that  it's  the  last  one. The  last  hidden  state  is a  function  of  all  of  the  previous  words. If  you  were  to  change  any  of  the  previous  words, in  general,  the  last  hidden  state  will  change. That's  not  true  for  H2h2.  Doesn't  get  to  see  happy. Okay,  we're  going  to  use the  last  hidden  state  for  the  purposes  of  classification. We'll  have  our  output  value  one. It's  like  an  output  score. Let's  actually  call  it  a  vector. 00  is  equal  to O  output  times  h  three. So  this  is  going  to  be  basically  a  vector  of scores  for  however  many  classes  that  we  have. Then  we'll  have  a  probability  distribution which  is  equal  to  Softmax  of  that  we're going  to  turn  our  scores that  are  in  O  into  a  set  of  probabilities. And  then we're  going  to  have  a  loss  function, which  is  our  normal  loss  function. Loss  is  going  to  equal  to  negative  log  probability  of the  correct  class  given  our  vector  P, whatever  that  correct  class  is. We're  obviously  going  to  learn  these. The  capital  W  here. We  basically  have  three  weight  matrices that  we  need  to  learn. We  have  the  output  weight  matrix, the  hidden  weight  matrix, the  input  weight  matrix,  WI. We  need  to  learn  those  things. The  way  that  we're  going  to  learn  them is  it's  through  back  propagation. We  have  a  loss,  we  need  to  calculate  how  is the  loss  change  when we  change  each  of  those  weight  matrices. It's  a  little  bit  than the  previous  back  propagation  examples  that  we've  seen. But  there's  nothing  actually, it's  a  little  bit  Nas  to  work  through. I  think  we're  going  to  work  through  it  next  time. I  decided  not  to  do  it  today. But  it's  just  an  application of  everything  that  we've  seen  so  far, just  repeated  application  of  the  chain  rule. It  does  have  some  very  weird  properties  though, that  we  have  not  seen  yet. As  a  lead  up  to  understanding  those  weird  properties, I'd  like  to  actually  look  at something  it's  related,  but  it's  not  quite  that. Let's  look  at  a  long  sequence  and  what  happens? Imagine we're building  a  spam  classifier. Uh,  let's  see, it's  1,000  words,  124. Okay.  What's  this  weird  set  up  that  I've  drawn  here? I  want  to  build  a  spam  classifier. I  want  to  take  in  a  sequence  like this,  Determine  is  it's  spam. The  way  I'm  going  to  do  that  is look  at  the  entire  sequence, run  it  through  my  RNN. I  get  out  a  final  hidden  state,  1006. Why  is  it  1006? Because  we  had  six  words and  then  1,000  other  words  that  I'm  not  showing  you. Assume  that  all  the words  from  the  Lama  Lama,  red  pajama  books. Now  this  is  spam, we  have  a  link  here,  let's  say, embedded  in  the  beginning  text that  gives  us  the  site  where  we'll  buy  our  illicit  pills. Then  we  have  children's  books. After  that,  we  should  classify  this  as  spam. What's  the  problem? We're  going  to  be  running  1006. What  we're  going  to  do  with  H  1006, we're  going  to  send  it,  we're going  to  multiply  it  by  this  matrix. That's  going  to  turn  it  into  scores  for the  two  classes,  spam,  spam. We're  going  to  get  a  score  for  each  of  those. We're  going  to  soft  max  them  or  sigmoid, whatever  we  get  back  probabilities  for  spam  spam, then  that's  going  to  be  the  probability that  we  get  what's the  problem  might  not  work. Yeah, exactly. Let's  think  a  little  bit  about  why we're  going  to  lose  that  information. All  of  the  information  about  this  thing  being spam  is  contained  in  the  hidden  state. That's  going  to  be  the  only  place  where  the  model actually  sees  that  this  thing  was, that's  where  all  the  information  about  this  was  recorded. Now  we  get  to  H4h4 is  a  function  of  H  three  in  the  word  ma. The  model  at  this  point  doesn't  know  the  spamming. It  doesn't  know  whether  it's  going  to  be seeing  more  spamming  information  in  the  future. Or  maybe  Ma  is  part  of  the  spamming,  right? Who  knows  what  they  want  you  to  buy  for  the. Mao  will  now  include  information  about  both three  and  about  the  word A  five  will  contain  information  about  this  word, ma,  as  well  as  the  information  from  four. At  each  time  point, we're  mixing  the  old  information about  spam  with  new  information  about  the  incoming  words. If  you  do  this  for  long  enough, the  information  at  all  of those  words  in  the  beginning  will  start  to  disappear. It  will  just  be  overwhelmed by  the  new  information  that  you're  adding  on. Rnns  in  general  have  a  very, very  difficult  time  using contextual  information  from  the  distant  past. It  just  gets  lost.  This  is  the  concern  you  always  have  to worry  about  with  an  R  and  it's loss  of  information  about  earlier  in  the  sequence. Any  questions? Okay,  this  is  related, but  it's  not  identical  to  another  problem, which  we'll  learn  about  next  time, which  is  called  the exploding  and  vanishing  gradient  problem. This  is  another  problem  that comes  about  from  the  fact  that  you are  sending  information  through long  sequences  of  function  applications. You're  applying  the  same  over  and  over  and  over  again. Is  this  R  and  N  F  applies  to  four,  then  56, then  H  seven,  sending information  into  the  future  from  very  far  back  in  time. This  is  another  problem  that  results  from  that. We'll  talk  about  that.  It's  a  problem  for doing  back  propagation  in  RNNs. Okay,  I  want  to  talk about  another  major  application of  RNNs  besides  classification, and  how  we  would  go  about  doing  it, that  is  called  language  modeling. Language  modeling  is  language  models as  you  know  them  primarily  do. We're  going  to  introduce  language  modeling  today  then. Basically  we're  going  to  be  studying  it  for the  rest  of  the  quarter  language  model. Again,  it  does  not  mean  anything  having  to  do  with language  refers  to  next  word  prediction. Another  more  precise  way  of  saying  this  is a  probability  distribution  over  sequences  of  words. So  we're  going  to  be  trying  to  learn a  probability  distribution  that  assigns high  probability  to  frequent  words and  low  probability  to  frequent  sequences  of  words, and  low  probability  to  infrequent  sequences  of  words. Okay? Any  probability  distribution  again, W  one  through  WN  are  words. Any  probability  distribution  over a  sequence  of  words  can  be  decomposed. Any  probability  distribution  over any  sequence  can  be  decomposed  in  this  way. And  this  is  something  called  the chain  rule  of  probability, different  from  chain  rule  of  calculus. How  is  it  decomposed? You  first  say,  what's the  probability  of  choosing  the  first  word? You  get  that  you  multiply  that  by  what  was the  probability  of  the  second  word  in  the  first  word. You  multiply  that  by  what  was the  probability  of  choosing  the  third  word, given  words  1.2  You  keep  going  in this  way  until  you  get  to the  last  word  where the  last  word  is  generated  given  the  full  context, W,  one  through  and  minus  one. What  we're  saying  here  is we've  just  used  a  mathematical  law. Okay,  This  is  not  something  that  we're  inventing  here, It's  not  something  that  we're doing  splicifically  for  a  language  model. This  is  just  a  law  of  probability  distributions. Okay,  We're  using  this  law in  order  to  transform  this  very  hard  problem, which  says,  if  you  give  me  a  sequence  of  words, give  me  back  the  probability  of  that  sequence. That's  ridiculously  hard,  Like  these  are  going  to  be. This  sequence  these  days  may  have  100,000  words  in  it. I  want  to  be  able  to  give  the  machine a  sequence  of  100,000  words  and  ask  it, what  was  the  probability  of  this  sequence  of  words? I'm  turning  that  problem  into a  sequence  of  much  simpler  problems. Instead  of  asking  what's the  probability  of  the  full  sequence, I  imagine  the  sequence  was  being  generated  incrementally. What  would  be  the  probability  of this  sequence  being  achieved? I  first  ask  about  the  probability of  generating  the  first  word, then  generating  the  second  word, given  the  first  word,  and  so  on. Where  is  next  word  prediction  coming  in? Here?  Well,  in  each  of  the  terms, here  is  a  next  word  prediction  problem. This  is  the  next  word  after  the  first  word. This  is  Generate  the  third  word that  comes  after  the  first  two  words. I  have  different  next word  prediction  problems  that  I'm  solving. Any  questions  about  this? No. Okay. What  does this  have  to  do  with  RNNs? Well,  we  can  we can  train  a  network  to do  language  modeling  with an  RNN  in  a  very  straightforward  way. How  would  we  do  it?  Let's  say that  I  want  to  train  a  network, an  RN  N,  to  do  language  modeling. What  I  mean  by  that  is  I want  to  train  a  network  such  that if  I  give  it  a  partial  sequence, it  will  tell  me  what's  the  probability  of the  next  word  after  that  sequence. It  will  do  that  for  any  sequence. If  I  have  such  a  network, I  can  use  it  to  generate  sentences  or  generate, How  would  I  build  such  a  network? Yes,  the  same  thing  we're  doing,  pre. Okay,  So  just  take  the  very,  very  last  one  there. So  the  very  last  words,  so  Thomas  is  happy, predict  the  next  word  after  happy. That  would  be  the  idea.  Okay,  let's  try  that. What  we'll  do  is  we  map  this. I'll  put  it  in  a  little  circle  here, or  I'm  going  to  call  it  three. The  reason  I'm  calling  it  three  is  because  it's the  output  that  corresponds  to  the  third  hidden  state. We'll  map  this  to  another  vector, which  is  33  will  be a  distribution  over  the  entire  vocabulary. We're  going  to  be  asking our  model  to  predict  the  next  word  that  comes  after. Maybe  it's  like  the  word,  or  maybe  it's  a  period. It's  not  necessarily  a  word,  it's  just  a  token. We'll  ask  the  model  to  predict  what  comes  after. Happy.  Let's  say  this is  the  classification  problem,  right? We  have  a  correct  answer. There's  only  finitely  many  classes that  the  correct  answer  can  fall  into. Classification  question  over  here. Yeah.  Yeah. The  idea  would  be  like  we  literally  show  it  this  text. So  we're  going  to  be  pulling down  texts  from  the  Internet. We're  going  to  be  pulling  down  billions  of  sentences  from the  internet  showing  the  sentences. You  know,  we  show  a  part  of  the  sentence and  we  ask  it  to  predict the  next  word  after  that  part  of  the  sentence. Does  that  answer  the  question? So  I  mean,  I  just  described  the  training  protocol. I  mean,  I  ain't  describe  back  propagation  yet  here. But  it  is  just  it's  just  back  propagation. I  just  compute  a  gradient. But  look,  there  should  be  something. So  you  may  be  confused  about  the  right  thing  here. Because  there's  something  that's not  quite  right  about  this. So  we're  not  done  here.  We're  not  done.  But  no,  no,  no. But  if  anyone  is  confused  right  now  there,  that's  good. So  I'd  like  to  answer  any  questions  about  that. No.  Okay. Right. So  there's  going  to  be  some  word  that  comes  after  this. The  correct  word  will  be  like there's  an  actual  sentence  that  we  pulled  down. Let's  say  the  actual  sentence that  we  pulled  down  from  the  internet. Let's  say  the  next  word  is  today  at  the  point  of  H  three. The  model  has  not  seen  the  word  today, we're  going  to  ask  it  to  predict. Today  is  the  next  word  that's  there. This  is  a  naturally  occurring  sentence. Okay. The  way  that  we've  set  this  problem  up, we've  shown  it  an  entire  sequence or  like  a  large  fraction  of  a  sequence. And  we're  asking  it  to  predict the  next  word  that  comes  after  that. What's  the  problem  with  doing  things  this  way? Does  anyone  see  a  potential  issue  with  this? Yeah,  that's  true, and  that's  a  persistent  feature  of  all  language  modeling. That's  correct.  33.3  are each  going  to  be  the  same  dimension  as  your  vocabulary. So  those  are  big  vectors  like  50,000  and  that  means  that your  here  that  maps from  a  hidden  state  to  the  output  predictions, that's  going  to  be  a  very  big  matrix. This  is  a  persistent  problem  in  all  of language  modeling  that  yeah, it's  a  problem,  but  it's  not  a  problem  that  we  can  solve. Yeah, okay,  that's  a  great  question. Where  do  we  stop?  The  answer  to  that is  we  choose  a  maximum  text  length. That's  basically  going  to  be  our  maximum  context  length. Maybe  it's  1024,  that  would  be  a  common  one  with  RNNs, it's  actually  much  tinier,  it's  like  128  or  something. That  you're  not  going  to get  too  much  better  performance  by  if  you're using  a  plain  RNN  like  this. Long  texts  are  very  hard  to  deal  with. You're  going  to  choose  a  maximum  context  length. You're  take  your  corpus,  your  set  of  data, and  you're  going  to  break  it  up  into contexts  of  length  128. And  feed  those  contexts  into the  R  and  N  predict  the  next  word, choose  some  word  within  that  context, predict  the  next  word  after  that. You  definitely  have  to  do  it.  I  wouldn't  say that's  the  central  problem  here. Yeah, I  don't  know  what  to  call  it. If  it's  exactly  a  problem,  it's a  persistent  feature  of  language. You're  asking  the  model  in  some  sense, to  perform  an  impossible  task. You  can't  read  someone's  mind.  How  are  you  supposed  to? Next  word  is  the  best  thing  that  you  can hope  to  do  is  have  the  model  take  into account  as  much  context  as  possible to  narrow  down  what  the  likely  next  word  is. Maybe  you're  never  going  to  be able  to  perfectly  predict  what  the  next  word  is. But  what  you  can  say  is  it's probably  one  of  these  four  words. If  you've  narrowed  it  down  to  four  words, you've  learned  a  lot  relative to  where  you  were  starting  with. What  you're  describing  is  like  what  I  would call  uncertainty  or  irreducible  noise. Essentially,  in  the  language modeling  process,  it's  there. But  don't  think  of language  modeling  as  a  problem that  can  be  perfectly  solved. Think  of  it  as  a  problem  that  can be  solved  up  to  a  few  bits,  basically. Yeah,  next  word  today. Today  is  the  correct  next  word  by  assumption  we get  word  today  for. Okay,  so  you're  getting  at  a  very  important  point, which  I'm  going  to  get  back  to  in  a  few  minutes. But  yes.  So  let  me  get  back  to  that  question. Okay.  Yes. Well,  that's  very  closely related  to  the  question  over  here. And  so  what  I  would  say  is  that any  particular  sample  does  force the  mall  to  make  one  discrete  choice. And  you're  right,  there  are  multiple  words  that  are  okay. There  maybe  like,  from the  perspective  of  what  we  want  the  mall  to  learn, like  any  of  those  words  is,  okay, we're  telling  the  mal,  if  you  do  not  get that  particular  word  correct,  like  you're  punished. Right?  So  you  might  think  that  that's  not  good, but  the  hope  is  that  we're  showing  the  model  billions  of different  sentences  over  those  billions  of  sentences. It  will  see  a  lot  of  examples similar  to  this  where  different  words  occur. As  a  result,  it  will  learn,  oh, there's  actually  multiple  possible  words  that occur  after  Thomas  is  happy, but  many  words  that  are  not.  Okay.  There. You're  right  though.  On  any  given  sample, we  are  penalizing  the  model if  it  doesn't  get  exactly  this  word  right. Okay,  let  me  describe  the  problem  that  I  have  in  mind. You're  all  raising  some  valid  concerns about  this,  but  let  me  tell  you, the  issue  I  have  with  this, which  is  what  we've  done  here  is  tremendously  wasteful. We  have  an  entire,  let's  say this  is  a  full  sentence  here,  last  words. Today,  we  have  a  full  sentence  here  worth  of  data, but  we're  only  making  a  prediction  about  a  single  word. That's  a  problem.  We  should be  predicting  every  single  next  word  in  this  sentence. That's  going  to  be  making  the most  use  of  all  of  the  data  that we  have  at  zero. We  should  be  predicting what  the  first  word  of  the  sentence  is. That  might  seem  like  an  impossible  task, but  want  the  model  to produce  plausible  first  words  of  sentences. In  general,  it's  not  like first  words  of  sentences  are  completely  random. Some  words  are  more  likely  than  others. We  should  make  the  model  predict what  the  first  word  is.  How  are  we  going  to  do  that? We're  going  to  feed  through  a  linear  model, make  it  predict  zero, and  then  we  get  a  probability  distribution  over first  words  in  the  sentence  zero. This  is  the  distribution  over  first  words. What  about  this? At  Thomas,  we  should be  predicting  the  next  word,  which  is, is  we  get  a  vector  one, a  vector  p  one  of  probability  distributions over  the  second  word  in  the  sentence, what  word  comes  after  Thomas. We  want  to  learn  that  it's  is the  point  is  in  language  modeling, if  someone  gives  me  a  text, there's  a  lot  of  supervision  signal. In  that  sentence,  or  in  that  text,  look  at  it  over  here. To  produce  a  text  of  length  N, I've  broken  down  the  problem  into  different  problems, into  different  sub  problems, produce  the  first  word,  produce  the  second  word. Given  the  first  word,  all the  way  up  to  produce  the  last  word. Given  all  the  previous  words, each  of  those  different  sub  problems can  get  supervision  from  a  particular  text. I  want  to  squeeze  as  much  information out  of  this  text  as  I  can. What's  my  loss  going  to  be? My  loss  is  going  to  equal  probability  of Thomas  given  certain  what? It's  going  to  equal  negative  log  probability of  Thomas  is  happy  today. What's  that  equal  to?  That's  equal  to negative  log  probability  of  Thomas times  probability  is  given  Thomas times  probability  happy  given Thomas  is  times  probability. Today,  given  Thomas  is happy,  it's  negative. Four  things  being  multiplied  together. What's  my  rules  that  I  can  take that  as  a  sum  of  log  probabilities. I  sum  together  the  loss on  each  of  these  four  different  prediction  problems. That's  equal  to  a  sum  from  I  equals one  to  four  of the  negative  log  probability  of  word  I  given. Let's  say  word  zero  up  to  word  I  minus  one, where  I'm  assuming  that  word  zero  is basically  saying  like  you're at  the  start  of  the  sentence. You  have  to  do  basically  a  word  of  caution  here, I'm  not  going  to  emphasize  this  today, but  you  have  to  do  slightly  funky,  nothing  too  funky, just  slightly  funky  things  for the  first  word  in  the  sentence because  what's  the  context  before  that,  you  need  to  know. The  sentence  starts  here.  Usually  you'll have  a  special  token  that  basically  tells  you, hey,  you're  at  the  start  of  the  sequence. That's  what  we  mean  by  word  zero. It's  like  a  start  of  sequence  token. For  this  one  sentence, we  have  four,  it  has  four  words. Each  of  those  four  words  gives  us a  new  prediction  problem that  we're  training  our  model  to  solve. If  we  only  predict  one  word  at a  time,  that's  extremely  inefficient. We're  wasting  all  of  this  information  from  the  sentence. Any  questions? How  are  your  final  projects  going?  Yeah,  Thumbs  up. Have  people  been  using  GPT  four  productively? I  mean,  in  this  case,  is  it  actually  helping? Yeah,  that's  good. Getting  very  enthusiastic. That's  encouraging.  I'm  happy  to  hear  that. If  you  are  avoiding  eye  contact  with  me  right  now, I  would  recommend  starting your  project  sooner  rather  than  later. I'll  be  getting  you  feedback  on the  project  proposals  by  tomorrow, but  from  what  I've  seen  so  far,  things  are  good. Get  started.  I  think I'm  very  excited  to  see  what  everyone  is  able  to. What  everyone  is  able  to  produce. Okay. I  want  to  talk  about  one  more  thing. We've  talked  about  how  to  do  language modeling  with  an  RNN. This  is  for  training  the  RNN. What  would  we  do?  Okay,  actually I'll  talk  about  two  more  things. What  would  we  do  for sampling  from  a  language  model  after  training? So  how  do  we  generate from  a  language  model that  has  already  been  trained?  How  do  we  do  it? So  someone  gives  me  an  RNN  is  what  is  an  RNN? An  RNN  is  just  three  weight  matrices. At  an  RNN  is  just  the  weight  matrices. Whwiwo,  I  guess  an  initial  hidden  state  as  well. Zero.  If  someone  gives  us  those  things, that's  the  full  definition  of  the  RNN. Someone  trained  these  things,  They  did back  propagation  through  this  loss  function on  a  big  text  corpus. They  trained  the  model  to  do  next  word  prediction. We  as  well  as  you  can  do  it  with  an  RNN. What  do  I  do  with  this  thing  now  that  I  have  it? If  I  want  to  generate  text?  Yeah. Yeah.  Basically  the  star  token  or  like  H  zero, let's  say  the  H  zero  story  represents  the  star  token  in. We'll  be  more  careful  about  this  later  on, but  for  right  now, let's  just  say  zero  represents  like  this  is the  start  of  the  sequence.  It's  the  representation. Okay,  so  someone  gives  me  this,  what  do  I  do  now? It  says  you're  at  the  start  of  your  sequence. I  compute  zero. From  this,  I  compute  zero. It's  a  weird  I  get  out  a  probability  distribution. This  is  the  probability  distribution  over  the  first  word. What  do  I  do  with  this  distribution? Yeah.  Uh  huh. There's  not  going  to  be  any,  these are  actually  probabilities  here,  right? So  this  is  a,  it's  a, it's  a  list  of  probabilities, One  probability  per  vocabulary, per  word  in  your  vocabulary. So  there's  a  probability  for  the  word, a  probability  probability  for  Bob, there's  one  probability  for  every  word. That's  one  thing  you  can  do  now.  Let's  say  we  did  that. It's  a  common  decoding  technique, but  I'm  starting  to generate  a  sentence  from  the  start,  what  will  happen? Say  I  release  the  language  model  that  does  this  type  of, it's  called  greedy  decoding. If  I  do  greedy  decoding after  I  release  my  language  model, what  will  happen  as  a  result  of  that? Yeah,  it'll  be  the  same  sentence every  time  we  want  a  little  bit  of  variety. At  least  in  this  set,  We'll  talk about  where  we  actually  do  want to  use  greedy  decoding  in  a  second. Okay,  if  I  don't  want  the  same  sentence  every  time  I  do. Yeah,  you  have  some  constant  ranging  from  01. Yeah,  let's  say  the  concept  is 0.1  You're  going  to  look  at  the  10%  of  most  likely. That's  interesting.  What  you've just  described  is  something  that  was  actually only  discovered  in  2020  or  2021  I  think, which  is  called  nucleus  sampling. There's  a  little  bit  of  a  twist  on  that. There's  something,  even  you're proposing  a  slightly  fancy  way  of  doing  this. There's  a  simpler  thing  that  we  can  do. We  have  a  probability  distribution, one  probability  per  word. What  can  we  always  do  with  a  probability  distribution? Sample.  I'm  a  sample  from  it. Chose one  first  word. Given  your  probability  distribution  P zero  over  the  vocabulary. I,  my  new  one. I  insert  it,  let's say  it's  over  here.  One,  maybe  it's  like. I  chose  Bob. Okay.  Now,  I  compute  the  hidden  state corresponding  to  Bob,  H  one. And  what  I  do  now,  I  just  repeat  the  process. I  have  11. What's  P1p1?  Is  a  probability  distribution  over the  word  that  comes  after  Bob. What  comes  after  Bob?  I  don't  know. Maybe  it's  like  is  or  was  I  sample  from  this? But  the  point  is  I'm  going  to  sample  from  it. Okay. And  I'm  just  going  to  repeat  the  process until  I  get  to  the  end  of  the  sequence. How  do  I  know  when  I  get  to  the  end  of  the  sequence? Usually  I'm  going  to  have  a  special  end  of  sequence token  that  says,  here's  where  you  stop. And  I  just  hard  code  it. As  soon  as  I  get  to  an  end  of  sequence  token, that's  when  I  stop  generating. The  alternative  is  that  I  just  keep generating  until  I  get  to  a  maximum  context  length. That's  another  possibility. Yeah,  we  discuss, let's  not  talk  about  the  starting  token  because  I'm  being a  little  bit  loose  with how  I'm  describing  this  right  now. Let's  say  how  you  get  80. Can  you  take  a  guess  how  you  can  get  010? Have  to  do  with  the  loss?  Exactly.  We  can just  learn  H  zero  like  we  would learn  any  other  parameter  in  the  network. If  you  don't  know  it,  use  back  propagation  to  learn  it. That's  a  pretty  good  way  of  thinking  about  it. That  question, what  input  is  that? What  I  was  considering right  now  was  just  how  you  would  get a  language  model  to  just  generate random  text  from  its  distribution. The  language  model  learned a  distribution  over  text  sequences. How  would  you  get  samples  from  that  distribution? And  the  way  that  you  do  it  is  just  one  word  at  a  time. You  have  it  sample  the  first  word, then  sample  the  second  word, then  sample  the  third  word. Eventually,  you're  going  to  get  a  sample from  this  probability  distribution. That  was  the  first  question  that  I  was  asking. I  think  you're  asking  a  question  is  like, what  if  you  ask  the  language  model? If  you  give  it  a  prompt,  you  say, I  want  you  to  do  this  for  me, how  would  you  get  it  to  sample  from  that? Is  that  the  question?  Okay,  great  question. That's  what  I  was  talking  about here  was  called  unconditional  generation. This  is  just  like  generate  random  text  for me  where  the  random  means according  to that  probability  tribution  that  we've  learned. This  is  not  honestly  such  an  interesting  thing because  it's  going  to produce  fake  Twitter  threads  for  you  or  something,  right? Like  it  was.  Try  a  bunch  of  Twitter  data. It's  just  going  to  generate  a random,  dumb  Twitter  thread. That  didn't  even  happen  before.  You  don't  want  that. That's  right.  Unconditional  generation  is  not such  an  interesting  thing  to  do. But  if  you  can  do  unconditional  generation, you  can  almost  always  do  the  much more  interesting  conditional  generation. Conditional  generation  means  someone  gives  you a  prompt  and  you want  the  text  completion  given  that  prompt. The  way  that  we  do  conditional  generation, it's  almost  the  same  as  unconditional  generation. If  someone  gives  us  a  trained  RNN, how  do  we  do  conditional  generation? They  give  us  a  prompt.  Someone  choose  a  prompt, a  very  short  prompt  for me  so  that  I  don't  have  to  write  much. Yeah,  okay,  here's  my  prompt. How  do  I  use  this  prompt to  generate  conditioned  on  this  prompt. So  what  I  want  to  know,  what  is  conditional  generation? Conditional  generation  means  what's  the  sequence  of  words three  through  K  that  come  after  today? That's  the  distribution  that  I  want  to  sample  from. What  are  the  likely  word  sequences  that  come after  this  particular  prompt? How  do  I  use  my  pre  trained  R and  N  to  sample  from  the  sequence? Yeah,  the  same  thing. Okay? Exactly.  So  it's  going  to  be exactly  what  we  did  for  unconditional  generation, except  that  we  don't  care  about the  probabilities  that  came  out  of  H  zero  and  H  one. Because  we  already  know  what  the  first and  second  word  in  the  sentence  are. We  ignore  the  probabilities  that come  after  H  zero  and  H  one. We  only  start  with  the  probabilities  that H  two  produces  two, produces  two,  which  then  produces  two. This  is  the  probability distribution  for  the  third  word  in the  sentence  we  sample from  this  distribution  P  two,  the  word  a. We  keep  going  in  this  way. We  sample  the  word  a.  Then  that produces  the  hidden  state  h  three. That  produces  a  new  sequence  of probabilities  over  what's  the  next  word after  today  is  a  sample  from  that. Okay,  it's  in  the  conditional  generation  setting where  this  issue  of  how  to do  decoding  becomes  much  more  interesting. The  decoding  algorithm. Decoding  basically  means, is  another  way  of  saying  generation given  a  model  that's  already  been trained  is  just  the  term  for  it,  the  decoding  algorithm. It's  basically  like  how  do  you  select  the  next  word, given  a  trained  language  model for  the  purposes  of  there are  fancier  things  that  people  do. Honestly,  I'm  pretty  skeptical of  almost  everything  fancy  that's  been proposed.  The  die  out. It's  a  very  sad  thing. People  propose  lots  of  fancy  stuff. None  of  it  really  works with  the  next  generation  of  models. There's  two  things  that  the standard  practice  that  we'll  focus  on. The  first  one,  sampling  we've  been  talking about  sample  from the  distribution  probability  distribution. It  says  the  word  a  gets  probability 2%  2%  of  the  time  you  select  the  word  A, select  words  according  to  their  probabilities. The  other  standard  thing  that  people  do is  what  was  already  proposed, which  was  greedy  decoding. Greedy  decoding  means  the  following. After  we  see  the  word  is, there  is  that's  the  most  likely  word. There's  some  word  that  gets  the  highest  probability. Always  select  that  word. That's  all  that  greedy  decoding  says. Greedy  decoding  is  interesting when  you're  doing  conditional  generations. That  is,  if  someone  gives  you a  prompt  and  you  want to  know  what  happens  after  that  prompt, that's  when  people  use  greedy decoding  sometimes.  Why  is  that? Let's  say  someone  gives  you  a  math  problem. The  prompt  is  a  math  problem, you  want  to  know  the  most  likely. What's  a  good  continuation  after  the  math  problem. You  don't  necessarily  want  a  random  sample from  that  distribution  like  you  want  the  right  answer. You  don't  care  about  everything  that the  language  model  thinks  is  possible  there. You  want  to  know  what's  the  thing  that the  language  model  thinks  is  the  most likely  you're  going  to select  the  best  word  at  every  step. Now,  here's  a  puzzle. We're  selecting  the  best  word  at  every  step. That's  what  greedy  decoding  says. Is  that  necessarily  the  sequence that  has  the  highest  probability? No.  Okay.  I  was a  leading  question  but  you  were  very confident  about  that.  Why  not? You  can  get  stuck.  That's  exactly  right. The  first  few  words,  those each  have  very  high  probability, but  then  all  the  words  after  that, it's  diffuse  the  probability everything  is  spread  evenly  over  all  words. And  as  a  result,  the  best  word  has  very  low  probability. Whereas  it  could  be  the  case  that  you can  take  a  hit  early  on, in  probability,  in  return  for  later on  getting  a  much  more  likely  word. This  is  a  subtle  point,  honestly  not  very relevant  for  the  day  to  day practice  of  language  modeling, so  we  don't  need  to  worry  about  it. There's  a  solution  that  people used  to  use  which  is  called  beam. I  used  to  teach  what  beam  search  was, but  it  doesn't  work  very  well in  practice.  I  don't  teach  it. These  are  basically  the  two  things  that  people  do. Sampling  of  some  sort. There's  a  little  bit  of  interesting  stuff that  we'll  still  discuss  here, but  it's  basically  sampling. You  have  your  probabilitbution  choose  approximately  in proportion  to  what  the  probabilistbution  says. Or  greedy  decoding,  choose  the  best  next  word, highest  probability  word  people  do. Let's  end  there.  I'll  see  everyone  next  week.
