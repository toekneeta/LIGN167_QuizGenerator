All  right,  let's  start. All  of  you  should  have  gotten  an  E  mail about  your  project  proposals. I  left  comments  for  everybody,  so  please  take  a  look. They're  not  graded.  Like  I  had  to  put  it  in  the  grade, but  you  should  ignore  the  grade. The  comments  are  the  thing  to  actually  look  at. Um,  They  were  very good  better  than  previous  years  proposals, I  think  for  a  few  reasons. One  is  that  the type  of  project  that  we're  doing  now because  it's  very  focused. The  other  reason  is  that  it  looked  to  me  like almost  everybody  had  some  help  writing  their  proposals. That's  fine.  That  was  perfectly  within  what's  allowed. I  did  get  the  sense  that  there  were a  few  groups  that  maybe  did  not actually  read  the  proposals  that  they  submitted. That  was  apparent  based  on  some  of the  wording  that  was in  the  proposals.  That's  not  so  good. You  want  to  know  what  you're  submitting  in  general, I  guess  general  feedback  would  be, keep  the  project  very  narrow  and focused  like  the  building  an  AI  tutor. It's  bad. You're  going  to  have  a  better  project and  a  better  product. If  you  just  do  one  thing  and  do  it  very  well, that's  the  recommendation  I  would  give  to  everybody. Any  questions  about  that? About  the  final  project? No. Okay.  Anything  else  before  we  get  started? Yeah. Oh,  you  mean  as  far  as  building  a  167  tutor  is? That's  the  question.  I  think  broadly  similar. Yeah.  It's  a  little  hard  to  know. I  don't  know  what  the  world  is going  to  look  like  next  year, but  yeah,  it's  a,  it's a  fair  assumption  to make  for  when  you're  building  the  tutor. Yeah,  I  have  not. So  can  you  say  what  that  means? I  don't  know  if  we  talked  about that  last  class  and  this  is  a  brand  new  feature, so  yeah, basically,  context  specialized. But  do  you  actually  know what  they  are  or  how  they  differ? So  I  haven't  used  them  yet.  I  should. Is  it  just  a  matter  of  giving  some  instructions, Like  giving  a  prompt  and  then  are you  just  basically  creating  a  prompt  and  that's  it. Basic  knowledge.  Yeah.  Personal  assistant  restrictions like  financial  spreadsheets,  like  here's  one? Yeah. Yeah,  there's  like  some. Okay.  Okay. Yeah.  So  it  might  be  a  useful  tool. Let  me  look  into  it  more. Maybe  we  can  talk  about  more  next  class. A  little  bit  skeptical  that  it's  more than  just  a  user  friendly  wrapper  for stuff  that  you  could  do  with the  API  or  you  can  do  in  a  more  explicit  way  with  APA. Open  is  trying to  basically  the  amount of  capture  that  they  have  on  this  market. Part  of  that  is  building  tools  that  are  more opaque  where  you  don't know  what's  going  on  in  those  tools. I  think  that's  probably  a  bad  thing. I  mean,  look,  if  it  works,  it  works. My  sense  is  that  they're  not  releasing very  explicit  documentation  about what  exactly  this  is  doing. It's  just  like,  hey,  build  a  GPT and  give  some  instructions  to  it, but  what's  going  on  under  the  hood? They're  not  really  telling  you  any other  comments  or  questions. Okay,  so  we're  going  to  finish off  recurrent  neural  networks. Actually  we're  going  to  start  today  on transformers  after  seeing  some  problems with  recurrent  neural  networks. So let's  go  back to  the  use  of  an  RN  N  for  oh, there  was  some,  okay, there  was  something  I  wanted  to  show  everybody. I'll,  I'll  do  it  a  little  bit  later. I'll  do  it  a  little  bit  later.  I  just  remembered  now. Okay. Okay. So we  want  to  classify. We'll  go  back  to  the  language  modeling. Not  next  word  prediction,  but  classification. So  we  want  to  classify, is  this  a  happy  or  sad  sentence? It's  very,  very  sad. So  these are  all  vectors. We  have  our  hidden  vectors. We'll  write  down  the  equations again  for  these  hidden  vectors. Once  we  compute  these  hidden  vectors, we  compute  an  output  score and  then  a  probability  from  that. Where  are  our  equations? We  have  that  vector,  HI. By  the  way,  we  have  word  vectors  here  obviously. The  word  vector  for  llama. Word  vector  for  llama. Word  vector  for  Mrs.  word  vector  for  mama. Okay,  So  what  we  have, we  have  that  HI  is  equal  to  u. Makes  a  little  bit  more  space  here. Hi  is  equal  to  relu of  our  weight  matrix that  takes  into  account  hidden  states. We  have  a  weight  matrix  that multiplies  the  previous  hidden  state. And  then  we  sum  the  result  of  that to  input  which  looks  at  the  input, the  word  associated  with  the  word. Okay,  what's  one  rather? One  is  equal  to our  output  weight  matrix  times  like  last. Call  it.  In  this  case  it's  four  but  last  in  general, the  last  hidden  state  then  is  equal  to  sigmoid  of  one. So  we  get  back  a  probability. This  is  the  probability  that the  sentence  is  a  happy  sentence. We  want  in  this  case  to  be  Lowes. It  mean  that  should  be  low. What  we're  saying  is  that  the  loss  is  equal  to the  negative  log  probability of  the  correct  label  given. We  just  basically  compare the  probability  of  correct  label  given. If  the  correct  label  is  happy, that  probability  is,  if  the  correct  label  is  sad, that  probability  is  one  minus  p.  Okay, we're  just  reviewing  what  we've  been  doing. What  do  we  need  to  learn  here  in  this  network? There's  four  things  that  we  need  to  learn  to  learn. This  weight  matrix  H, we  need  to  learn  weight  matrix  I,  this  weight  matrix. And  then  finally,  this  initial  hidden  state  H  zero. Any  questions  about  this?  Okay,  So we're  going  to  be  taking  the  gradient. We're  going  to  be  calculating  the  gradient  of  the  loss. Gradient  of  the  loss  is  going  to  be a  collection  of  partial  derivatives,  right? So  it's  going  to  be  the  partial  derivative of  the  loss  with  respect  to, let's  say,  and  then  the  first  element  of  that. And  then  partial  derivative  of  loss  with  respect  to  H12. I'm  just  like  literally  going  through, this  is  a  big  weight  matrix  H.  We can  just  go  through  all  of the  elements  of  the  weight  matrix. Take  the  partial  derivatives  of the  loss  with  respect  to  all  of those  elements  we  just  keep  going  through. We  have  three  weight  matrix  and  H  zero. Now  the  question  is  what  happens? So  you're  calculating  this  gradient. The  question  is,  what's going  to  happen  when  you  actually  do  this? We're  going  to  be  using  this  gradient  in  order  to  update our  weights.  Things  okay? When  we  do  this,  they  looked  okay  when  we  were  using a  multi  layer  perceptron  or  we haven't  noticed  anything  wrong  yet. Are  they  okay  in  the  case  of  an  RN,  N  in  general, unless  you're  doing  something very  clever  and  even  then  it's not  clear  you  run  into  problems  with  this. The  problem  that  you  run  into, we  can  call  this  the  problem  with gradient  descent  for  RNNs. The  name  for  this  problem, we  mentioned  it  last  time. It's  the  exploding  and  vanishing  gradient  problem. What  this  means  is when  we  see  exploding  and  vanishing  gradient, what  we're  saying  is  that  the  gradient have  a  tendency  to  either  get  very  large  or  very  small. When  you  look  at  the  magnitude  of  this  thing, it's  either  very  large  or  very  small. There's  a  slight  wrinkle  on  that. First  approximation,  that's  what  it  means. We'll  see  there's  some  subtlety in  exactly  what  we're  saying  there. But  basically,  when  you do  your  update  with  this  gradient, you're  either  going  to  be  taking a  huge  step  or  almost  no  step  at  all. What's  the  problem  with  that? Yeah,  I  forget  about you  skipping  over  your  global  minimum.  You're  right. But  even  it's  going to  be  completely  unpredictable  where  you  end  up. You  might  just  end  up  in  somewhere  really  terrible. We  don't  worry  about  global  minima, I'm  not  even  sure  we  worry  about  local  minima. It's  more  of  an  issue  of  do  something by,  but  you're  correct. Okay.  That's  the  issue. Really  he  gradients. What's  the  issue  with  really  tiny  gradients? Yeah. Or  you  just  might  not  make  any  progress whatsoever  basically.  That's  right. Let  me  give  first  a  little  bit  of  an  intuition  about what's  going  on  and  then  we're  going  to  see  some  math. I  used  to  do  a  much  fuller  version of  this  that  was  not  very  illuminating. So  what  we're  going  to  do  is  we're  going to  just  buy  off  a  piece  of  this. But  hopefully  we  get some  intuitions  about  what's  going  on. Last  time  I  mentioned  a  problem that  RNNs  have  when they're  dealing  with  very  long  sequences. Do  you  remember  what  that  problem  was? So  let's  say  you  have  a  sequence  with  100  or  1,000  or 10,000  words  tokens  that's  processing. What's  the  problem  that  you  end  up  with? Yeah,  you  lose  the  information  from  the  beginning. In  this  case,  there's  only  four  words, so  it's  not  that  big  of  a  deal. But  four, the  amount  of  information  that  has  about  the  first  word, Ama,  it's  going  to  be  limited, or  at  least  it's  not  going  to  be  as  much  information  as the  RNN  had  when  it  was  processing  that  first  word. Ma.  Every  time  that  you  take  an  additional  word, you're  squishing  the  information  that  you  saw  before. Eventually,  the  information  from  the  very  beginning, the  sequence  gets  squished  out. Okay,  that's  a  problem  with  the  forward  pass  of  the  RNN. There's  a  related  problem  on  the  backward  pass, that  is  when  you're  doing  back  propagation in  order  to  compute  your  gradient. Let's  think  about  how  back  propagation  works  in  an  RNN. We  first  see  how  does  changing, let's  say  four,  change  the  loss. How  does  it  change  the  loss? It's  actually  very  easy  to  think  about  this. When  we  change  H  four, it  directly  changes  one. Which  directly  changes,  that's  basically  the  loss. That  log  of  that  is  the  loss. We  can  measure  easily  how  H  four  changes  the  loss. When  we  measure  how  does  changing H  three  change  the  loss? We  have  the  third  hidden  state. How  does  changing  the  third  hidden  state  change  the  loss? A  good  finger  diagram  there. Can  someone  just  verbally  explain  what's  going  on  there? Yeah,  four. Great.  The  effect  is  now  more  indirect. We  measure  the  effect  that  H  three  has  on  H  four. Then  we  measure  the  effect  that  we  previously  computed on  four  on  the  loss  for  two. It's  the  same  thing  how  H  two change  three  and  then  how  H  three  change, how  four  change  the  loss. The  result  of  this  is  the  effect  that each  hidden  state  in  the  distant  past has  on  the  loss  is  more  indirect. You  imagine  a  sequence  with  10,000  words. You're  trying  to  measure  an  extremely  indirect  effect that  let's  say  changing the  first  hidden  state  would  have had  on  changing  the  loss. You  have  to  go  through  10,000 intermediate  steps  to  get  there. The  exploding  and  vanishing  gradient  problem is  basically  a  consequence, The  fact  that  you  get  really huge  and  really  tiny  gradients. It's  a  consequence  of  how  indirect  the  effects  are. The  vanishing  part  is  actually,  I  think, easier  to  understand  why  you  would  get  very, very  tiny  gradients  here. Can  someone  give  me  an  intuition  for  that? It  doesn't  have  to  be  mathematical  yet, just  I  want  a  high  level  intuition for  why  you  might  get  really  tiny  grade. Yeah. Yeah,  that's  right. I  think  that's  the  right high  level  intuition  to  have  here. It's  like  you  had  to  go through  so  many  intermediate  steps. The  effect  from  the  beginning,  it  basically  disappears. It  vanishes  due  to  noise  or  whatever. From  the  intermediate  states, I  think  that's  basically  right. The  exploding  one,  I  think  is a  little  bit  more  surprising. It  turns  out  they  have  the  same  source. It's  a  simple  mathematical  source, which  is  that  the  chain  rule says  you  need  to  multiply  stuff  together. When  you  have  a  long  sequence, it  means  you're  multiplying lots  and  lots  of  stuff  together. When  you  multiply  stuff  together, a  lot  of  it,  that  quantity either  gets  really  big  or  really  small. Let's  look  at  that. Let  me  also  work  with  a  simplified  RNN. This  is  already  a  very  simple  one, but  let's  work  with  an  even  simpler one  where  we  replace  all  of  our  vectors  with  scalars. H  zero  now  is  a  scalar. It's  not  a  vector. We  have  our,  we'll  think about  our  word  vectors  as  scales  as  well. The  word  for  word  for  ma  is  a  number. This  is  ridiculous,  obviously. But  the  math  will  stay  the  same, basically,  when  we  move  over  to  vector. Let's  just  do  everything  in  scales. There's  nothing  really  gained by  talking  about  vectors  here. H  one,  that's  a  scalar. All  right,  let  me  forget  about  Llama  here. Let's  just  call  this  word  one. Since  I  have  words, let's  just  say  that  we  directly compute  our  loss  from  this. What  are  our  formula?  They're  going to  be  the  same  formulas  there, except  that  instead  of  the  weight, weight  matrices  are  replaced  with  scalars. We  have  the,  the  hidden  state, which  is  just  a  single  number  is  equal  to. I'm  just  going  to  call,  now  I have  a  collision  naming  collision  here. So  let  me  just, let's  give  these  things  another  name because  they're  not  even  word  vectors  anymore. Let  me  just  call  this  like  I  one  through  IN, where  I  indicates  that  it's, it's  an  input  to  the  sequence. Okay,  I'm  just  renaming  my  word  inputs  to, I  have  a  weight  scalar  here. I  multiply  this  scalar by  the  previous  hidden  state,  HI  minus  one. I  add  that to  times  b. We  have  a  weight  scalar  here, we're  multiplying  that  by  the,  the  input. I'm  changing  my  notation  a  little  bit. The  main  thing  is  that  I'm  making the  weight  matrices  lower case  because  they're  single  scalers. That's  the  most  important  thing  to  think  about  here. We  can  compute,  let's  say, do  we  need  any  other  formulas  here? Then  we  really  don't  even  need  any  other  formulas. Let's  talk  about  computing  how the  loss  changes  with  respect  to  H.  Is  this  easy  to  do? We  have  our  final  hidden  state, we  want  to  see  how  it's  changing the  final  hidden  state,  change  the  loss. Is  that  an  easy  thing?  Yes,  easy. We  know  how  to  do  this,  let's  do  it. We  know  how  to  do  it.  Nothing  interesting  happens  here. I'll  just  say  this  is  easy,  you  can  do  this. Let's  not  talk  about  how changing  HN  minus  one  changes  the  loss. What's  the  formula  for  this?  Yeah. Right. So  if  I  have  HN  minus  one, I  look  at  how  does  changing  HN  minus  one  change  HN. How  does  changing  HN  change  the  loss? I  multiply  those  two  things  together. What's  this  term?  Let's  talk  about  that. We  have  H,  HN  minus  one. How  do  I  calculate  that? Yeah. Okay.  Yeah,  Yeah. So  let's  actually,  so  to  simplify  this, let's  actually  give,  let's give  the  stuff  inside  a  name.  How  about  that? We  have  that  HI. I'm  just  going  to  write  down  a  new  notation. Hi  is  equal  to  L. Let's  call  it  where I  is  equal  to  the  stuff  that's  inside  of  here. I'm  just  giving  a  name  to  the  stuff  that's  inside  of the  lue  times  HI  minus one  plus I  times  I. Okay,  so  let's  go  back  to partial  derivative  of  HN  with  respect  to  H  N  minus  one. So  yes,  partial, great,  great. So  this  is  something  that  we  can just  like  we  have  a  formula  for  this,  right? It's  either  zero  or  one  depending  on  where  you  are. That's  it  is  just taking  the  derivative  of  the  lue  function. That's  like  something  that  we  already  know  how  to  do. It's  either  zero  or  one. Okay,  What  about  this? Yeah,  it's  not  even  the  weight  matrix  anymore. That's  why  everything  here  is  a  scalar. Now  our  life  is  much  nicer. It's  just H. Let's  substitute  this  in. So  I  have  my  loss  with  respect  to  HN  times  that  thing. Partial  derivative  of  the  rail  you  with  respect to  I H. Great. What  about  the  loss? Let's  draw  a  line  there  about  partial  derivative  of the  loss  with  respect  to  HN  minus  two.  What  do  I  do? So  it's  going  to  be  partial  derivative  of the  loss  with  respect to  HN  minus  one,  right? So  I  look  at  how  does  changing  H  minus  one, how  changing  H  one  change  the  loss, and  how  is  HN  mus  one  changed  by  HN  minus  two? Okay. I've  already  computed  this  thing. How  is  the  loss  changed  with  respect  to  HN  minus  one? What  about  this  part  here? How  do  I  compute,  how  does  HM  minus one  changed  with  respect  to  HN  minus  two? What  I'm  asking  here,  if my  sentence  has  four  words  in  it, then  I'm  at  two  here. What  I'm  asking,  how  is  changing  H  two  change  three? How  is  changing  H  three  change  the  loss? So  how  do  I  figure  out  how  is changing  H  two  change  three? It's  exactly,  it's  exactly  the  same  as  this  formula. There  was  nothing  that,  all  I  did  is  I  shifted the  indices  back  by  one  instead  of  HN  and  HN  minus  one. Now  I'm  doing  HN  minus  one  and  HN  minus  two. So  I  have  my  formula  over  here. I'm  just  going  to  copy  for  this. So  I  is  a  mistake  here. This  should  be  a  yes, this  should  be  A  and  that  should  be  okay. Good.  Okay,  I  got  this  over  here. How  does  the  loss  change  when  we  change  HN  minus  one? I  copied  that  over  here. And  now  I  need  to  calculate  this. How  does  HN  minus  one  change when  we  change  H  and  minus  two? To  get  that,  I  just use  the  formula  that  we  got  over  here. It's  going  to  be  how? Luche,  a  n  minus  one times  H.  What  simplification  can  I  do  here? I'm  going  to  collect  the  H  is  together. Let's  put  them  out  front. I  h  squared times  Los, let's  move  it  onto  A.  This  is  important. So  let's  give  it  space  that  deserves  Los. Hn  minus  two  is  equal  to H  squared times  L  U  n  times lu  N minus  one  times  loss  N. This  is  how  does  the  loss  change when  I  change  HN  minus  two? Let's  see  if  we  can  think  about  the  general  case  here. Let's  think  about  actually  H  zero. Let's  think  about  what  happens  with  H  zero. How  does  the  loss,  we  have  to  learn  zero, by  the  way,  the  initial  state  of  the  network. How  does  the  loss  change  when  we  change  80? You  can  think  about  this  for  any  of  the. We  now  have  actually  a  general  formula  for  this. What's  the  general  formula  that  I  have  here? What  happens  every  time  that  I  go  one  step  back? There's  a  recursion, a  recurrence  relation  that  we  have  here,  right? So  what's  the  general  relation? Yeah,  exactly. Times  a  bunch  of  rail  use or  times  the  partial  derivative  of  a  bunch  of rail  use  N  the  rail. A  and  minus  one.  And  we're  going  to  go  all  the way  back  to  relu  one, we're  going  to  multiply  them  altogether. Then  the  loss  change  when  we  change  HN, that's  it,  right,  that  we've  now  computed. How  does  the  loss  change? We  compute  partial  derivatives  with respect  to  all  of  the  H.  We  have a  general  formula  for  this  is  this is  the  key  quantity  within  an  R  And  why  is  that? When  you're  computing  gradients  within  the  RN  N, this  is  the  key  quantity  to  know  about. Well,  first  of  all  we  know we're  going  to  be  directly  learning  zero. We  need  to  know  the  gradient  of  the  loss  with respect  to  H  zero  because  that's  how  we're  going  to  do gradient  descent  on  H  zero.  Okay? That's  one  reason.  What's  the  other  reason  why? We  want  to  know  how  does  changing  H  two  change  the  loss? Or  how  does  changing  H  one  change  the  loss? Why  is  that  so  important? Yeah,  Philosophy, We're  not  going  to  be  doing  that  ever because  we're  not  trying  to  learn the  inputs,  but  you're  close. It's  for  computing  another  quantity. What's  the  main  thing  besides  zero  that  we  need  to learn  in  for  an  RNN? Yeah,  WH  and  we  have three  WHWIWreeightteed  to  learn  matrices. How  do  you  compute  the  partial  derivative of  the  loss  with  respect  to  these  weight  matrices? Let's  think  about  the  simple  set  up  here  where  these  are weight  scalers.  The  W's  over  there. These  are  just  single  parameters  that  we  need  to  learn. We  want  to  compute  how  does the  loss  change  when  we  change,  let's  say  H. How  do  we  do  this? Yeah,  Changes  all  of  the  HIs. That's  a  great  point.  So  good. We're  in  a  setting  now  where  changing  this  variable  has effects  on  many  different  variables,  right? Or  many  different  activations. We've  seen  that  before.  Where  did  we see  that  we're  changing one  thing  has  effects  on  many  parts  of  the  network. You  had  a  homework  on  this,  the  MLP. Well,  what  was  the  situation  with  the  MLP,  right? So,  we  had  our  MLP,  we  had  our  input. We  had when  I change  this  node  here, I  have  two  nodes  that  change  downstream. It's  the  same  exact  thing  here. When  I  change  this  H, all  of  the  hidden  states  change. When  I  do  that,  all  of  the  hidden  states  are  defined in  terms  of  WH,  they  all  change. What's  our  rule  for  dealing  with  that? Yeah, I'm  going  to sum  over  all  of  the  hidden  states. I  have  hidden  states.  I  sum  over  the  effect that  changing  H  has  on  all  of  them. Partial  derivative  of Hi  with  respect  to  H. What  do  I  multiply  that  by? Exactly  I  measure  how  does  changing  WH  change  HI. How  does  changing  HI  change  the  loss? I  sum  up  all  of  those  effects. Basically,  changing  H  has  effects  on  all  of  the  nodes. Changing  each  of  the  nodes  then has  an  effect  on  the  loss. I  do  is  I  sum  up  all  of  those  effects, that's  the  effect  of  changing  WH  on  the  loss. Questions  about  this,  okay, let's  put  all  the  pieces  together. I  now  know  how  to  compute partial  derivative  loss  with  respect  to WH  or  basically  all  of  my  weight  matrices. They're  all  going  to  be  basically  the  same  here. There's  slight  differences,  but the  general  point  is  going  to  stand. I've  now  decompose  that  into  two  parts, which  is  how  does  changing  the  weight  change, HI,  that's  actually  very  easy  to  compute  that. I  have  a  formula  over  here. Change.  All  I  get  back  is  like  times  partial  derivative. The  rail  un  changing  HI,  change  the  loss. I  decomposed  it  into  these  two  parts. I  also  have  a  formula  for  each  HI  changes the  loss  changing  I. Measuring  the  effect  of  HI  in  the  loss  that  appears  in every  time  that  you  want  to  take the  partial  derivative  with respect  to  one  of  your  weights. This  thing  here  is  like the  crucial  quantity  in  the  RNN  gradient. Let's  understand  that  thing, then  we  have  a  formula  for  it  here. What  do  we  see  will  happen  to  this  quantity as  the  sequence  length goes  up  or  down,  let's  say  goes  up? What  happens  to  this  quantity? Yeah,  it  depends  crucially  on  the  value of  H.  What  happens  if this  is  the  current  value  of  our  weight  parameter? It  could  be  a  weight  matrix.  Same  thing. Basically,  we  have  the  current  value of  this  weight  parameter  here. What  happens  if  that  weight  parameter  is less  than  one?  It  shrinks. The  change  in  the  loss  goes  to zero  when  we're  changing  the  hidden  state. What  happens  when  is  greater  than  one? It  will  explode. The  partial  derivative  loss  with  respect  to HI  will  grow  to  infinity. In  that  case,  that's the  exploding  and  vanishing  gradient  problem. We  have  this  term  here. Or  the  loss  with  respect  to  HI that  appears  in  all  of  our  gradients. That  thing  either  goes  to  zero  or  goes  to  infinity. As  the  sequence  length  grows  longer, that  will  result  in  our  gradients  either becoming  real  big  or  really,  really  small. There  are  some  nuances  to  this that  I'll  let  all of  you  think  about  some  of  the  nuances  here. When  you  think  about  this  formula, this  formula  really  says  it all  but  to  a  first  approximation, this  is  what  happens  when  you have  an  increasing  sequence  lengths, or  as  a  function  of whether  W  is  greater  than  one  or  less  than  one. I'll  let  you  think  about  that  for  a  few  seconds. So  let's  see  if  this  works. No,  Well, I'm  not  going  to  guarantee  that  I can  get  the  volume  on  this  to  work, but  someone  last  class  pointed  me to  the  fact  that,  oh,  let's  try  that. Okay, Pajama  here, just  curious  in  Mapa, reads  the  story  with  who  is  mama  kisses  baby  baby. Mama  goes  all  the  way  downstairs. Map,  this  is  really  good. I  have  to  thank  the  person  who pointed  me  to  this  last  class. There's  a  whole  series  of  these  things. I  guess  there's  like  a  rap  station in  LA  that  gets  on  famous  rappers  and has  them  read  from  the  book,  they're  really  good. My  son  loves  this. Yeah,  thank  you.  Okay.  I've  given everyone  some  time  to  contemplate  this. Yeah,  yeah, that's  correct,  That's  a  big  problem. This  is  related  to  something  that's called  the  dead  neuron  problem.  Yeah. If  any  of  these  are  zero, the  whole  thing  becomes  zero. A  problem  with  RNNs, it's  a  problem  with  Lu  as  we've  currently  defined  it. People  try  to  do  something  to  alleviate  that  problem. I  think  Lu  was  actually  not  traditionally  used  in  RNNs. I'm  not  sure. In  standard  RNNs  there  are various  non  rail  activation  functions  that are  used  for  exactly  this  reason. I  think  the  hyperbolic  tangent  function  is  a  popular  one. We  don't  need  to  worry  about  that  right  now, but  there  are  various  other  activation  functions that  are  used  for  this  reason. Relu  is  it's  a  transformer  activation  function. It's  a  great  observation.  Any  other  questions? Okay,  basically  this  is related  reasons  just  like these  issues  with  sequence  length  and weird  stuff  that  happens  when  you get  longer  and  longer  sequence  lengths  in  RNN, the  compression  of  information, these  exploding  and  vanishing  gradients. These  are  the  reasons  why  RNNs simple  RN  N's  like  this  don't  work  very well  for  text  data, why  people  switched  over  to  using  transformers. That's  what  we'll  talk  about  right  now. We'll  start  talking  about  the  transformer. We're  going  to  start  by  talking about  the  most  famous  feature  of  the  transformer, which  I  think  increasingly is  not  probably  the  most  important, but  it's  the  most  famous  feature  of  the  transformer. It  was  invented  directly  in  order  to  address these  problems  with  long  sequence  lengths  in  RNNs. And  that  feature  of  Transformers  is  attention. Have  people  heard  the  phrase, attention  is  all  you  need? No.  Okay.  So  that's  the  name  of the  paper  that  introduced  transformers. That  paper  was  published  in  2017. It  was  from  Google.  It  was  a  Google  paper. There  were  like  ten  people  on  it,  I  think. There's  not  a  single  one  of  them  left  at  Google  now, and  they're  all  extraordinarily wealthy  as  a  result  of  that. Yeah,  it's  funny  because  there  are random  interns  that  are  on  that  paper that  I  think  probably  didn't  do  very  much. The  world  really  care  about  that. But  no,  basically  the  transformer  is  extremely  important, Fame  and  wealth  is  very  well  deserved because  Transformers  solve  a  lot  of  problems  with  RNNs. We're  going  to  start  by  talking  about, when  we  look  at  an  RNN, what  do  we  see  is  happening  there? See,  this  is  attention. Let's  just  draw  our  simple  RNN  diagram  again. All  of  these  nodes,  they're sequentially  connected  to  each  other. All  of  the  effects,  all  the  bad  stuff that  we've  been  talking  about  related  to  RNNs, it  has  to  do  with  the  indirect  effect  of  stuff  at the  beginning  of  the  sequence on  things  that  are  happening  later  in  the  sequence. That's  where  all  the  problems  come  from. What  transformers  do  is  they  directly address  this  problem  by making  everything  connected  to  everything  else. One  way  to  think  about  a  transformer,  in  fact, it's  not  a  bad  mental  model  to  have, which  is  that  a  transformer  is  almost like  if  you  just  drew  arrows between  a  pairs  of  nodes  in  the  RNN. Rather  than  having  all of  these  things  separated  by  a  long  distance, we're  actually  going  to be  connecting  everything  together. This  is  not  quite  true,  there  are  some  wrinkles  there, but  this  is  an  approximate  idea that  you  can  have  in  your  heads. Okay, let's  say  we have  a  sequence  of  words. So  what  we're  going  to  be  doing  over the  next  one  or  two  classes, I  explicitly  write  these  out. What  we're  going  to  be  doing  is writing  down  something  that's  not  quite  a  transformer. So  you're  not  going  to  be  able  to trust  anything  literally  that I  tell  you  over  the  next  couple  of  classes. Because  a  transformer,  there's a  bunch  of  components  in  it. It's  this  big  hulking  machine  basically. Do  you  know  if  these  Rube  Goldberg  machines  where. Yes,  it's  a  little  bit  like  that. There's  a  lot  of  little  contraptions  in  it. If  I  directly  presented  to  you, it  won't  be  good  for  anybody. What  we're  going  to  be  doing  is we're  going  to  be  building  up some  intuitions  about little  components  within  the  transformer. We'll  eventually  get  to  the  actually  correct  definitions, but  just  need  to  know  right  now  that none  of  the  definitions  I'm giving  you  will  be  quite  right. Until  the  very  end, we  have  an  input  sequence,  one  through  four. What  attention  says  is  that  we're  going  to  do  pair wise  comparisons  between  all  four  of  these  words. So  that's  section. Makes  more  space  for  myself. Okay,  So  I  don't  want  to  draw  in  all  16  terms  here. I've  drawn  in  six  of  them.  We  can  fill  them  out. What  am  I  doing? What  I'm  doing  is  that  I  have  a  matrix  in  n  by  n  matrix, where  n  is  my  sequence  length. Where  I'm  comparing  every  pair  of  words. In  my  notation  here,  Sim  12, let's  say  this  is the  similarity 1-2  I have  two  vectors  for  every  pair  of  vectors, I'm  calculating  their  similarity. What  do  I  then  do  with  this? I  have  a  calculation  of similarity  for  every  pair  of  vectors. I'm  then  going  to  use  these  calculations  in  order  to compute  an  updated  representation  of  each  of  my  words. Okay, so  how  do  I  do  this? What  I'm  going  to  say  is  that the  new  word  representation for  word  is  a  vector. I,  one  way  to  call  it, what  I've  done  is  I'm  taking  in  a  sequence  of  words. Whatever  sequence  it  is, I  want  to  process  the  sequence  of  words. I'm  going  to  be  computing  a  new  word  representation for  each  word  in  this  sequence, I'm  going  to  have  a  new  123.4  I'm  going  to use  this  attention  matrix  that  I've computed  in  order  to  compute  a  new  word  representation. Here's  going  to  be  my  formula  I  have  that I  is  going  to  equal,  well, let  me  actually,  I'm going  to  first  define  a  probability  distribution  I. This  is  the  probability  distribution  for  the  yth  word. This  probability  distribution  will  be  equal  to Softmax  of  the  ith  row of  this  attention  matrix. Well,  let's  just  break.  Let's  assume we  have  four  words  for  right  now, so  I  do  a  soft  max  over  this. It's  a  pretty  bad we get,  we  do  we  get  back. I  have  four  numbers  that  I  put  into  the  soft  max. Right?  I  have  the  row. You'll  notice  for  word  one, if  I'm  trying  to  compute a  new  representation  for  word  one, I  take  the  first  row,  I  look  at the  similarity  of  the  first  word with  all  of  the  other  words. I  put  those  similarities  into  a  soft  max. What  happens  when  I  do  a  soft  max  on  four  numbers? I  get  back  four  probabilities. So  let's  call  this  123.4 I  have  four  probabilities. I've  gotten  back  from  this and  there  was  a  question  over  here. Yeah,  great  question. Let  me  get,  it's  a  good  question. Let  me  get  back  to  that.  Yeah. Okay,  so  what  are  these  probabilities? These  are  going  to  be,  we  can  think about  these  as  probabilities. That  word  I  assigns  to  all  words  in  the  sequence. We're  going  to  use these  probabilities  in  order  to  compute a  new  representation  for  the  ith  word. That  was  the  reason  why  we  did  this. We  first  compared  word  I  to  all  of  the  other  words. We  calculated  the  similarities. We  ran  those  similarities  through a  Softmax,  got  back  probabilities. Now  we're  going  to  use those  probabilities  in  order  to  compute a  new  word  representation  for  word. We're  going  to  have  a  I  is  equal  to  a  sum, weighted  sum  of  all  of  the  other  words, where  the  weights  are  the  probabilities. It's  going  to  be  one  times  one plus  two  times  two  plus  three  times  three, plus  four  times  four. We're  taking  all  of  the  words, we're  weighting  them  by these  probabilities  and  we're  summing  them  up. The  new  word  representation  for  word, it's  actually  a  mixture of  all  of  the  other  words  in  the  sequence. Yes.  Which  weights  are  you talking  about?  Those  are  not  weights. These  are  word  vectors.  Yeah.  Okay.  Yeah,  I switched  before  I  can see  where  the  confusion  could  have  come  from. This  is  the  word  vectors. So,  I  take  all  my  word  vectors  and  I  mix  them  together. That's  my  new  representation  for  word  I. Yes,  similarity.  I  haven't  defined  it. Yes.  Great  question. We'll  get  to  that  in  a  second. It's  to  be  a  scalar, it's  a  scalar,  correct? Yes,  similarity  is  always  going  to  be  a  scalar. Any  other  questions?  Okay. So,  we're  going  to  basically  do  this  process  repeatedly. So  we're  going  to  do  this  for  every  word, so  we're  going  to  compute new  word  representation  for every  word  in  the  sequence. We  have  four  words.  We  get new  word  representations  for  each  of  those  words. The  new  word  representation  for each  word  is  a  sum  of  all  of  the  words  in the  sentence  where  the  weights are  given  by  this maybe  funny  formula  that  I  have  over  here. This  soft  max  then  gives  you  probabilities. Words  are  now  all  being  mixed together  in  this  particular  way. We're  then  going  to  repeat  this  process, K  times  the  K  times, those  are  the  K  layers  in  your  transformer. There's  other  stuff  that  goes  on  in those  layers  as  well  that we're  not  talking  about  right  now. But  this  is  one  of  the  things  that  happens  there. You  do  attention. If  you  have  a  100  layer  transformer  that  means you  mix  together  all  of  the  words. You  get  new  word  representations  through  this  process, through  attention  times  100  times. Okay,  at  the  very  top  our  transformer, what  we  have  is  at the  final  layer  of  the  transformer, we  have  new  word  representations  for  every  word. In  this  case,  they'll  be  four  new  vectors, a  one  through  four. We  can  think  about  these  as new  word  representations  for the  words  in  the  original  sequence. They're  just  new  representations  of  a  red  pajama. What's  the  difference  from the  original  word  representations? The  original  word  representations  that  we  had? The  ones  that  were  input  into  the  model. This  is  just  a  representation of  the  word  llama  on  its  own. All  that  this  word  vector  here  knows  about  is  that  it  is the  word  llama  doesn't know  anything  else  about  the  rest  of  the  sequence. Same  thing  for  red,  same  for  pajama. This  is  just,  we  have  a  vector  representation for  those  individual  words. What  happened  after  we  did  this  transformation? We  did  attention  a  bunch  of  times. We  did  attention,  we  got  new  word  representations. We  then  fed  those  word representations  into  attention  again, and  we  just  keep  repeating  this  process. What's  happened  to  these  new word  representations  that  we've  computed? Yeah,  they  have  information about  all  the  rest  of  the  words  in  the  sentence. This  one  it  probably knows  everything  there  is  to know  about  the  rest  of  the  sentence. Same  thing  for  23.4  Why  is  that  relevant? We  can  actually  immediately  see  why  that  could  be useful  for  this  example. A  llama  on  its  own  is  it's  a  farm  animal. Or  maybe  something  that  exists  in  Peru, or  something  that  climbs  up  mountains,  a  red  pajama. When  you  see  the  word  llama  in  this  context, it  means  something  very  different. It's  no  longer  that  actual  real  animal. It's  something  else. These  words  now  know  that, they  know  that  they're  in  a  context where  they  mean  something different  because  they've  been mixed  together  with  all  of  the  words. Okay,  now  what  we  can  do  is, let's  say  we  want  to  build  a  classifier. There's  subtleties  here,  we'll  talk  about  this. But  one  simple  thing  that  we can  do  that  would  work  reasonably well  is  we  could  just choose  one  of  the  words  in  the  sentence. Just  arbitrarily  choose, let's  say  the  last  word  in  the  sentence. We  can  then  build  a  classifier  on  top  of  this. Just  do  logistic  regression  on  top  of  this. That  is,  multiply  this  by  a  weight  matrix. Out  we  have  an  output  vector  O, then  we  have  a  probability  vector. We  use  that  to  calculate  a  loss, just  as  we've  been  doing  with  all of  our  other  architectures. We  take  the  word  representations from  the  final  layer  and we  run  that  through  a  classifier. If  we  want  to  know  like  is  this an  object  from  a  book  or  is  it  a  farm  animal, like  that's  the  task  that we've  been  assigned.  We  can  do  that. Now  I  want  to  highlight, we're  going  to  go  into  a  lot  more  detail  about  this. What  I  want  to  do  right  now  is  just highlight  how  this  is  different  from your  RNN  and  how  the  RNN  does  classification  or  sequence understanding  at  every  stage. When  we're  doing  attention, information  is  directly  flowing across  all  of  the  words  in  the  sentence. The  first  word  gets information  from  all  of  the  words  directly. Because  sum, it's  a  weighted  sum  of  all  of  those  other  words. The  last  word,  same  thing. It  gets  information  from  the  first  word  directly. That's  different  from  how an  RNN  processes  things  which  is  completely  sequential. And  where  there  are  lots  and  lots  of  steps that  are  barriers  between  far  away  words  in  the  sentence. This  was  the  original  purpose  of  trans  of  attention, which  was  to  bypass  long  distances  in  sequences. Any  questions?  Yes. What  does  it  matter? It  doesn't  matter. In  this  case,  there  can  be  some  wrinkles, but  for  this  case,  does  not  matter  whatsoever. It's  a  fantastic  question. Okay,  let's  end  there. See  everyone  on  Thursday.
